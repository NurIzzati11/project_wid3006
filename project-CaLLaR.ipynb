{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting librosa\n",
      "  Downloading librosa-0.8.1-py3-none-any.whl (203 kB)\n",
      "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from librosa) (0.23.2)\n",
      "Collecting resampy>=0.2.2\n",
      "  Downloading resampy-0.2.2.tar.gz (323 kB)\n",
      "Collecting audioread>=2.0.0\n",
      "  Downloading audioread-2.1.9.tar.gz (377 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from librosa) (20.4)\n",
      "Collecting soundfile>=0.10.2\n",
      "  Downloading SoundFile-0.10.3.post1-py2.py3.cp26.cp27.cp32.cp33.cp34.cp35.cp36.pp27.pp32.pp33-none-win_amd64.whl (689 kB)\n",
      "Requirement already satisfied: joblib>=0.14 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from librosa) (0.17.0)\n",
      "Requirement already satisfied: scipy>=1.0.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from librosa) (1.5.2)\n",
      "Collecting pooch>=1.0\n",
      "  Downloading pooch-1.3.0-py3-none-any.whl (51 kB)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from librosa) (1.19.2)\n",
      "Requirement already satisfied: numba>=0.43.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from librosa) (0.51.2)\n",
      "Requirement already satisfied: decorator>=3.0.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from librosa) (4.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa) (2.1.0)\n",
      "Requirement already satisfied: six>=1.3 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from resampy>=0.2.2->librosa) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from packaging>=20.0->librosa) (2.4.7)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from soundfile>=0.10.2->librosa) (1.14.3)\n",
      "Requirement already satisfied: requests in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pooch>=1.0->librosa) (2.24.0)\n",
      "Collecting appdirs\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from numba>=0.43.0->librosa) (0.34.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from numba>=0.43.0->librosa) (50.3.1.post20201107)\n",
      "Requirement already satisfied: pycparser in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from cffi>=1.0->soundfile>=0.10.2->librosa) (2.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->pooch>=1.0->librosa) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->pooch>=1.0->librosa) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->pooch>=1.0->librosa) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->pooch>=1.0->librosa) (2.10)\n",
      "Building wheels for collected packages: resampy, audioread\n",
      "  Building wheel for resampy (setup.py): started\n",
      "  Building wheel for resampy (setup.py): finished with status 'done'\n",
      "  Created wheel for resampy: filename=resampy-0.2.2-py3-none-any.whl size=320724 sha256=f3db0b5ed730c57ea49baed448124079bbb7dba69a49eca9f3a6624049e49f50\n",
      "  Stored in directory: c:\\users\\lenovo\\appdata\\local\\pip\\cache\\wheels\\6f\\d1\\5d\\f13da53b1dcbc2624ff548456c9ffb526c914f53c12c318bb4\n",
      "  Building wheel for audioread (setup.py): started\n",
      "  Building wheel for audioread (setup.py): finished with status 'done'\n",
      "  Created wheel for audioread: filename=audioread-2.1.9-py3-none-any.whl size=23146 sha256=752d69bff3dc6e67972654e0c3cfdb751f796573641ed0f43ca847f4da5ba6d5\n",
      "  Stored in directory: c:\\users\\lenovo\\appdata\\local\\pip\\cache\\wheels\\49\\5a\\e4\\df590783499a992a88de6c0898991d1167453a3196d0d1eeb7\n",
      "Successfully built resampy audioread\n",
      "Installing collected packages: resampy, audioread, soundfile, appdirs, pooch, librosa\n",
      "Successfully installed appdirs-1.4.4 audioread-2.1.9 librosa-0.8.1 pooch-1.3.0 resampy-0.2.2 soundfile-0.10.3.post1\n"
     ]
    }
   ],
   "source": [
    "!pip install librosa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename= 'B_TIG01_EU_FN_GIU01_201.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "import librosa\n",
    "import librosa.display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.8394549e-04 1.3996716e-04 5.1123124e-08 ... 2.9759272e-04 1.0348492e-04\n",
      " 0.0000000e+00]\n"
     ]
    }
   ],
   "source": [
    "## reading a sample audio using librosa\n",
    "librosa_audio_data,librosa_sample_rate=librosa.load(filename)\n",
    "print(librosa_audio_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" >\n",
       "                    <source src=\"data:audio/wav;base64,UklGRnp/AABXQVZFZm10IBAAAAABAAEAQB8AAIA+AAACABAAUEFEIMwPAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABkYXRhgm8AAAYA/P8CABIA//8IAPf/7P8GAAoAAwDg//T/HAAUAPb/+f8MAPj//P/v//D/GgARAP3/EAASAAoAAgAAABIAAAD+/wgA9v8BAAMA5v/3/xYA/P///wAA8v8SAAYA6P/u/wAA/P8OAB0ABAAMAAAA+v8AAOz//f8AAAYAFwD4//n/HAAHAPb//P/+/wwACgD6/wYAGAADAPH/7v/4/wkA8f/e/9r/6v/+//7/+v8EABoACAATAA4A7v8SAAoA4//w/wQAAgACAAAAAAANAAYAFgAKAO7//P/p//j/EwD7/wgAGgAIAP//DgD6/+z/EAAJAPb/7v/a//z/CgD6/wAA+v8GABYA9//w/wAA/f8MAAQA6P/4/woAAgD6//L/AgAXAPr//v8GAPj/AADu/wIAFQACAPX/7P/8/woACAD7//7/EAAKAAAAAAAAAPr/AAABAAEA/f/+/xAA7v/l//D/7v8PABgABADo/+b//v8FAAQA+P/s/wQABgD+/wkA+/8IABYA///2////CAAHAP3/AgAJABIAEwACAAIA+P/z/wMACgD//wgAIAAEAPD/7v/y//7/8v/s//7/AwD4/woAFgAYAAgA7P/6/wUABgD0/+r/+v8IABIAEAAAAPz/DgD7/+j/7v8JABsABQD7/wAAGAAIAO3/+v8CAAAA7v8GAB4AGAAOAPT/AgD9/+r/BgARAAoACgAAAPb//v8HABQACwDy/wIADAAAAAAA9f/5/xQABgD4/w4AFQAIAAIA8v/2/wYA6//4/wYA+P8GAAQACAD5/+L/9v8QAAYA8v/5/wsAGQAUAAAA7P/9/woA/P/4//L/BgAOAAYA///m/wAAEgAAAA4AFgD8//b/CgACAPX/+v/6/wMA9f/4/woA/P8UAA4A6v/z/wwAFgAIAPj/6f/x/+7/7P8EAAYADgD//+v/AAAEAOr/6//u//D/EgD1//H/EwD3//T/8v/8/xQABQD2//L/CQAAAPz/CgAIACYAHgAJAAYACQAOAP7/DAAUAAoA9f/w/wQADgAYAPf/3v8EABQABAAEAP3//v8AAOb/7v/9/wkAAAD3/wIA9P8DABAAAgD+//z/AgAIAPb/9P8BAO7/9f8AAPz/EAAMAAAA/P/2//j/CAAVAAoABAAMAAQA9P/4//r////9/+T/9P/+//7/DAD9//r/EQAQAPj/9P/+/w8AEgAHABAAGgAUAPT/AAASAPX/EwASAPj/EwAOAAUA8//m//L/9P/+/wMABgAXABgAAwD+/wwADgAAAO7/BgAaABAACQD6/w4AHQD1/+j/+f8IAP7/8f8KAAQA+v8EAAIACQD4/97/6v8CAAoADgAOAA4ABQDy/+//8P/5//D/4//v//b/8v/u/wYAEAANAAgA9v/6/wYA+//q/wAADAAAAAAA9v/0//n///8WAA0A/v8GAP//AwAAAPH/9v8LABoABADz//z/EAADAOP/8P8hACgA9P/y/w4AAgDv/+z/AgAQAAIA8P/z/wQAAgDp//T/DgDw/+f/EAAgABQAAgAQABAA8f/8/wYA9P/u/wQABgDs/9j/1v/2/xIAAgD4/wQAAwD9/woAEgASAAYA7P/r/wQABgDu//P/CgAVAAoABAAEAPD/7v/+/wIA+//z/+7//f8MAAgACgADABMADgAEABoA9P/q//z/+P8EAPr///8UAAoAAAD4//b/EAAQAOr/7v/7//b/9//z/wYA/P/2/wwA/f8IABIACgAFAP7//P/+/xAAAAD6////AgAJAPD/BAATAPr/+//7//7/BgD+/+P/7P8UABAA+P/6/wAAGAAOAPz/FQADAAAAAgD4/woA+P/r//r/CAAJAP7/+v8GAAMA9//8//z//P/+//z/AgAAAOz/8P/9//r/BgAFAAYAEAD+/woAGgD3/+f/8v8KABQA+P/o/+f/9P8HAAgACgAIAAAACQABAPz/AQAHAAwA+/8AAAYAFQAmABMAEQAFAPL/7v/j/+n/7//w//b/9P///wEA+P8PABoAAgDy//z/GAAWAAgACQAAAAQA+f/2/woABwAYABYA+f/0/wYADgD5//n/9P/u/wsAEAD8//v/+P8XACAAAAAAAPf/BwAFAOL/8v/9/wcABgD5/wgAEAAKAPT/7v8EAAUAAgACAAQAFgACAOb/9v/6//j/+P/x/xIAHQAVABoACgAGAAUA/P/a/9z/BgAEAA8ABgD4/wAACAAVAPT/9P8SABkACADa/9b/+f8MAAQACgAAAO7//v8FAAoA+v/3/wwADAAQAAQABwAdAAsABAAdAAYA8f8AAOT/0P/e//P/BAAKAAcA9P/5//L/+v8AAOf/AAD4/wQACwDt/xAAEgAIAAAA3P/t/woA8v/k//T/BAATAP7/9P/y/wQAKgAHAO7/9/8EAAgA+/8BAAoABAD4/+7/6v8OAB8A7//g//r/9v8AAPr/9P8aAA4A9//4/wAAAAD1/wwAFQAIAP7/7v/s/wcAAgDy//z/BAADAPf/DAAQAPb/+v8RABMABQD///r/EgAUAPD/7f8EABAAAwAAAAUACgACAPz/CgAGAAQAAwDy//L/BgASAAwACgD9//f/AQDy/+7/8P/d//P/HAAOAPb/8P/3/wwADAAIAAgADAAQAP7/+v8BAP7//v/7//H/9v/6/wgAHAAGAAYACQACAAQA7v/8/wwA9v/x/wkAHAAQAAYA+v/q/wQAHQAOAPn//P8SABwA/v/m//L/DAAYAAAA6v/y/wkABwD0//v/BQAIAPz/7v/6/xIA9v/m/w0ADwAGAPj/9v8XABQACwD1/+r/BAD3//D/AwAMAAQA+v/+//z//P/3/wMAAwDy/wgACgAGAPL/8v8YABAABQDs/+//EgACAPL/7P/7/xAA+f/u//7/BAACAPz/DgASAAUADgACAAcAEAD8/wQABAD3/woABQD0//z//v8WAB4A+f/v/w4AEgACAAEA+f8DAAQAAgD9//X/+v/8/wAA7P/+/xgAAAD5//v///8QAA4ABAD4//j/CgAKAPz/+P8IAAYA+v8AAAAACAD6/+r/6f8AAAoA8P/u/+b/CAAZAPz/FAASACwAJgD0/woACgDy/97/CQAeAOv/3v/X//n/EADu/+b/3P/s/xgAGAAaABMA+v8JAAoAAAD4/wkAIgAOAPf/8P/8/+z/9v8mABgA7v/Q/+P/DQACAO7/5v/m/wwAFgASAA0A6P/3/xoAAwAAAAoA5P8EADAAEADy/9z/6//w//7/EgACAPj/5P8IABYADAAIAOL/DgAOAOL/6P/u/xAAKgAWAPz/8P/0/xAAEwAQAA4A/v8KAOb/+f8ZAPz/HgAfAP7/8P/6/xoAEgAGAP7/+v/y/+7/BAD9/xQA8v+l/+X/FgAgAB8A///0//T/JAAaAOL/AAAmACMAEgDt/87/5P8sAEoAFADf/9n/8v/v//f/LgAeAOv/zv/V/xIAJgARAAIA+P8YACMA7v/Z//r/CQASAAwAAgAOAPn/BgAcAA8A9P/l//P/xv/a/xMACAD8/+z/6v/u/yIALwD3/+L//P8QAOj/9P8cAP7/2P/m//f/DgAbAO3/9P/2/xAAQAD+/7r/1P9EAFIAHADH/2j/7P96AGYABgC5/9b/EAA0AEAA+v+U/8j/KgAYAOb/AQAKAOT//v/x//T/BQDd/9z/CQA1APT/tP+6//L/MgAuADAAAAC9/7j//v9GAC0A+P/C/7r/zv/w/0oAQADk/8r/3f8aADIABgD2/xAAJgD//+r/8//y//b/DQBIAB4A1P+7/7P/2v8YACwA3P/E/97/+/9EAEYAHgDk//b/WwBwAFsASgA4ACwAIQAaABQAEAAMAAkACAAGAAQABAACAAIAAgACAAEAAAABAAAAAAAAAAEAAAAAAAAAAAAAAP7/AAAAAAAAAAD+//7/AAAAAAAAAAD+//7//v8AAAAAAAD///7//v///wAAAAAAAAAA/v/+//7/AAAAAAAAAAD+//D/7v8YACwADwADAAEA9P8CABQAAAD1//z/8v/z/97/6P8IAAoAGgAUAAIA/P8dABwAAgAWAAAA+P/g/9T/BAAXAAYAv//E/w0AMgAmAOD/5v8YACYAGgAGAAoABQAAAPL/AAAdAAwA+//e/9X/BwAMAPj/8P/w/wkAEgD5/+D/4v/0/ygAQgAaAOr/yf/+/x4A/P/q/wgAEADW/9T/3P/0/yoAJAAOAPz/8P/j//T/JAAwACYAAgAEAAQA3v8DAB4ACgAGAPz/w/+4/wgAHQAuADAABgD4//L/GgA0AAIA6v/2//j/EgD9//L/AwD0/xIAEgDv/9b/9f8mABMA+f/y/+7/5f8CACQAIgDs/7b/1P/0/yEASAAgAOj/6//0/+T/AgALAAgAGAACAPz/9v/s//T/AwAHAAYAEQD5/9z/2P/0/wYACwAeAP7/6f/0/wcAAwAOADEA6P/W/x4ABwDk/y0AKACo/8f/CAAEABoA/P/y/+f/6v8KAAcAGwALAN3/2v///////v9KAE4AGAD8/9X/4P/3//L//P8OAOb/t//M/8j/zv8IABgADQAOAAkADgApADwAPAAwACQAGwD8/9b/+P8AANz/8/8GAN7/tv/R/9T/2f8BAAAAAQALAB8ADgAdAE8AUQBOAEIALADs/+L/EwDy/97/6f/S//j/FADW/87/7v/u/xcAQgAKAOr/+v/y//z/KgBiABoA0f/g/87/3f8IABgA6v/o/xoA6P/s/xAA5f/5/yoACQDe/wcALAAeABIA8P/Z//D/EAD8//r/CADk/wYAEgD8/ygARAD7/5z/4/82AAoADAAYALT/tv8KAAIAGAA0AAMAx//e/w8AGgAXABAACADx//T//v8OAAAA9P8UAAgABADx//z/BgDu//j/6f/0/wYA8v/b/+r/HwArABwACgAKAAgABwAXAAgA6//i/w4AAwDa//r/4//a/xEAMAAnAAYA9//Q/9L/DgAgABAA+v/o/+D/AAAUAPL/9v8EAPP/DAAMAO7/1P/X/wIAHQAnAPD/8v/u/9b/CAArADAA4v/6/x4A5P/2//T/9v8aADIADADe/9v/7v8OABUAJADa/6//6v/0/x0AOAAkACAABgDa/+f/IwAGAO3/+P/e/xoAOAAEAOn/3/8NAAgA2P8GAB0A6v/i/xIACgD8/yAA9P/h//T/1P/e//j/GAAWAPL/8v/s/+D/8P8iACUACADj/8f/EgBCAA0A+f8qAAwAAAAUANX/CQApAPf/8P/t/wYAEAAIAPb/5f8AACkADADu//7/9P8DAB4AKwAuABwA9v/V/9//BgD8/7v/2P/z/9r/EAAAAK3/2P8QAD4AhQA5APj/IAAQADoAbAAhAPL/5v/E/+//AAC4/7L/vv+5/+L/6v/s//X/CgA2ADYANAAYABoAQgBMAFIAEgDm//T/AwDu/8v/2v/9/xIA8f/+/wUA7P/6//b/+f8PADAAFAD8/xQA///1/wQAFgD8/97/6//c/9L/xv/r/0YAJgDk/+z/4f8AAC4AAwAGABMA9v/2/+L//P8eAPz/6//4/9r/5/8mAAEA8f////b//v/9/xQAHgACAPD/9/8QADQAOgAaAPH/0v/8/yQACwAIAAoA+P/o/9T/5P/4/xoANAAGAOD/AwAsAP7/6f/8/+r/FAAqAPj/6P/w/9r/7P8dAAAA5P8EAP7/8v8dABQAAAD1/9z/HAAmAAkA+P+5//D/OgAUABYAAADQ//T/FgAQACAAMADj/8T/+P8SACUAHgACANL/6//1//L/OAApAPv/3v/6/wQA/v87AAwA1v/h/+z/LgAyAPD/2f/q/x8AHAAIABQA7P/m////2v/p/zIAEADa/+T/AAAOAAgAEwD0//b/GAAQAPb/9v8tACgABQAAAPv/+v8cAAUAtP/K/+r/FgBIAPz/4/8CANv/2f8YABYA8P8GAOT/1P8QACQALAD8/+r/BADz/xQABwDc//L/+P/q//D/DgAgAP7/0P/Y//b/DAAxAA4A7v8CAPL/JgAoANv/6P/+/+7/AQAQAO7/IwAkALr/0P8LABYAEgDr/9z/7P8IAC4ABwDn/xQACwDy/wsADgD4/w0A8P/u/zQAMAAiAAYA0v+8//v/+P/L/w8AJAAQANj/2P8IAA4AOAAcAOT/4v8CAAAA9v8NAAwABgAJAPr/9v8JAAcADgD6//j/CgAVABgA8f/5/+r/CwALANf/9v/9//v/4//M/9//NwAPAMb/EADk/9n/KAD8/+r/CgDi/wkACADe/wQA8v/+/wYA/P8SACgA6P/H//T/AgBFABwA0f/H/8L/9P8jAB0AFgAaAP3/AAAEACMAJgDa/8r/4P8EAFAAUAAGANz/of/h/zIAFgAZAPz/1P/m/yAAHwA2ABkAqv/G/wwAJwAQAPb/3P/U/+z/EAAsAOz/2v/t/+r/BgD+//D/HAD7/87/+P/6/wYAFADs/9P//f82ABAA4v/n/wYAKQAaAPz/4v8RADQA6P/Z//7/GgAqAAIA5v/u//j/IgALAPH/JAD0/+//8f/g/ykARgAbAML/1v8YADYABQDU/w4A/f/s//r//v8AABYA/P/3/w4A9P8wACAACAAAANz/BgATAO3/v//x/xEAAwDw//L/IAD6/8j/5v8ZABUAFADk/97/CADv/xwAHgD+/wEAEgAKANL/8v8EAPn//v/Z/9//KAA0ANj/qv/o/woAAQAGABIADADi//z/GgD5/w4ACQDe/+z/7v/y/woACAAGAA4AFwD6//n/8f/g/woADAAgAAQA4P8IAAwA8v/K//f/HgAaAPn/wP/9/zQA+f/c/wkAKwAiANr/uv/k/woAKgAhAOz/t//F/wYARAAfAOr/DQAKAPr/AgAeADEAEwDO/8n/JAA1APD/0P/m/9b/3v/6/xMAJgDK/8L/IgAqAA4AKgAzABIA3P/a/0UANQDk/+b/8v8HAAgAAgAMABQA5P/W//r/BAAkABgA8P/o//L/CgAcABgA///O/8b/EgAqABgAKAAPANT/xP8BACAAFgAMAPb/6v/1/y4AOwAOANb/0P8OABAAGgASAPr/AwDu//z/NgA+APD/yP/S/97/HgA6AAUAwv/V/w4AGgAYAOL/zv8CAPz/2f8CACsAGQDm/+z/OAAiAPb/2//G//7/HgAiAAwA5f/q/+7/8P/2//b/+P/6//r//P/8//z//f/9//7//v/+//7//v////7//v8AAP///v/+/////v8AAAAAAAAAAP7//v/+//7//v8AAAAAAAAAAP7//v////7/AAAAAAAAAAD+//7////+//7///8AAAAAAAD+/////v8EAAwAGQA4ACYA8v/t/woADgABAPr//P/2/+v/FgAeAAEA+v8DAAwA4f/U//z/HgACAOT/BAAFAAIABwANAPj/1//u/wIAEgAQAAAABQDu//T/EAAeACoA///Y/+z/DAAUAAwA/P////7/4P/t//v/+P8GAAQAAgAIAAoABAD8//3/+v/8/xIA9//O/+X/AQASAAwA+P8EABQAGAAaAAgABAAQAOf/1f/8/wQA/P/f/8b/5f8EABQAGAAaAB4ABgAlAEwAIADk/+T//P/Q/8z/wP+t/8v/qv/q/yAAAAApAE4AagB0AFUANgAgAOz/1v/8/9D/hv9m/3z/0P/7/xIAJgAfAEYAeABwAEgARgBeADIA8f/M/77/sP+U/5j/p/+l/9f/HAAhAEAAdABlAEwASgA+AC4AFADa/7n/sv+s/7j/tP+w/9D/5P8NABgACgA4ADcAQgBQACoALAAYAPb/5v/G/7n/3P/h/7T/uP/U//j/BgD7/xoAGgAPABQAFgAkACEAGwASAPr/3P/0/w0A3v/F//b/DgDi/9j/5//6/yoALQAVAP7/9P8jADAADAD7/+P/9v8QAOL/zP/u//X/7v8IAAwACQATABsAGwAOAA0ABAD6//3//P8AAAoA+P/3/wsA/P/8/w4ADwD8//b/DQAiAPv/6v8EAPD/7v/s/wQALgAIAOD/5v/8/xYAHAD4/wMAAwDY/+j/GAAwABAAAgD0/+b/2f/t/yoA8//h/xYAGwAMAAQACAAdACQA2//a/+T/wv/n/w0ADwDn/+H/+v8kABYA4f8JACAAIAAFAAYAFgDg/9j/7P8SAB0A8P/u//r/DwAAANb//v8pAB4A5P/b//z/6/8SABIACAASAND/yv8GAA0A0v8UACAA0f8NAAIAGAAlAND/6f8aAAYA+v8wAAYA1P8NAC4AOgD9/9z/AgDO/8r/DwD0/7//1f/6/wEAMwAGANT/DgAOADQALwAfACgAAAD4//L/5P+W/4z/uP/a//n/DABeAEQANgBMACoAMgD6/87/AAAeAAQA+v/w/+f/zP+u//b////G//P/XQBSACoAIAD2/yAABQDZ/+z/xP/I////GQACAO7/4v8IACoABQASABcA9v8QABsAAgAIAOD/3P8GACwAIwD8/wQA/P/0/9z/GAAvAOL/6f/4/xgAHgAOAP7/9P/w//D/JgApAP7/0P/k/xMAEAD0/+L/7P/8/xsAHAAhAPb/6P8aAP7/7v8EAAIA9P/4/wYALAAZANz/1P/x/xQACgD3/9r/6v/q/wMASQAGAO7/7v/w/ygAHQD6//r/HQD2//3/IQD6/+T/3P8KAA4A6P/4/xwAFAAGACIAHwAEAOr/DABAADAA+v/z/wYA/v/u/9b/5f/8//j/5P8DABoACAAgACQAKgATAAAA/v/i/w8AKQAWAAIAz//C/wgAJgAGAAgA+f/W/+//GgAQAAMA8/8KADYAFAD1/+z/8P8DAOT/4v80ABwA1//n//n/LAAPAOz/DgAMAPT/6P8zACYA9f/i/8r/BADz//D/BgDn//b/HAAsABwA/v/y/xQAJgAYAPv/8v/s/9j/+/8gACUADgDq/9z/+v8eAB4ACwDx//b/+f8YABoA/f8GAOb/BAAfAPb//v/y//X/EQAIAPr//f/v/+f/BQD2/+7/IAAAAN7/AADy//z/DgDu/wQABQDx/w4A/P/k/wwAAADm//z/7v/+/wwABAAQAAYADAACABEAGAAPAAIA3P/y//T//P8fABcA9v/v/wcAAwDz/+D/+P8OAP3/AgABAP//+v8AAAMABwAaAAYA8v/u/wIAEgD2/wAAEAD+/+//5f/6/woA8v/4/xsAEgD1//T/7P/2//z/7v/4//T/CgAGAOj/8v/2/xAAFgAMAAAA6f8DABIAEwAQAAQABgDV/8b/7v/3/xcADgD6/woAAgD4/+3/DAAgAPz/6P8EADAABADW/woAFAAWABEA8v8IAO//7P8aABkA4v/W/xIAAgAaAPz/wf8SABQAIAAYAPT/BQDv/wUA+v/s/+n/1f/z/wQAFgAUAAwA/P/u////CAAoACgAGQAaABAADgAKAAgABgAFAAQAAgADAAEAAgAAAAAAAAAAAAAAAAAAAAAAAgAAAAAAAAAAAAAAAAAAAP7//v/+/wAAAAAAAAAA///+//7////+/wAAAAAAAAAAAAD+//7//v/+//7//v8AAAAAAAAAAP///v/+/////v////7/AAAAAAAAAAAAAP7//v////7//v////7/AAAAAAAAAAD///7//v////7//v////7//v8AAAAAAAAAAP//AAAAAP7//v////7//v////7/AAAAAAAAAAAAAAAAAAAAAP7//v////7////+/////v8EABQADAD4/wYA/P/2/wYA+//0/wYADgD5/wgAEQD//xYAIgDr/2n/dv+qAHoA1P+CAGr/z/+aALX/mgDm/wgAaQCC/xIA6f8oABEAFAAEAK7/FAAWABgAtP8PAN3/0f9bACUACgAHAE4As/+8/8b/6/9uAOH/MQAQAKb//v9hADAArv+u/5sAJQAa/1AAjv/c/1QAZP9DAKD/VAD0/0f/yv9r/4oASQCQADYAsv9sAE4AGgDC/yoADgCsACgAwP9CALH/PADB/wMA6v+O/ykAv//i/5//nv8aAMP/5P+QAO3/0P/A/6z/ZADs/5YAJQCw/54AEwAOABwA3P+e/+7/PgA0ABQAzP+EAH4Aiv+0/0AAw/+DANsAjv8mAOj/uP+U/yj/KAHM/zP/wAAI/5D/RAAEAIAAsP/u/2QALADq/xwAEABw//D//P/s/+X//P9QAG//+f9WAIv/9P9iAC4A7v8DAAUARQAMACkAhgDY/0YASgBg/6z/7v/Y/8L/dv/+//r/jv/0/+r/xP/S/3wAYgAwAH4AHAA7ABAAQgD9/2z/HwAuAMD/7f8nAJb/7f9cAMT/wf+l/5z/xP/C/+P/EAAIALr/KABdAA0AagBeABwAvv/y/3oAAwAsABgA9v8RABoA5AAGAcgAOAAMAC4AxP8mAP7/MP+h/kj+e/5x/qb+1P5m/nj+qP84AfoBHAOKA48C/QHOAFwAGQCV/8j/d/5Q/tD+qf5i/33/VP8a/6r/SwBDAbEBgQC9AAAAuP+GAFEAtgDN/1H/rf/Q/1gANgAyAI7/Qf9u/2X//P+q/6b/AADq/0IAIgBKAGAAMwAaAMD/LgCGAEgAlv/6/zoAUP8rAAUAtf+2/y3/+P+O/xYAkQDf/0oAOgBHAOL/KACHANv/LQAqADgAEgDC/yAA2/8YAFgAJwA0ADIARQCm/5z/CwDR/3T/Iv96/3f/3v+OAGAAJADE/wkACAA5AIgA6v8eAAwANACeAEIAPADS/6r/tP/S/1AA7v+j/8r/wv/B/+T/1P/x/1IANgAsAD0AVgAyAO3/LgAoAOH/EAA8ABYA3v/A/7n/uP/k/77/AQBEAMz/DgAGAAgAKQAAAEUA3P/a/xsADAAcADUASgCk/8b/BACs/87/7v9mAGgA3P+6/woAGgDU/+z/AgBiAHsARAB+AE0ABgDa/4X/Mv8i/8T/7v9M/0f/sP8cAGIA5ABnAYAApP+fALYBlADX/4AAFAB+/yP/ZP+q/0f/Kv9B/6T/0/8GABQAIgBTACcALAAeABcAFwA0ADsAyP89ALIANAD+/zIA/v8O/9L+UgBVAeL/yv+IADP/QP9y/0//1v+4/xIAegDEAOoAJAHIACIAcAAcAP3/FAAOABcArP+P/17/VP8o/+f+Vv/s/zQAGABzANYAaAADAAIANgAAANz/kgCSACMA///l/5b/aP83AFIA8P+Y/4j/+v9z/33/1v9Y/xj/5v/oAJMATgBjANIA9gAeACgAegBAAOT/5P88AC4Atv+0/wwAvv/L/xAA6//b/xEAcgAZAHD/Yf+I/zn/GP+s/6P/0P7q/tj/ZwDaAAgB8gD6AEIBXAHHAHQAZABCABQAov+C/7r/b/8c/0P/KP9z/5z/Mf+K/8z/yP8aACQA3f/M/7//0P8gAB0AaAD+AE4BJQF+AEQAhACAAEMAMgAqAAMAiv9N/4b/kP+E/0b/IP8E/yz/yv/S/8L/yP/J//j/2P8EAKwAEQG6ACoAmACQATcB7v/q/4QAWgDM/3D/nP+i/x7/7/5g/1X/Mv9f/2D/zP///9T/2P9lADoBMQFEAAAAPgFhATsA+P9TAJ0ACgBS/3r/6f+j/yD/UP9b/1L/dv+p/9j/tP/A/+r/WgDDAMEAZAAgAMwAHQFsAPj/OABgAOT/Ov96/y0A+v98/2r/0//0/7L/pv/b/yQAHQDC/27/wv/6/77/hv/M/6IAmgAbAGwAFAHhADYADABiAKQAAACF/8T/4P+a/0D/Wf+c/47/Tf94/+b/OABKAPz/NABuAEgAQgA8AHwAhwApANT/1v8eAAIAzP/u/+b/8P8CANf/6f/2/xIA4v+2/wYA/v/+/wgAKgAqAPb/FQAcAD8AHwDo//z/DQAwAPf/4v/4/8T/gv9m/7j//v8KABwAOgBLAGIAfABOAEgAYgAcAPT/EAD9/9z/1P/K/9X/3P/M/+T/9P/g/xAAEgAJACoAAAAGAOL/4v8kAOX/2//6/woABADy/wQA8f/f/+r/LAA2AAcA6f/+/zYAGwAQABQAKABCABoAFAAbAB4ACQDd/9H/2v/I/3z/UP9o/6z/xP+U//j/hAByACYAUgDZAMUAZABGAFYAUADy/7X/wv/Y/7T/gv+M/7D/2v/k/+r/HAAuAB4AGwAkAEAAGADM/8b/0v/X/8D/p//E/9j/3P/o//L/FAAuAFQAiAByAEYAWwCEAE8ACgAKAAgA8f/Q/7b/pP+v/67/oP+m/7L/x//Q/9z/7v8TABYAFgAIAPH/DwAGABQAIQBUAH0ADgAUAGYAXgA8ABIAFwAiAO7/zv/f/+L/yP+O/5b/s/+q/7T/tv/Y/9P/wP/k/wAAOgBwAG0AQgA/AHYAcgAgAA4AJAAOAPv/1P/g//L/xv++/7r/wP/M/8j/5P/f/9L/7v/y/wQA+v/u//b//P82ACcAIQBGAEQAQAAUABkAEgDo/9r/wv/U/9X/yP/Y//H//v/8//7/CQAgAA8AAwAjACMAKwA8AB4ACQADAOL/tv+g/5D/qv/I/7z/2P/s/wgADgAhAH0AcgA4ADoAXgA8AAgADgDy/wIA8v/A/+f/8v/z//X/5////xQAEAAEABgAPAAuAMb/ef+6//z/tv+E//7/UQAuAAEABAB0AIwAFgDs/0EAYgARANj/4v/8/9T/oP+v/+L//v/l/+b/DgBFADYAGgA1AD4AJQDO/8r/5P+m/5T/of+6/8D/uP+8/+b/RgBwAGgAdwB8AI4AkgBSAGQAUwAGAAgAxv/C/+b/t/+y/6z/vP/G/6b/vv/m//X/6P/s/wMA///4/93/8v8SAPr/9f8aAHAAdAA6AE4AXQBPAEIAKgARAA8AIADk/7n/wP+s/5L/SP9S/5b/e/9+/87/2//s/0MAGAAhAJIAmwCGAGgAgQCZAG4AJAAKABUA2P/E/4z/cP+m/3T/TP8//4b/rv+N/+L/IACgAO0AjgB0AJgAuQByAEsAVgArAPD/lv+Q/5b/aP9a/2H/gP+c/4T/kf/a//j/IABWAL4A7gCQAFsAjgCmAD4ADQAiACcAAwCg/43/rP+e/2z/Xf9y/5D/iP+S/9r/6f8FABEAUgDGAKIAkgCIAH0AqABRADIASAAQAO7/oP/B/8z/gP+H/0L/bv/E/6L/iv/G/2AAIgCG/xcA6ACtAGIAfAB+AIgAagBSADUAAAAAAAIA2v/P/63/eP9y/1j/jP+m/47/4P/Q/8b/+P/O/xsAsAC0AFYATACwALYAZwAwACIANwAEAKP/uv/c/6L/ff98/5n/jP9j/7T/+//Z/+D/7P8KAHAApgCFAGwAigB0AGgAegAWAAYAPgD8/+r/x/+X/7r/gf9i/23/bf/S/8X/kP/k/9z/9P9NAGAAtAB4AFQAwgB2AGEARAAkAFAA6f8AAAQAnP+o/6P/lf9m/1b/nf+8/+7/yf+O/+T/+v8GADkATQCeAMUAagB2AMAAfgBKABYAEwAwAPL/uP+I/57/kv+u/6j/DP92/8v/gf+5/8L/5/8iAOT/DgDGAM4AkQC4AFIAXgCtACgAEADy/zEAQABA/1r/aP9w/5z/OP+g/5D/nP/p/8L/GgAnAF8A6gDKALIA+AB+AEkAfAD2/+T/nP+h/6L/HP9I/xD/TP9G/yr/vP/E//z/FgC0AAQBhQCyAO8A1ABmAFQAXgAKAMb/uP+g/1b/ef9I/xD/W/8E/0n/3P/C/ywApAAGAfwAxgDCAJwAtgBEABIAIAC1/77/q/98/0f/R/9o//3+Mv/S//r/CABIALAAmwCsALkAmgCiADgAJgDY/8b///+i/7z/WP8e/yr/2v4k/4L/JgDaABgBBQGnAGIAiACQAAUA9P8IANz/DACW/0//TP/x/h//KP9y/8L/MgAAAegAvgARAfsAoACYACcADAAoAOL/xP9W/2X/PP/Q/r7+gP7M/k3/2/+dADQBXwEYAWYBlAGwAGoAQwDu/wYA0v+W/yf/Jv/A/hb+j/6x/ib/3v+UADgBvgAUAWcBMAE+AZ0AdgCSALAARgC+/6j/5P6v/iL+Uv7s/rL+rP8oAMsAdAECAbkADgEQAXYAmgBSAEoANwDC/9P/XP8p/9H+XP6e/kD/wP9GAPAA8QD/ALgAkADOAF0AkACKAI4AwQD//87/6/4h/hD+Ev7S/g3/0P+/AGQBcQH9AGoB2QB+APgAjgBCAEMAqP/h/iL/yf4F/jD+QP7T/gb/qwBSAlYBIALoAe4ASAHAAG8AQgBgAGD/LP9x//z96f0t/nr+sP74/oQAYgE8ARQBwAF+AQkB7gAzALsABgCG/wEAU//S/r799f0o/nL+ov8mAGoBkAH7AdUB7ABGAaAApQBwADMAKAD+/sL+4v2Q/cb9sf3p/rL/TgHoAdsBaQJ5AXoBTgFWAdgAFABwAKz/ff/i/hT+Vv0B/Tr9xP3g/7QAOAExAboBcAKVARgCHgHaADwBUABYAFL/5v4B/i792/3k/Rz/FwCgALwAoACvATkBIQFeAaIA2AB6AB4AAAA8/5b+Pv4S/hr+2/6y/4QAkgCWAJQBOgGwAKgAagBiAHsAGAHYALj/a/5I/kb+jf37/u3/egDyAIwAGAGcAI4AvACIAL4AgACVABgADABq/4j+9P5P/mD+av9fABUBoACmABUBiAAdAFQAIQB9ALIAAQBmAAgAjP5x/uz+kv6K/m4AdgH1APIASgFOAQIA+v+iADMAiwAyAK7/v/8I//b94v2Y/kH+Rv9SAVABkAC+AB4CFgGg/woB9ABTABUA4f/e/wj/kP7//UL+VP54/oAAxwEzAXQABgIeAtD/FgA6Ad4A9P8sACwAIP+w/ij+OP5X/kr+0/94AVIBTACeAb4CcQCy/1IBPwHa/77/PwBS/5T+fP4W/h7+SP6H/1gBIAFQAG4BsQLGAIj/+AAEAUUA6v/W/zj/yv7m/iT+9v1a/o7/OwFkAY0AjgEAA7YA0/6F/wIBcgFl/1v/q/+i/pD+Cv5x/mf/UgBQAW8AEAA8AmYCBAA4ATkCpP+e/5b/NP6u/g7/2P/o/ob94P6O/7IAQwGaAaAD2AFi/7IALAEaAMD/PABs/1D+vf6Q/vL94f3A/6QBqgDU/7IBVgPuAGT/YgGqAar/6v6o/wL/6P1y/vD+Mv6G/qoAcgEkAHwACgNsAsz/SgCcAbMAQv9b/4T/mP5O/pT++P0q/kwATwH0//D/gALeAkIA+P+jAdgAtP+2/zT/AP8y/wT/kP2C/Z8AjgGO/+D/igLeAtf/RP+sAWgB4v+w/6f/5v4G//n+Vv0k/rgARgF1/+D/DwNqAlz/rv+xAUYBtP8G/zb/XP96/6b+M/1W/4wBRgAi/9cAiAPuAXH/qgBmASQABf/u/ir/mP9u/8D9Mv7WACoBRv+M/9wC5wK4/+7/VwGwAFn/nv7y/kr/tP9c/r79fAAZAWb/XP84At0CNACQAO0AJQDn/6z+eP5M/8L/a/5m/u4AzACt/0AA8AHXARQAvwDuAAwAnv9u/pL+Vv/e/l3+MP/7AJMAWv/oADoC6AAoAAQBiACy/4r/tP68/ob/8f6U/vb/mgD6/8z/0AGEAlwA5wD0ALj/q/+3/vf+pv8Y/wL+x/7ZAGAAw//0AI4C5AFOAHoA6f8cAKD/sP0z/xwAQv5p/k4AmABm/yoAfAL+ATYA9AC2AKL/vv9o/oH+EAC9/qD92/8SAZb/7v9WAlcCegDMAPIAx//a//H+GP6q/23/tP00/wcBAACW/9sBiAJvAKoACQG5/8r/FP8w/nv/vv8m/vz+5ADr/0T/sAHUAp4AoQCEAcb/iP9C/03+eP+0/0b+sP6qABgABv9QAXcCrwCyAIEBEgCI/2T/Kv4y/9D/Ff7C/rkAzP8h/0UBbgIkAaIA+gAbANj/gf9B/oL/FgAD/q/+ngDR/zH/UgF8AsoAuABDAen/uv9O/3T+8v8bAOr9x/76AFr/sv6/AaAClgCfADUB3/+u/77+ZP6/AND/cP2w/4QBuv7m/rQCTgLs/7AA6AB0/4j/fv7a/mYBGv+W/R0BDQH1/R0AHgO3AM7/VQEdAGj/jP9w/hUAFAEI/s3+FAIDAO79AgIdA0r/PgB4AVX/Rv/K/pb+GAEOAI/9cAAUAlr+JP9aAx8BKP9uAUoA1v5K/yz+pf9YAVD+Wv40AkYAwv3+AQwDiv+oAFwB5v4X/+f+jf4AAQkAmv3pAMoB4P0cAGMDTQB7/8ABoP8q/n7/pP4UACIBCf57/1kCI//U/lwD4gHR/vcAgAAl/gz/vf5w/2gB/v7M/gICAQA4/nQC0wJU/5oA6ACG/sL+s/72/gcBX/8K/gYCGAHC/cYBjwOA/8b/mwHO/kn+1P68/m0B7v+b/SsB6QFk/roA1gPu/0j/ngEX//b92v6w/uoA0gDw/ZQATQKP/jQA0AMUAOT+twFk/5z9p/7a/tIAwwDr/S4ATgKq/rb/3gOoAJT+OwEi/yL9uP4W/+AA7QDw/RQAVgLB/g0AAgQ1AOL+nAFc/8T9jP7Y/tYAvADp/eX/KALu/jEA0ANcACT/bAEG/7b9vv7f/j4BMgHK/RQABAIl/nIAGATK/zb/rgH3/sT9Qv5q/q8BJAFE/TQA5AHr/Y4AQgQIAEH/4AHe/pv9WP5x/h4CQAFq/ZgA5wG8/Y4AVAS9/2//6AGq/of99/1n/hYCLAEq/T8AAALU/ZQAVgS+/1b/mwHs/gr+gf0K/qMCbAFs/boA7wHD/eAAXwQUAPb/YAHR/kL+kv1A/rYClgGS/XcAlAEm/gwB+gMPANL/VgHh/gf+Vv1i/soCMAGV/aYARQH2/eIAAAREAD3/MgFm/xT+W/00/mcCOgHX/X4AJQER/sEADARgABb/GQFe//L9VP0m/lICcgHk/UUAPAEh/k4AIATRAM7+/wCS/xn+tP3+/esB4wE4/hgAWgH+/cz/DARCAcr+2gD4/1/+3P12/T8BRQKg/uD/agFK/iH/4wMzAgT/pADy/5z+Cv4p/e8AmgLs/oX/eAFo/rb+igN5AlD/QAAMAAL/Uv4q/WAArAIo/zf/TgF7/nL+SAP8An7/+v81AOD+Nv4e/RMA+AJm///+RgHU/jb+5gKgA8n/x/8yANr+XP5w/dn/zALS/yj/aAHK/tb9mQLmAzYA0f8MAK7+iv5c/Zj/rALf/wL/QQEd/679LgK0AywA0f8yALD+fv5u/UL/kAL9/wr/UwFO/4j90gHSA0QAqv8AAM7+3P62/QT/hwJaANz+VAHF/7b9TAG8A4YArv84AKb+xv6o/cr+XAI0AKv+kQF3AKn94ACWAxYB/P8aAMT+iv71/cj+RgKYAF7+XgFqAKT9qgB6A0kBwv8aANH+Zv74/dr+TgKkAD/+CgHGAOb9VQCtA64Bov8AAPP+WP7u/b7+PAKZABj+BgHSAOb9HACOA5sB1P9xANX+Cf7n/fT+WAK/APv9cQDuAFz+NgBpA5EBAACTAOb+uv30/Wz/bgLAAPT9jgAmAXr+NgBSA24Bv/9uAMT+n/3m/Vj/owL7AMD9MQAOAdb+dADQAgUB8P+AAL7+ff28/Zz/ygLyALj9CADxAOf+qgDHArwAzP+cAJr+Pf2S/aj/BAO2ALH9FgC9ADj/TAHRAnUA4P+MAI/+8PxY/TsAMgNZAIT9OACUAFP/CAKUAub/1/+4AI/+mfyc/QMBegO4/3H9cwCSAOz/LgI4AoL/FACOAP79vfzW/fUBdQPQ/pj9hgAvAHQA4gJzAQb/TgCKAKD9c/yb/r4CxgIU/mb+sADY/wEB3wLiAPj+5gBoAAL9oPy0/2gDigFz/Qz/vACm/0gBtgICAOj+ngBG/3b82vzaANkDiACP/dD/nwDl/wUCagJk/77/cgHT/s77Xf3mAUcDYv+8/WIAYwAqALQClQGz/mYAMgHG/bD7QP4AA7kCX/5k/swAKgCjAP4CwgBR/tYA9AAa/bL7Yv/AA5MBfP0a/xIB9v9wAesCXP+4/ogBBgBn/B/81wAaBD4ALP36/wYB8P9OAm4Chv5G/+IBVv/L+678CgLpA9z+If3FAJYAuv/+AugBQf5gANABE/65+yD+8gIkA+D97/1KAcj/LABgA8cA5P0hAU0BRv1E/Dr/qwP+AVH9QP9gAVL/5gBEA5L/bv6EAVIA3vyC/GQAtgOcAFj9RgA8Afj+sgHIAtf+lP+eAQL/qfx7/UcBJANK//v9OgGAAB7/XgJTAu7+JwBCAVb+z/xb/uwBgAJr/uH+lAF9/2//mAKMAdz+jgAQAeL9+vwC/zICoAH7/Wn/jAFK/4r/lAINAQr/yABiAOL9YP1x/wAC4gA4/uH/TwEi/+7/ZAIwAaz/eQAQACj+D/54/2YBnAB7/mwA/wD8/ub/1gFoAdP/GgBGAJ3+PP6s//gASgDB/iEA/wAU/2j/agGeASUAGACoANf+Rv5i/1AAuwD2/qD/ywHK/5z+nQDOAfYAKwAyAC7/uP40/57/uABo/67+pQHoAIP+yf9sAaYBZgCj/9D/bP84/yz/SgBcAIL+TgDTAaL/+v5KACgClgEi/0//1P9S/w//eP+nAIj/3/5sASABlv4U/6cBRgLv//X+IgDy/y7/4f7a/7UAlf7T/7ICxf/0/UgANAKGASb/Jf9OANb/vP7c/iEBOgDP/QcBbgIO/2f+jgBnAkMBKP+F/woAev+Q/pv/WAFQ/y7+BAL0AZL9ZP4+AVICHAEY/2r/cv9i/7b+lP9UAZ7+tv7nAmgBcP21/vQBrgKQAB7/nv8kAKX/RP4sAHoBQP4F/0wD5QDc/O7+JQKqAkoA4f5w/wQAiP9E/ncA+gC9/bT/uwNcAH38av+CAnwCQgDQ/lL/CwB2/yz+wAD9AHj9CgCuAwoAp/zA/+UCKwLA/8r+ZP8IAO7+HP5jAcgAJv3wAAAEwv46/IIALQMSAsb/vP5Q/xgA0v5G/o4B9P/o/MwB3QPW/Vz8ggFYA0cBOv/G/q3/GgA6/kX+rwFa/zT9wgK6AwP9tfxkAg4DFQFu/9r+rv8GAFP+2v78AZL+eP3gA9oCmvtO/UQD4AKoADT/5/7C/wkApP58/9MB9v0O/noESgHk+oD+cgOXAmoAPP8K//X/GAA8/t7/wgF6/Ub/WwVNAPL6zv+OAycCNwDj/ur+BADp/x/+QAAiARv91AAABTL+vPpBARAEKgHN//7+/P44AJL/BP6PALkAc/0eAmAEcPx++40ChAPcAIT/8v5Y/zIAS/9P/l4BzP+x/XIDmgJ6+4T98AMHAx4Arv/J/kD/NwCK/tz+oAEY/yb/FgQ0AA77/f+KBMwBwv/Y/7v+kf/7//D9pP9YAY7+7gCuA6D9wPtIAioEdgCm/5z/mP74/3L/5v02AAwAl/7VAkoCkvsW/i8EfwK8/woAhv/q/iIA9P4r/rsAXv/G/7ADNP8g+9wAlgQqAR//RQBa/27/EAB6/cr+MAHs/iABgwNJ/fr73gIJBM7/M/9cAGD/FgCK/0D9zf/QAMz+GgKEAgb8PP3yA2YDUP9c/0QAOv9EAEj/E/0eAHAA6P6oAsoBC/wI/jAEEgN8/gT/rAA9AFAA5/5W/QUAgADQ/hkCtgHm/CX+ZwNKA1L/m/9yAOj/LgDU/pr9oP+yADL/rACxAdT+4/1rAXYDfAAn/7AAQQC+/8D/Zv64/tX/sv/0/wABjgAo/pL/1AI6Aqr/dv+0AN7/lv81/37+dP8cALH/XAC3AWz/6/0dAVAD5gC1/kUATAAI/w7/Pv9n/4j/pv9EAPIBkgBi/YD/1ANfAoP+i//SACD/tP7G/4j/Cv9i/woA8gEPAT79K/6UA2wDdP7t/uYAVf+c/q7/ov/8/jr/EgBDAl4B+fze/ZkDswPg/tb+7gCo/5D+yP94AE//NP7V/9ACXgHU/Mj9bANWA0z/Kv9gAHn/rP7m/2YAHv9b/kQA8QLYAMj88P6AA0wCOP+u/7gAXP/c/vL/DADO/gr+0AAKA87/vfz0/74DogE1/0gAhADS/mf+w/82ANz+dv4oAYQC3P7q/A4BSAOzALb/oAA0AKD+WP7g/y8AoP7U/nABrAFc/qr9tgH4Aub/Xf/rAG4Atv6d/hAAPwAA/1r/mgEmAfj9bf4wAnwCsf9f/8kAYgDC/rr+rv+y/0T/yv+RAaAA5P0Q/y8C7gGA/5D/wABwAFD/yP5+/9b/uv83APMAWABG/hr/MgKsAYz/eP+BAMoAdP/0/pH/jv9K/8r/5gBuANb+Ev9DAQ8CNgBE/2oA5gDr/0L/a/+G/5//tf/m/1MABABM/w4ASAH3ABYA6/+SAJEAjv9D/3T/h/+i/2P/cP8IAGAAKQDq/7AAGgF1AEAAPgAeANr/sP/c/6L/Rf/8/kL/LwB0AC0A8P90APwAlgAcANH/IABAAPz/kv/m/gb/Sv92/zAA0gBvAJr/DAAAAfYAZgD4/xUAXgDK/wj/Cf9E/2j/qv+LAPQAFgCU/1oAVAH0AEoAPgAmAPT/Lv/H/kn/bv8n/8v/FgHjAJT/cv/PAJgB5gAsABYAAAB+/w7/I/9S/x7/M/9wAEABKAAm/yIAoAF2AXYAIQDm/4f/FP8j/57/K/8A/zYAOQGKADT/lv9AAacBpAAbAB4Arf/6/hD/jP8w/+D+0P8eAbgAdv+2/+gAegHsAGYAIgCM/xT/HP/E/5r/5v7K/ykB3ABx/27/4gBIAbcAcABYAP//Hv/R/nr/fP/0/qj/AAHUAIj/cP/YAHwBwABMAFYADABk/9z+IP9r/xP/r/+2AHYAnv+i/9QAagHKACoANAAsAHj/JP9G/x//FP/I/6YAdACW/7v/1ABEAcYAVAA/ABgAqv9O/yb/zf7G/tP/4gB+AGP/kv/SAGYB+gBhAEEAOADQ/2b/Q/8I/73+Mv+KAPwAxf89/y4ARgFAAYAAWABKABIAk/9a/1T/4P7c/pT/yQC+AJn/lv+wAGQB4gBUAFQAdgAOAGb/SP8U/67+p/6Q/9AAvgDE/8n//ACOAQgBVwAuAFAAyv9O/0L/4P6B/t7++f/1AMsAvv+2/9wAfwE6AXgABAD5/9//kf8E/4L+Yv7m/g4AAAHMANL/sP/SAIIBIgF/AOj/zf/O/0f/vP5o/nD+Kv9QABQBvgDI/8z//ACeASABUQDu//T/uv81/7b+hP6c/j7/jwAcAXcAr/8CAFgBtwEMAUYA3v/W/27/6P6q/pL+2f7c/+YAAgEbAKz/rACeAZkB4QAuAOD/aP/4/rr+of68/jD/SAAbAeMACADh/9wAhAFcAbQAEACp/1D/5v6G/oz+z/6L/5QAIAG2AO7/RABaAZAB8gBdAOz/fP/t/oz+gf62/hT/3P/wABYBXAAAAL0AkAFcAcQAMgDM/0P/nv6C/pH+0P5n/0UA8wCkAAQAZgA3AUIB/ACkABYAtP82/6H+fP6a/h7/HgCvAHoAFwBgACABMwHmAJgAOgAYALL/AP9//mr+5P6y/1oAjABKAFAA5wAgAQwBxgB1AGAABACC//D+df5A/sj+8P+eAKIAOABQAOkAJgEDAcIAawD3/+7/uv8C/1j+GP61/uL/2gD6AE0A9P/FAIYBOAGUACwAIADc/3T/KP+U/iT+bf6w/+YAFQGHAPb/bQB8AbMB4gAmAOL/1P+W//j+Zv4w/pX+yP86AWUBXAC8/3YAmgGSAbwA9f+i/4T/VP/w/nj+bv7e/iMAKgH6ACwAwP+uAJQBdQGvAMr/Xv88/yz/xf5k/pD+RP+uAHAB5AAnAA4ADgG8ATgBXwCQ/yj/IP/0/q3+pv7t/tr/AgEEAS8AAAC2AGgBaAG+AAgAcv8V/+b+tP6z/ur+pP+TANwAkgA0AGQAJgFaAQYBbACH/zT/7P6w/s7+Av+S/zwAtgCgAEgAeQAEASoB8QCUABEAcP+8/oD+rv4s/6b//f9UAFYARACHAAABHAHtALwAXACk//f+lP6c/iL/nf8WAGIANAAWAHkADQEgAc4AuAC+ACUALv+a/nz+1v5P/77/SQBcABwAWgAKAUoB4ACrAK8AaACh/97+m/7C/kr/wf8tAFoA/f8QANwAFAHWANoAqgB4AP7/MP/G/sT+BP+Z/w4ABQDS/9j/gADkAK4AkwCiAKQAQQCg/x7/6P70/nj/6P/o/9f/6P93ALAAiAB1AIcA1AC9AEgApv82//j+yP4Q/6L/QgBQAPj/HQB0AJ4AfwB9ALQAswB7APT/Tv/V/rj+EP+I/wUAPgDx//T/hwDiAK8AcgCUAI4AagA4AKr/Qv8A/wb/L/8y/6P/NgBUAEQAeADUAAQBtABQAEIAMAACAKz/fv9u/zH/9P70/j7/4/+aAM4AugCWAMAA7QCYACoA3v/e//D/oP80/wT/5v4C/4T/SgAQAfgAXgBqANkA1ABlAAYACAAtAN7/XP/1/tv+7/4a/7z/hAAKAeQAZQBMAKIA2wB5AA4A8P/t/8j/Of+2/rT+9v5U/w0A0wAGAbQATwCSABwB1ABeAA8A5f/2/5L/8P6M/qD+Av+e/2gA2gDWAG4AXgDXAO8AxgBlAOL/4v/S/z7/qP6A/sX+Xf8YANAA+gB2AFEAuAAeAdoAiQBuACYA/P9g/8H+aP5+/vj+qv+lAOoAogBEAHwA9ADHAKAAfABcAAoAdv/q/lL+Vv7c/or/ZgDPALQATgBMAMYA3wDsANAAdQBAAOT/QP+V/lj+mf4r////ugDsAHUAIQCKANYAyACqAFoAQgAfAIr/8v6T/rv+Fv+a/3wA4gChAEgAXgC6AMAApgBaABoAEAC4/0T/1v6m/vD+b/8eAKwAogBdAGgAugDIAG4AOQAvADsAKAC0/zL/zP7C/hb/fP8BAJsA3ACwAHAAbQBpAC4AGgAcAEQAbQAWAJT/Jf/s/vf+MP+c/wQAcADNAOsAtQBdAFwAVAA4ACQAAgAKAPT/9v/W/4L/Xv9H/1z/bv+k/xIAigDOAIwAaABwAE4AKgAOAPT/9/8UABgAJgD//5z/YP9U/2r/bf+G/9r/ZgDUAKgASgAWAD0AagAvACIAIgAWADIACwDU/5H/UP86/zL/Wv+B/6T/9P9oAN8A6gB+ADwAZAB/AF4ATQAmAA4ACADI/5H/WP8w/yD/Mf9p/4T/pv/1/7QAPAH2AIAAYgCLAIIATgAHAO7//P/m/6T/Qv8O/wT/JP9c/37/yP9oABQBJQGfAGwAkQCoAIwAKgALABAAAAC9/yz/uv6u/vD+LP98/wkAzwAuAcoAnACnAJEAlgBsACwAIQAZAOz/b//Y/rL++f4k/zf/tv+hABcBygCLAIkApgCcAGoAXgBCADYA9v9w/wL/sP6//gz/VP/W/3kA1ACqAGkAjACtAJgAlwB8AHYAaADv/27/4f6e/uj+Iv9e/+v/ngDfAI4AYABeAHoAnAB1AHIAdABCAOz/Uv/1/rv+zv5I/57/PADaAN4AfgBCAGEAcwB1AHYAhwCCAFUABgB2//r+u/7W/kD/tv8hAI0AuABlADoAXACAAIsAZwCIAI4ANgAiAN7/bf8q//7+/f4R/3r/vv/x/3wAtQCrAF4AIABPAHAALgD9/0IAeABcAB8A//+g/3D/cP8S/xr/Uv+O/9r/NgCUAJoAeAA2AE0AcgBUADgAKABFADsAEADl/9T/xv+v/6j/p/+8/5n/g/+f/5z/sv/c/yIAowDFAHAARgA+AEwAWgA4ABYAGgAsACAAAgDK/5T/i/+k/6D/fv+P/47/lv/E/7b/1v80AKoA/ACzAIwAiQBsAHEAMADl/9T/AAD+/8T/pv9w/1z/Uv9X/23/gf+o/9b////3/yIAkgDzAAEBpACDAIoAggBrAB0A9P/g/8r/p/9Y/zH/Ov9Q/2X/dv+0//v/CgAcAB0AJgBGAEoAqQAMAfgAtgB4AFQAIADm/6D/bv9q/2H/bP9x/2j/bP9+/63/qP+c/+T/KgB6AO0AMAEMAbgAmACRAGsAKADJ/5r/pv+S/27/Rv9C/07/TP9N/1r/lv/g/1YA+ABAAesApgC6AMYAlgAoAOX/5P8EAOz/kf94/0z/HP84/zb/bv+t//z/tgDUADUA4f9WAKQAcABSACoAWgBQAOb/8P/t/87/xf/G/8r/tP+r/9D/+v/5/w4AEAAKACkAJAAYABgADgAgACYAAgAwAH4AhQA0ANj/2f/i/8b/sv/G/9P/6v/l/8P/1P/D/93/4//M/+z/7v8gAFUAZgBsADMADAANAA4AJgAgAOz/3v/m/+j/xv+M/5T/r/++/87/7v9HAG8AWAA2AC4ASgA4AAwACAAIAOb/6v/m/9r/5v/R/8n/4f/0/+b/8P8BAAMALAAyABYA+v/7/xMAFAD4/+7/+f/9/w0A7v/k/wAA+P/o/+X/+P/6/wYADwAQADcASAA2AAwAAAASAPj/5v/4/wAA6f/Y/97/4//e/+L/6v/i/+z/BAAiAC8AHwAkACIAGAAUAA4ACQD7//n/8f/x/+z/0f/l/+n/6v8OABYADwD6/w8AHgAMAAwA/P8iACAA9v/4//L/EAAKAA4AFAAAAPz/2v/w//D/6v/3/97/9v/w/wwAFwDq/xgACAAEACQA8P/S/8b/9f8KAOD/0//a/wwABgD2/wIAEQAXAOr/9v8AAAoAAQDr//f/3f/0/+r/5v/9/+X/AAAEAA4AFgASACgAFgD4//j/GAASAP7/EAAGAPb/6//r/wgA9v/y/wwABQAMAAQA9f8QACIAFAD6//b/BwADAP7/FAD8/97/6P/R//b/BgDj/x4ALAAUAPD/3P8WAA8A9v8CAPz/AQAVABkABgDm/9L/3P8BAAsACwAVAAwACgD+/+z//v/5/wQAMgAXABAA/P/s/yEA/v8FAAoA+v8QAN7/8v/1/+j/IwAqABIA9v/u/xIALwAAAOb/9v/w/wIACAD6/+7/AQAUAPj/6v8UAAwA0f/2/yEAKgAiAPT/7P8OAAgA9P8NAOz/7P8MAP7/8v/g/wQAEgAQAPX/7f8cAAgAEAADAOr/AgD6//L/DAADANz/AADw/+b/GQALABAA8v8GABQA3//u//n/HAD5/+z/9P/w/xwA7v8aACIAAwANAOT/9v/s/xIAHgAHAB8ABwAcAP3/4v8FAO//9P8GAPf/7/8CAP7/8P8CAPj/EgADANv/GgAoACIAEADi//j/8//q/xgAUgBFAAwA3v+Y/8H/Zv/3/yABlP/4/5cASP93ADQAwP8zALD/IgDa/wAAAgDS/x4A0P8oAOr/CgAqANb/LwDW/+z/OgASAB8AEgAPAAoA1v/H/97/3v/o/+3/8P/1//T/+v/4//v/+//8//3//P/+//3//v/9//7//v/+/wAAAAAlAA4A5/8AAPz/LgAaAOD/+P/w/xsA9v/K/wgA+f/5/wIA+P8CAO//0//2/xgA7P/v/xQAEAAqAAYAAgAmAOz/FwAGAOP/AAD1/xMAwP/y/zYA6P8UAAAA7f/g/+b/JAD9/w8ACQD4/yYAGgAQAPL/6v/3//L/+f8KABoA8v8AADoAHQD+/wgAAAAEAOz/yP/r//j/8v8SABUA+P/w/wYAFgD8/+7/6v/8/zQAFQABAPb/7v8QAAgA+P/7/yEA///g//L/7f/2/xQAJgD0//z/8P/M/+r/8/8MAAYASAA3ALb/KgA+AOT/BgD2/wQAHAD4/9r/4v8QACgAAgD2//T/2f/6//D/FwAgAAEAMwD6//r/KAA6ABYAGACgAGIAgv+w/sr/zgCu/zwADQBq/14A0f8aABwAuP8zANL/AwDq/0AAMgDG/x8A0P88APX/u/8jAM3/3P/y/x4ARgBUAEIAUABCAM//4P/W/6j/yv/Q/wwAQAADACwAMgAMABsA3P/z//L/CAAQAOL/9P/a/ywAEAD8/1AADgASAPL/0/8KAEYAaAAEABAA+v+9/wYADABIAPD/tP/6/6X/0v/K/4z/Wf97/yQAPgCVAIMAlgDGACAAUQDT/yv/DgBaABgAFgCz/3b/TgDhAMIAlQD+/3f+XvyR/SwCOAROAcL/kv8+/dz+ygEuAm4Aov4y/3L/KAFqAV8AvP+0/rn/JAAdAekAQv+I/5X/oP8NAE8Ayv+s/5f/Yv8QAP7/9v8OAPX/CAD9/wEAAQD+/wMA/v89AH0AEAAaAAYAsv/u/9z/6P/0/+n/NgBgAEQA/P/A/9j/4P/i/+//7P/2//T/+P/6//r/+//8//z//v/7/wAA3v/s/xgAAABOAFYAMwDe/3T/qv+0//z/LgD2/6r/ov8sACwA2P/c/+H/rv+i/97/KQBiACIA6v/6/xwAfQBoAAwACgALAOH/xP8SAEMAAgDC/+b/IAANAD4ANQCw/4L/mv+x/9L/DwDK/5H/6/8GAFYAZAA6ACUA6P8iADIAGgAeABAA7P8RAOAAIgFMAMT/x/9a/2//2v+I/27/Tv92/+D/qv8dAD4A3P8eABYANAA5ADwARgAVAFwAVABaADgA/f9IAA0AAQDy/7T/8P/U/8T/4P+9/8b/wv+7//7/BQDM/9D/4f8CAA4ABAD4/9r/7/8QAEQATAA2AB8A/P8CAPL/AgAIAAAA8v/u//7/5P8OAP3/4v/u/9//FgAaAAgA6f/Q////6P/m/wwABAAFAA8AMgAtAAUACgAIABoAEgD0/////P8UABAA8f/5//j/+P/u/+L/7f/w//L/9//4/xIAAgD9/xQAHgAtAP7/OABOAKz/U//g/xwBegBo/yIAyP/C/zYAu//4/8T/y/8OAOb/AgDo/ygACAAUACYA5v85ACAAFgAIAP7/IAAFABAABgAGAAcAAgAEAAEABAAAAAIAAAACAAAAAAABAAAAAQAAAP//AAAAAAAA/v/////////////////+//7/DgAvABIA6v/0//T/9v/6//j/AQAkAAwA9v8lAA8A6f/u/+//9P/2//j/+f/6//v/AQAeAAUACAAeAOj/8v/2//L/+v/9/xgADAASAPP/9v8cAAwABADo/+7/+P8PAP7/6P/8//b/FAD4//L//P/+/wYABAATAPH/CgAYAAYA+//o//j/8//4//n/BwAaAAwADgAIAA8AEwD+/wAAAgD+/wAAAADy/+r/BgAHAAIA9P/s/woACADz//z/DAAGAAUABgACAAMAAADy/+v/+v8IAAgA9v/x//z/FAAUAP7/+v/s//T/9v/5/wYA9//4//z/CQAgABIAAgAQAAwA+P/8//7/BgD0/+j/AQACAAIACQAaABMACgAOAPT/EwAKAN7/AgD+//D///8HAAYAFAAcAPz/DAAYAAAA9P8GAAwA+v/+//L/9P8FAPb/6P/6/wsACAADAPr/CgAUAAUA+f/y//j/9//6/wMABwD8//j/+P/w/wQA/v/o//X/CgAKAPb/+v8JABMABgDw//b///8LAPn/6v/y//n/EQAEAO7//f8YABgA6v/o/wgAFAAMAPr/8v/6/w4AEAD+//L/7v/6/wQA7v/+/xIA9v/0//j/HAAgAP3/AAAEABQABgAAAP7/+P/9/+X/9v8IABQAEgDw/woA+v/m/wgA7P8AACIADgAFAPT//v/9/+v/7P/s//L/9f8AAAEABgAIAAQABQAEAP7/+v8CAO//5P/6//z/AAAKAPT/9v8QAPr/6//0//T/EAAQAO//+f8EAAgAAAD4/wQA/P/2//P/AgAOAAgAGgACAPT/CAD///j/+/8NABIA/v/+/wIABwD8//T/CgABAPr/CAAGAAsAFADw/9z/+v8EAA8A9P/o/woAAgD+/+T/6f8UABoA/P/w//v/+v8cAAoA+/8BAP//GAD1//L/+P8AAAQA3v/t/wMAEgAAAN//7P/2/+r/5//5/woAEgASABkACgD6/xAAEQD+/wEABgD7/wYA+v/2/wwA6v/k//r/+P/q//b/CADn//7/HAAMAPb/1P/w/w4A6v/c/+v/BAAOAPT/+P8YAB4AFgALAAcADwAAAPT/CAAGAP7/DAABAAQAEAD//wEA+P8BAAEA6/8GAPn/3v/u/wMADAAMAAAA/P8cAA4A/v///+//EgAbAAgAAgDu/wUACAD0/+7/9P8bAPL/6v8QAPP/BADw//b/EgD2//z///8JAA4AAAD6/w8ACgDo//D/CAAWAAcA8P/q//j/6//s/wgA+//q/wgAGgDi/+b/6P/i/w4AAQAOAP7/9P8XAPj/+v8OAAoA/P/q/wQAHAAQAAIA8P8GAA4A6P/e/97/+v/+/+j/9P/z//v/BAALABAABAACAPL//P8IAOr/6//y/wwA/v/p//b/8P8aAAoAAQAGAOD/+P/6/wQA+v/o/wQA9v/8//r/8v8DAAAA4v/m//r/6P8KABMA//8AAPL/BAAIAPv/7v/o/+7/+v8IAAUA/v/4//v/8P/q//b/DgAUAPL/+v8GAAwADADx/+z/9P8EAPz/CAAQABMAFADt//z/BAD+//b/6v8HAAMA7v/m//z/GQATAAwA+f/r//v/7v/1/xsABgACAPz/9v8JAPT/+P8OAAwAAAAAAPr//P8EAPD/+v8GAAQAAQD1////CAD+//b/7v/w//7/AAD+//b/9P8GAAkA6P/k//3/7v/4/wkA+P/q/+T/3P/y/xQACAAUABwAIAAqAA4ADQASAPb/6P/6/wQABgDy/+T/BgAQAPv/8v8CAPr/4v/y/+7/+v8OAAIADQDy/+r//v8LAA4A3v/q//z/7v/g/9///P/4//n/9P8QACoAEAAEAPD/CAASAPz/9v8AAA0A8P/k/+v/CQASAP7/AAD4/xYAFgAAABIA8P/5/wYA6v/6//j/6P/0/wcA/P/2//z/AwAIAPr/+P8DABIADgAIAPL/9v8ZAAAA/P8PAA4ADAD+//j/DAAEAOf/8//7//b/8//6/wIAAAD6/+z/7P/6/xIACADy/+//8P8CAPD/8P8LAPj/8f/0/wkACwDx/xAADgAFAAoA8P/2//D/+v8UAAkAEAAIAAAABADw//7/CAD3/+n/3P/u//D/7v8GAA0AFAAgACIAFgACAP7/CgAIAO7/7v8KAAwA+P/w/wIADgD2////BgANABgA+v8EABAAAADx//j/EAACAPr/8P///xAAAgAKAAIA+v8EAA4A/P/r//7/7/8AABAA9f8CAPr/AAAHAPT/AwAGABEABgD3/wAAEAAYAOT/7v8EAAMA+v/d//f///8OABAA7P8CABAA+v/s//b/FAAWAAcADAAOAA4AAgAEACIAEgAKAP///f8jABUAFAAQAAoABQDq/+z/2v/z/xsAEQAQAAwACAABAOj/9v8QAOz/5v/o/+z/7v/i//X/7v8BAAMACAATAAEACAAAABUACAAEAAoA+P8WAPn/9v8KAAwAAQDw/xIABwD2/+b/6v8OAPT/6P/y//7/9f/g/+r/+v8FAPb/9v/3/wQAFAD+/xIAFwAXABIA5v/4//n/3v/v//z/AgADAAQAEQAGAPr/DAAGAPr/3v/i/wgAAAAEAPz/CwAQAPz/EwAIABEAEgD8/xEAGQAUAAYAAQAPABAA+f/u/wYADgAGAOz/9v8SAPn//P8WAA4ABgD8/+P/4P/c/8H/wf/o/zIALAD8/xQACAAWAD4AOgA9ACQADwAQAAcA+P/z/+j/2P/i/9L/vv+8/8j/5f/8/ygALAAvADgAGgAkABUABwANAOb/7P8DAAYADAD9//H/4P/5/x4ACADw//j/GgAOAPT/8v/8/xYA8P/y/xAABAD///r/GAAeAP7/AgAAAOb/9v8EAPP/9//4//n/+//7//3/CgAVAAsAAgAAABEACwACAP//9v8QAAwA8f/+/w0ACgABAPv/AAAIAAQA+f8AAAgACwAGAPT//v8GAAAAAgD8/wIA8P/o/wAADgAIAOz/+v8MAAQAAgAIABgACAAEAAUABAAGAPj/+v/3/wwA+f/2/w4A7v/2/+f/+/8XAPz/BgD3//X/8v8GAAoA7f8JAP7/BgAIAOr////8//r/9v/x/wUA/P8IABgA/v8IAAwAAQAUAAgACgALAAgAEAD4//n/8v/y//b/8/8AAAIAEwAFAAQAEgACAPX//P8GAPz/AADy////BAD2/wMA9P8MAO7/5f8cAPf/+P/+//r/FgAEAPz/9v/4/woA+v/6//j/BgAcAPb/8P/8/wQAAQDq/wwADAD6//j/BAAOAAAABgDy/wYACgDt//n/AQAIAOj/8P8EAAIAEAD6/wEACQD4//T/8v/+/wQACwAPAAEA+f/6/wUAEAAGABIAHAD+/wUAAgD4//7/7P/4//z//P8CAPD/+v/6/wgAEwD+//r/AAD8//X/BwAQAAYA9v/o//T/7v/q/wIAAgAGAP//8P8KAAYACgD+/+T/AgD//wIA+P/l//j/8P/8////9v/+//z/DAARAAoABAD//w4A+f/w/wYA9P/+//3/9/8OAAgA/P/o/9T/2P/w/wcAGwAWAA4ADgD4/wUADwD4//L/CQAWAAgA+f/+/xcA/v/g//7/DAD2//D/9v/4//7/9//+//z/4v/g//T/FAALAPj//P8JACIAFgALABYAEQAbABAA9//3//T/BAAGAPj/9v/w//7/CgACAP7/BwAIAAYABgDw/+P/+P/6//L/+P/6/wgABAAGAA4AAgACAPX/AAAWAA4ADQAJAAQADgAEAPn/AgAIABQAFAAMAAoAAAAEAAwACADw/+b/8f/0/wAAAAD0//n/+f8EABgACgD+/wAA+/8IAAoACAAeAAwA9P/6//j/+//6//P//P8JAAgA9f/t/w0ADADy/wcAEgAAAPP/9v8KABQA/v/w/wwAEAD0//z/BAD4//r//P8GAAoA+v8MABoA/f/0/wQAAgD0/+7/9v/0/+b/8v/0//v/EAARABQAEgAIAAYACgAEAAQACAAHABAABAD2/wAA/P/y//L/+P8CAPz/BQAAAP7/DAD+/xAAGgAOAPv/9P8KAAYAAgD6/wEAAADu//j/BQASABUACgD+/wQAEAD+//D/AgARABAADQAIAPb/6v8CAP7/9P/9//T/AAD4//P/BwALAAUAAAACAP7/+P/7/wMAEQAKAPn//P/+//T/4P/s/wgADAD2//L/GAAOAAMACgACAAIA+P8DAAYAAAD0//z/GQD0//r/EgAFAAAA7f8CAA0A/f/5//r/CgAEAPX//P8IAAAAAAALAPj/+P/2/+z/BAAFAAUADAAGAAsA/v/+//T/7P/8//z//v/r//L/+P/c/+7//P/+//7/8f8SABQA5//i/+j/AAAKAAEAGQAHAAIACgD+/wwABgAEAPb//v8LAO7/+P8IAAYAAAACAA8A///4/wQAAAD2//L/AgDy/+D/CAD4//T/AgD1/wIA8f/8//3/AwAQAPL/9v/1/wMABADs//j/AgAOABQAAQABABAACQAHAAAA+v/+//j/BAACAPD/+P8GABAAAgDw//j/AAD4//D/AAAEAPH/7v8AAAgA9v/5/wQAAQD6//P/EwAMAOz/AgANAAoA9v/s//T/+v8XAAwAAQACAAMACwDs//X/BQAEAAAA9v/4//L/+//u//L/CQD4//L//P8IAPr/8v8OACIAAgD8/x8ADgD4//n/DgAQAAUAFgD+//b/EgAMAPr/5//q/wQADwARAAgA+v8KAAwABgABAPz//v/+//z/+P8AAP7//P/+//D/+f/5/woAHAAVABgADAAQABgABgAOAAwAAAAQAP7/8/8MAPn/1P+V/4j//P9LAEYAKAAOAPr/BAArACAABAAIABgAEwD2//j/7v/h/+z/9P8FAOz/6v/+//b//P/s//b/CADy//P/BwAGAPz/8f8DAAgAAgAAAPv/BQDw//7//P/y/xAADAAWAPX/7/8JAAAA/v/w/wQACAANABMA+v8JAPz/AAAMAPT//P/7/wIADAD5/woAGAATAAkA+////wIAHQAUAPD/+P/4//j//P/6//z/BQAYAAkA6v/s//7/CwAGAPz/EAAWABYADAAEAAUA9f/4/wEAGwAUAAgADwAHABcABAD4/wYADQASAP7/9v/4//v/8f/6/wgA/P8GAPb/BAAcAAAA///w//T/+P/q/woAAgD6//P//P8KAOL/7/8IAPb/8/8MABYA+v/0/wcAFgAGAPb/AAD6/+7/7v/v//b/AgD///j/+P///wQAAgD6//7/BAACAPr/4P/s/wYAAgD6//r/EAAGAP3/BQAAAAIA8v/6/wIA9P/6/wQACgD0/+7//v8GAPr/6v8IAA8A+v8KABgAEgAKAAIABAAHAP3/AAASAAIA+/8IAPz/9P/w/+7/7v/0//L/5f///wAA/v8CAO7/9f/u/+D/6v8IABQAKAAhAAIAFgDw//L//P/Y/wEADAAEAP//CAAOAPb/+f8GAA4AAQACAPT/4P8CAAQA+P/q/+r/DwAUAAQA+v8BAAoA/P/s//v/FAD8/+T/BgASAPn//P8MABcABgD0//T/AAAPAAIA+//2//v/DQAKAPn/4f/0/wcABAD2/+z/BgAKAAYAAAD6/w4ADgD//wgAGQAQAPP//P8KAPr/8v/0/wYA+P/k//f/BwAKAPf//v8GAAIAAADw/wEAAAD9/wcACAAQAPX/8v/8//L/5f/o/wgA/P8LABMA9f8RAAIA9v8IAP7/HwAhAAwA+v/7/xAA9P/m/+b/7f/6//P/CAAIAO7/9P8IAA0A/P8CAAYACgATAPz//v8IAAYADAAFAPj/DAAUAPj/8//1//L/7P/h/+T/9f/9/wUADAAJAAUAAAAMABQA/f/y/wgAEAAQABIA/v/6//j/9v8BAP3//P8HAP7//f8KAPn/+f/+//b/9v/i//T/EAAAAPX/7f8CAA8A/v8AAPb/AAAAAOv/6v8CABUA/v/+/w4ACwAIAPf/AAAMAAUA+//s/wYABgD2/xIADgACAPr/6f/0/woAAwDs//z/AAD9/wYA/P8UABYAAAAIAAwA/P/s/+j/7P/7//z/AgAKAPr/+f/3/wgAIgAYAA8A+v/u//f//P8MAPb/3//u//b////y//D/+P8FAAwA7//+/w4ABAAIAAAA+P/w/+//+v/y/+z/+f/z/wAAEAAMAAsA8P8EABAA+P8DAPf/+/8CAAYABgDy//j/AQAXABYA/f8EAAgABwDy/+r/CAAAAAYABgDy/wwADgDw/+b//P8IAPj/8v/6/xYADAD0/wEACgAKAPn/BAAQAPr/+P/6/wEA+//v/wQACAD5/+7/AAAUAAAA7v8EABoA///y/wQABAD+//b/+P8GAA0AAwAEAP7/9v/+//L/7//f/+T/+v/0//H/8v8MAPr/7P8KAPz/+P/1/wMAFgD3/+3/8f8EAAEA5//v/+L/8P8MAAMACgD6/+H/6v/6/wwABgD3/xAAHQAKAAEAAAAEAA8ABADy//v/BAAGAAQACAAIAAYAEAD6//b//f/s//3/CwACAAgA/P/1//7/EwAbAAgAAAD6/xYADQDo/+T/7P8EAOP/+P8TAAMAGgAPABMADgD0/+7//v8IAP7//P8AABgAAgD0/xIA+//3//7/CAAfAPT/8/8IAPn/+//g/+7/DgAEAPr/BQAWAAwACgD+//r/+P/4/woA/v/0//D/GAAcAP7/DgAMAAQA7f/t//7/CgAJAAoAIgACAPT/9//1/wIA8v8GAPj/1v/8/xwAJwAPAPT/AwAOABAADAD4/wAAEQAAAAQAAADy/wwACgDv/+z/BwAGAPL/+/8EABMAEAAOAA4A+P/0/wYAAwAAAAoA+v8QABgA///v//b/HAAOAA4ADQACAPz/4P/2/wgA/P/c/+f/GAAQAAoACwAKABAA8P/5/x4ACAD9//f/AAAGAOr/8f8IAAoA+v/w/wAADAD2//D/HgAVAPb/AgAEABkA/v/w/wYAAAAEAOv//v8UAAwAEAD5//P/4v/o//z//P/+////CAD2/+j/AgAVAAYA7P/0/wQA+P/q/+T/BAAQAPr/CgAOABIABgD6/xwABgDq//z/CgD8/+r/6v/8/x4A/v/p/wIACgAWAAIA/v8JAPr/9f/8/woA///y//7/BAAMAAwABAACAPf/8P/q//T/5v/o//n/6/8OAAAA9v/8/9z//P/8/wIA///u/w4A6v/s/xMAAAD8//7/BQAKAPb/8v8FABUACADw//b/EAAEAN7/8v8HAPT/AgAHAAwACAD0/xIAGAD//wAAEAAIAO7/7f/+/w4AAgAAAPz/9P8MAAYA/f8CAP//AAAGAAgA/P8EAAYA+v/9/wUAEgD2/+j//v/+//f/6v8FAB4ACgDw//b/EAD8//T/BAAKAAwAAwAIAAAA8v/u//z/BQD+/wYAAAD3//j///8OAAQA+P/q/+r//f8JAAkAAgAMABIACgD8//z/9//x//j/2//l/wQACgAUAPv//v8OAA4AEQD6/+D/7P8IAAwABQACABQA/v/e//b/+P8AAAQA7f8AABAAEAALAO7/7P8OABAACwAMAAkAEAD0/+z/BwAeABEA/v8SAP3/9P/4//P//P/0/wQADQAGAP7/CAASAAcA9//u//z/8v/6/wIA+P8LABIABADu/+X/9v/7//v/AAD8/wkABgDy/wIAHAAMAOj/7P8KAA4A9v/3//n/AgD///D/HQAeAPb/DAAlABsAAADv/w8AIgD6//r/FgAMAAwABAD5/woACAALAAcACAAeABMAHAATAAEA9v/y/xQABgAIAAgA/v8HAPD/DwAPAPP/BAD6//n/8P/o//r/9f/q/+z//v8XAA8ACAAQAA0A/P/0/wEAEAAbAAoA+P/5/wYABgD2/xEAFAAKAAgADAAiAAIA8v8MABAA/f/v//D///8IAOz/+v8UAPv/9//q/+D/9P/s//7/BgD+/wQA9/8IAAcABQAHAOn/AgAEAAAABADy/xQA+//y/woA+v8QAPP/7v8CAAIAEAD6/w4AEQD6//b/7P8IAA8AAAAJABIACAD2//D/AwDy/9L/7//+//r///8EAB4ACwDy/xQAEgAEAAIA7P/+/woA+P8GAAkA9P/w/wQAFAAYAAoA8v8AAAAA+v/+/+7/AAAQABoAFQAAAP7/+P8UABgA7P/q/+z/6v/4//b/AgD2//j/CgD8//7/BgAgABIACgASAAYAFAAKABIA+P/c//f/+P/4//L/+v8PACIAHgAIABMADAAJAP7/+P8SAP//+//6/+z/9v/2//P//P8QAAIA4v/o/xoAGgDu/+z//f8WAAoA7P/4/xYAAgDw/xQACAD4//j//v8CAPT/+v8AAA4AAgD4/wgAAgD+/+7/+v8GAOz/BQAWAAwA/v/p/wIA/P/l//j/DAAQAAQA9v/y//7/+P/i/+7/+P/2//7//P8EAAAA9P8SABAA+v/2/wQAGgD+/wQAFgAFABgAIAAeABYAAAD8//j/+v/w/+D/5v/4/w4A+v/t//z/9v/+//f/7v8PAAoA/P8IAAsAFAD0/9b/5v/o//r/AAD+/wQA7v/8/xoAFgAEAPb/DwAOAPz//v/w//n/CAAKAPj/6/8LAPT/2v/v//v/DwAAAOj/+P8SAAQA7P/y/woADAD2/wQAFgAUAP7/9P8CAPr/9P/8/xIACgD6/wAAEgAgAAQA+v8IAAAA8v/0//b/+v/+//H/BgAGAPT//v/5/xwAGAAAAAQA+P8bAAYA9P8DAOL/DAAMAOb//v8AAP3/AAD4/+v/9P/8/wYAHAAHABIAKAAUAAAA8P8IABAA9v/y//n/+v/x//z/DgAYAAwA//8LABIABADq/+z/+P/u/+v/9v/o/+L/8//3/wgAHAAoACUADgAMABcACgAIAAYA8v/2//7/7f/s////BwD+/wAABADm//X/IgASAAIAAAANACAACADy//b//v8NAPr/7P8KAA4AAAACAAoABgD6/wIAEAAAAO////8OABUABADz////CADy/+r/FAARAPb/+f///wgA+P/q//7/DADz//L/+v/s/wIAAAD4/wwAEgAYAAwAAwAGAAIADAAWAA8A+P/n//T///8DAAoABAD2//T/BgALAAQA/v8GAA4A7P/k/wYABgD0/+j/4v/n//T/+P8AAAwABAAGAPn//f8IAO7/9v/v//X/DADw/+z/5P/m//L/9P////b/AwD+//n/EgAMABIACAD0//7//P/8/wQAEgAHAP7/CwD+//r/8v/u/wYABAD1//T/AwAMAP7/6v/0/wkACAD+//j/DAARAAUABgAOACAA/v/w/xAAEgAKAPn/AAAGAPb/3//g/wIADQAEAO3//P8WAP//6f/o/wUACAD1//T/BQAcABIACgACAPX/+//7//z/7f/s/wQABAD+//H/+/8hABgAAAD5//7/FwAAAOb/+v/8/wQA9P/+/xgADAAOAO3/6P/m/+D/9v/s/+z/8P8KABQAAAAGAAEA8//1//z/9v8AAPj/AQAMAPz/+v/6/x4AIAAeACgABgAOAP7/4//4//v/CAAVAPz/6f/x/wIAEgAQAAwACAD8//r/6//w//3/+v8IAPb/5P/4/wQABAADAPz//v8MAAoA+v/w/wAACgAGAPz/CwASAP//9P/i//j/AAD3//7/BgAaAAsACAAGAPj/CgAIAAUA+P/l/wAA/v/1/w4AEAASAAoA5//t//r/+v/+/wQAAgD6//z/9v8IABUACgAAAAAAIAASAPb/BQAGAAsA/P/i//z/DwD8/+r/6v/w//X/BAD7//b/AgDy/woAFQAFAA8AAgAAAP7/8P/s//P/EgARABIADgACAAcA9P/y//r/AwAGAPz/9v/q//7//P/y/wIA8P/k/+7/9v8DAA4A+v/s//b/AgAWAAkA+/8SABYACAAGAPf/8v8GABAABwAAAAcACgAKAPv/5//+/wEA+P8PAA4AFgAEAN7/BAARAAYA+P/6/xYA/f8AAAEA/P8YAA4ACgD4//X/FgAKABAADADw//j/+/8FAPv/8P8GAAYA/P/m//r/AAD6/w4A9P/8//r/AwAIAPH/BgD6/wYA9P/g/wMABgD4/+T/8f/8/w4ADQAFABgADgAKAPr////8//L/BgACAPD/8P8OABIADgAGAPT/AQAAAP//AADy//D/CgAYAAQA9f/k/93/9P8AAPb/9/8NABQA/P/0//7/+P/y//H/4//i/+z/9v/+/wwAGwACAAoAGQALAAUA7v/4/wgAAgAIABwADgD8/xUAGgANAO7/9f8JAPb//P8CAAQA9v/6/w4ACwAAAPb/9v8IAAgA+v/+//P/CgAYAPr//v/4//v/CgAGAP3/8v///wwADgAEAPT/AAAMAAAA7v/2/xUADwD6//j/EAAgAPf/6P8AAAIA+P/u/wQAEAD2/+7/BwAXABAA9P/u/wwACQAGAPz/9P8KAPX/6/8CAAIA/P8MAPL/3f/+//r/+P/z//7////w//D/2P/q//L/+P8EAPD/+P/7/wAA8f/w/wwAAwAIAAgAEAAdAP//AAAIAPf/+v/6/wAACAD6//r/BAAQABgACgAOABgACQD5/+T/+P/+//L/EAAGAP3/9v/8/x4AEgAAAPL/AAAMAPL/9/8GAA8ACgDw//b///8OABoA/f/y//n//P8MAAoA+v8AAAwAGAAPAOv/7v8MABIA8f/u/wgAFgAQAOn/+v8UAP7/+v8EAAcACAAFAOX/8f8RAAAA+f8GABoAGgD8//D/BgAOAP7/8P8AAA0A6v/w/xIACAAEAAgAGgAjAPr/9P8FAAIABgDr/+///v/+/xEAAgD0/+//AwAaAPz/+/8QAPj/4P/o/+7/+f/4/wAAEAD5//b/5v/u/woA/P8FAPz/DwAgAAYAAQD2/+r/9P8CAPj/BgD7/9j/6v8GABUA/v/7/xEAFAAGAAIAAgDk//H/FQAWAPr/+/8AAP7/CgD4/wQA+P/6/woA5//0//z/CQAIAOj/8P8DABIABgD2//r/CAAKAPP/8v8KAA4AAgDu//D/EAD//+r/9f8GABYA9f/+/w8A/P///+r/AAAUAA4AEgAAAAwAAgDq/wMADgD6/+v/9P/2//H/+//9//v//v/+/wYA//8CABwACQD+/xUACAA=\" type=\"audio/wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Brushing sound\n",
    "plt.figure(figsize=(14,5))\n",
    "datas,sample_rate=librosa.load(filename)\n",
    "librosa.display.waveplot(datas,sr=sample_rate)\n",
    "ipd.Audio(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1636d407100>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## plot the librosa audio data\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(librosa_audio_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slice_file_name</th>\n",
       "      <th>classID</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B_ANI01_MC_FN_SIM01_101.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>brushing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B_ANI01_MC_FN_SIM01_102.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>brushing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B_ANI01_MC_FN_SIM01_103.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>brushing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B_ANI01_MC_FN_SIM01_301.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>brushing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B_ANI01_MC_FN_SIM01_302.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>brushing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               slice_file_name  classID     class\n",
       "0  B_ANI01_MC_FN_SIM01_101.wav        1  brushing\n",
       "1  B_ANI01_MC_FN_SIM01_102.wav        1  brushing\n",
       "2  B_ANI01_MC_FN_SIM01_103.wav        1  brushing\n",
       "3  B_ANI01_MC_FN_SIM01_301.wav        1  brushing\n",
       "4  B_ANI01_MC_FN_SIM01_302.wav        1  brushing"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "metadata= pd.read_csv('ML project\\data\\metadata.csv')\n",
    "metadata.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "isolation           221\n",
       "brushing            127\n",
       "waiting_for_food     92\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##check wheter the dataset is imbalanced\n",
    "metadata['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "##extracting features using MFCC for every audio file\n",
    "import pandas as pf\n",
    "import os\n",
    "import librosa\n",
    "\n",
    "audio_data_path= 'ML project\\data\\dataset'\n",
    "metadata= pd.read_csv('ML project\\data\\metadata.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_extractor(file):\n",
    "    audio,sample_rate= librosa.load(file_name,res_type = 'kaiser_fast')\n",
    "    mfcss_features=librosa.feature.mfcc(y=audio,sr=sample_rate,n_mfcc=40)\n",
    "    mfccs_scaled_features= np.mean(mfcss_features.T,axis=0)\n",
    "    \n",
    "    return mfccs_scaled_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "440it [00:14, 31.34it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "#Now we iterate through every audio file and extract features using MFCC\n",
    "extracted_features=[]\n",
    "for index_num, row in tqdm (metadata.iterrows()):\n",
    "    file_name= os.path.join(os.path.abspath(audio_data_path),str(row[\"slice_file_name\"]))\n",
    "    final_class_label=row[\"class\"]\n",
    "    data=features_extractor(file_name)\n",
    "    extracted_features.append([data,final_class_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-396.81778, 136.41545, -74.92035, -11.478852,...</td>\n",
       "      <td>brushing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-542.6039, 158.09341, -73.43401, -11.935203, ...</td>\n",
       "      <td>brushing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-517.51764, 142.89006, -69.55315, -7.46689, 3...</td>\n",
       "      <td>brushing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-476.4709, 112.17069, -65.29729, -10.651881, ...</td>\n",
       "      <td>brushing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-511.12573, 135.90286, -62.427002, -11.154292...</td>\n",
       "      <td>brushing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             feature     class\n",
       "0  [-396.81778, 136.41545, -74.92035, -11.478852,...  brushing\n",
       "1  [-542.6039, 158.09341, -73.43401, -11.935203, ...  brushing\n",
       "2  [-517.51764, 142.89006, -69.55315, -7.46689, 3...  brushing\n",
       "3  [-476.4709, 112.17069, -65.29729, -10.651881, ...  brushing\n",
       "4  [-511.12573, 135.90286, -62.427002, -11.154292...  brushing"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## converting extracted_features to Pandas dataframe\n",
    "extracted_features_df=pd.DataFrame(extracted_features,columns=['feature','class'])\n",
    "extracted_features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split dataset into independent and dependent dataset\n",
    "X=np.array(extracted_features_df['feature'].tolist())\n",
    "y=np.array(extracted_features_df['class'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(440, 40)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       ...,\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Label encoding\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder = LabelEncoder()\n",
    "y=to_categorical(labelencoder.fit_transform(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(352, 40)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-6.2651703e+02,  1.6675206e+02, -6.2472973e+01, ...,\n",
       "         2.3507240e+00, -3.9628022e+00,  2.5059193e-01],\n",
       "       [-5.0628857e+02,  1.0420370e+02, -4.1188686e+01, ...,\n",
       "        -9.5325756e-01, -7.8021437e-01,  1.8900108e-01],\n",
       "       [-5.1112573e+02,  1.3590286e+02, -6.2427002e+01, ...,\n",
       "         1.0814322e+00, -1.2478454e+00,  1.6497887e+00],\n",
       "       ...,\n",
       "       [-3.6755038e+02,  1.4404439e+02, -5.6834351e+01, ...,\n",
       "        -1.2404530e+00,  6.7717478e-02,  1.7579875e+00],\n",
       "       [-6.1116144e+02,  1.7776347e+02, -8.5178993e+01, ...,\n",
       "         1.7360479e+00, -2.4014084e+00,  1.2079458e+00],\n",
       "       [-6.3292749e+02,  1.7310408e+02, -7.4483101e+01, ...,\n",
       "         4.9773377e-01, -8.7558186e-01,  1.2513393e+00]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88, 40)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5.51194153e+02,  1.30571091e+02, -6.97639389e+01, ...,\n",
       "         1.19552922e+00,  5.23097157e-01,  3.84192538e+00],\n",
       "       [-3.70140533e+02,  1.14199348e+02, -5.82798729e+01, ...,\n",
       "         3.20071757e-01, -3.37360764e+00, -3.27493191e+00],\n",
       "       [-5.24417542e+02,  1.13680504e+02, -4.87653732e+01, ...,\n",
       "         1.40203679e+00, -1.61289763e+00,  1.46254337e+00],\n",
       "       ...,\n",
       "       [-5.04581604e+02,  1.52437546e+02, -6.87483292e+01, ...,\n",
       "         1.90740097e+00, -9.52052712e-01, -7.94477761e-02],\n",
       "       [-5.25529846e+02,  1.48850098e+02, -6.05766029e+01, ...,\n",
       "        -9.45907116e-01, -3.84938812e+00,  2.24521613e+00],\n",
       "       [-5.62459290e+02,  1.56093781e+02, -8.89175720e+01, ...,\n",
       "         8.41846824e-01, -1.29717422e+00,  7.07806587e-01]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(352, 3)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       ...,\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1]], dtype=uint8)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88, 3)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## using tensorflowwww\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout,Activation,Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Number of classes\n",
    "num_labels=y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "###first layer\n",
    "model.add(Dense(100,input_shape=(40,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "### second layer\n",
    "model.add(Dense(200))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "### third layer\n",
    "model.add(Dense(100))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "### last layer\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 100)               4100      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 200)               20200     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 303       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 44,703\n",
      "Trainable params: 44,703\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5865 - accuracy: 0.7500\n",
      "Epoch 00001: val_loss improved from inf to 1.57877, saving model to save_models\\audio_classification.hdf5\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.5968 - accuracy: 0.7045 - val_loss: 1.5788 - val_accuracy: 0.6591\n",
      "Epoch 2/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4518 - accuracy: 0.6875\n",
      "Epoch 00002: val_loss did not improve from 1.57877\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5724 - accuracy: 0.6903 - val_loss: 1.9853 - val_accuracy: 0.6705\n",
      "Epoch 3/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5270 - accuracy: 0.6875\n",
      "Epoch 00003: val_loss improved from 1.57877 to 1.37577, saving model to save_models\\audio_classification.hdf5\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5771 - accuracy: 0.6903 - val_loss: 1.3758 - val_accuracy: 0.6477\n",
      "Epoch 4/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6110 - accuracy: 0.5625\n",
      "Epoch 00004: val_loss did not improve from 1.37577\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6294 - accuracy: 0.6733 - val_loss: 1.6395 - val_accuracy: 0.6591\n",
      "Epoch 5/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7803 - accuracy: 0.5625\n",
      "Epoch 00005: val_loss did not improve from 1.37577\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5971 - accuracy: 0.6932 - val_loss: 2.0367 - val_accuracy: 0.6591\n",
      "Epoch 6/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6130 - accuracy: 0.6875\n",
      "Epoch 00006: val_loss did not improve from 1.37577\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6186 - accuracy: 0.6733 - val_loss: 2.0794 - val_accuracy: 0.6591\n",
      "Epoch 7/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5654 - accuracy: 0.7188\n",
      "Epoch 00007: val_loss did not improve from 1.37577\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6146 - accuracy: 0.6449 - val_loss: 1.8865 - val_accuracy: 0.6705\n",
      "Epoch 8/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7405 - accuracy: 0.5312\n",
      "Epoch 00008: val_loss did not improve from 1.37577\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5916 - accuracy: 0.6761 - val_loss: 1.8881 - val_accuracy: 0.6705\n",
      "Epoch 9/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6966 - accuracy: 0.5312\n",
      "Epoch 00009: val_loss did not improve from 1.37577\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6156 - accuracy: 0.6733 - val_loss: 2.0652 - val_accuracy: 0.6477\n",
      "Epoch 10/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4834 - accuracy: 0.7188\n",
      "Epoch 00010: val_loss did not improve from 1.37577\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5838 - accuracy: 0.6932 - val_loss: 2.0305 - val_accuracy: 0.6818\n",
      "Epoch 11/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5320 - accuracy: 0.7500\n",
      "Epoch 00011: val_loss did not improve from 1.37577\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5867 - accuracy: 0.6648 - val_loss: 1.9084 - val_accuracy: 0.6591\n",
      "Epoch 12/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6535 - accuracy: 0.5625\n",
      "Epoch 00012: val_loss did not improve from 1.37577\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5782 - accuracy: 0.6903 - val_loss: 1.8834 - val_accuracy: 0.6932\n",
      "Epoch 13/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4056 - accuracy: 0.8125\n",
      "Epoch 00013: val_loss did not improve from 1.37577\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5551 - accuracy: 0.6989 - val_loss: 2.0766 - val_accuracy: 0.7045\n",
      "Epoch 14/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6093 - accuracy: 0.6250\n",
      "Epoch 00014: val_loss did not improve from 1.37577\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5735 - accuracy: 0.6648 - val_loss: 2.4815 - val_accuracy: 0.6818\n",
      "Epoch 15/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5008 - accuracy: 0.7812\n",
      "Epoch 00015: val_loss did not improve from 1.37577\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5771 - accuracy: 0.6790 - val_loss: 2.6481 - val_accuracy: 0.6818\n",
      "Epoch 16/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4674 - accuracy: 0.7812\n",
      "Epoch 00016: val_loss did not improve from 1.37577\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5770 - accuracy: 0.6619 - val_loss: 2.2247 - val_accuracy: 0.6705\n",
      "Epoch 17/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4877 - accuracy: 0.6875\n",
      "Epoch 00017: val_loss did not improve from 1.37577\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5973 - accuracy: 0.6591 - val_loss: 2.2271 - val_accuracy: 0.6818\n",
      "Epoch 18/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5319 - accuracy: 0.6875\n",
      "Epoch 00018: val_loss did not improve from 1.37577\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6038 - accuracy: 0.6705 - val_loss: 2.1808 - val_accuracy: 0.6705\n",
      "Epoch 19/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6245 - accuracy: 0.6250\n",
      "Epoch 00019: val_loss did not improve from 1.37577\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5477 - accuracy: 0.7102 - val_loss: 2.0606 - val_accuracy: 0.6477\n",
      "Epoch 20/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5396 - accuracy: 0.7812\n",
      "Epoch 00020: val_loss did not improve from 1.37577\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5868 - accuracy: 0.6932 - val_loss: 2.3685 - val_accuracy: 0.6705\n",
      "Epoch 21/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7152 - accuracy: 0.6562\n",
      "Epoch 00021: val_loss did not improve from 1.37577\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5745 - accuracy: 0.7017 - val_loss: 2.5302 - val_accuracy: 0.6705\n",
      "Epoch 22/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4990 - accuracy: 0.7500\n",
      "Epoch 00022: val_loss did not improve from 1.37577\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5806 - accuracy: 0.6875 - val_loss: 2.7640 - val_accuracy: 0.6591\n",
      "Epoch 23/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5008 - accuracy: 0.8125\n",
      "Epoch 00023: val_loss did not improve from 1.37577\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5687 - accuracy: 0.6903 - val_loss: 2.8839 - val_accuracy: 0.6591\n",
      "Epoch 24/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7351 - accuracy: 0.5625\n",
      "Epoch 00024: val_loss did not improve from 1.37577\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5913 - accuracy: 0.6619 - val_loss: 2.3568 - val_accuracy: 0.6818\n",
      "Epoch 25/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5073 - accuracy: 0.7500\n",
      "Epoch 00025: val_loss did not improve from 1.37577\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5935 - accuracy: 0.6932 - val_loss: 2.2719 - val_accuracy: 0.6932\n",
      "Epoch 26/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5147 - accuracy: 0.7188\n",
      "Epoch 00026: val_loss did not improve from 1.37577\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6345 - accuracy: 0.6506 - val_loss: 2.0893 - val_accuracy: 0.6705\n",
      "Epoch 27/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4840 - accuracy: 0.7500\n",
      "Epoch 00027: val_loss did not improve from 1.37577\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6134 - accuracy: 0.6733 - val_loss: 2.2100 - val_accuracy: 0.6705\n",
      "Epoch 28/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4959 - accuracy: 0.8125\n",
      "Epoch 00028: val_loss did not improve from 1.37577\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6629 - accuracy: 0.6676 - val_loss: 2.1344 - val_accuracy: 0.6818\n",
      "Epoch 29/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6597 - accuracy: 0.6875\n",
      "Epoch 00029: val_loss did not improve from 1.37577\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6266 - accuracy: 0.6534 - val_loss: 2.0712 - val_accuracy: 0.6705\n",
      "Epoch 30/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4058 - accuracy: 0.9062\n",
      "Epoch 00030: val_loss did not improve from 1.37577\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6011 - accuracy: 0.7017 - val_loss: 2.0195 - val_accuracy: 0.6818\n",
      "Epoch 31/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7035 - accuracy: 0.5625\n",
      "Epoch 00031: val_loss did not improve from 1.37577\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5854 - accuracy: 0.6733 - val_loss: 2.4960 - val_accuracy: 0.6818\n",
      "Epoch 32/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5306 - accuracy: 0.6250\n",
      "Epoch 00032: val_loss did not improve from 1.37577\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5894 - accuracy: 0.6818 - val_loss: 2.9165 - val_accuracy: 0.6818\n",
      "Epoch 33/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6310 - accuracy: 0.5938\n",
      "Epoch 00033: val_loss did not improve from 1.37577\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5909 - accuracy: 0.6648 - val_loss: 2.4484 - val_accuracy: 0.6932\n",
      "Epoch 34/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6021 - accuracy: 0.6875\n",
      "Epoch 00034: val_loss did not improve from 1.37577\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6090 - accuracy: 0.6761 - val_loss: 2.1606 - val_accuracy: 0.6932\n",
      "Epoch 35/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4140 - accuracy: 0.7500\n",
      "Epoch 00035: val_loss did not improve from 1.37577\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5829 - accuracy: 0.6705 - val_loss: 2.5192 - val_accuracy: 0.6818\n",
      "Epoch 36/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6028 - accuracy: 0.6875\n",
      "Epoch 00036: val_loss did not improve from 1.37577\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6317 - accuracy: 0.6903 - val_loss: 2.0217 - val_accuracy: 0.6932\n",
      "Epoch 37/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6084 - accuracy: 0.6875\n",
      "Epoch 00037: val_loss did not improve from 1.37577\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6036 - accuracy: 0.6761 - val_loss: 2.1731 - val_accuracy: 0.6705\n",
      "Epoch 38/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7920 - accuracy: 0.5938\n",
      "Epoch 00038: val_loss did not improve from 1.37577\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6263 - accuracy: 0.6761 - val_loss: 2.8014 - val_accuracy: 0.6932\n",
      "Epoch 39/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6854 - accuracy: 0.5938\n",
      "Epoch 00039: val_loss did not improve from 1.37577\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5897 - accuracy: 0.6875 - val_loss: 3.3746 - val_accuracy: 0.6932\n",
      "Epoch 40/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4941 - accuracy: 0.7188\n",
      "Epoch 00040: val_loss did not improve from 1.37577\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5708 - accuracy: 0.6903 - val_loss: 3.6443 - val_accuracy: 0.6705\n",
      "Epoch 41/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4992 - accuracy: 0.6875\n",
      "Epoch 00041: val_loss did not improve from 1.37577\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5900 - accuracy: 0.6676 - val_loss: 4.0373 - val_accuracy: 0.6705\n",
      "Epoch 42/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4856 - accuracy: 0.8125\n",
      "Epoch 00042: val_loss did not improve from 1.37577\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5559 - accuracy: 0.7131 - val_loss: 2.9258 - val_accuracy: 0.6818\n",
      "Epoch 43/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5717 - accuracy: 0.7500\n",
      "Epoch 00043: val_loss did not improve from 1.37577\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5772 - accuracy: 0.6875 - val_loss: 2.9668 - val_accuracy: 0.6818\n",
      "Epoch 44/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5038 - accuracy: 0.6562\n",
      "Epoch 00044: val_loss did not improve from 1.37577\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5756 - accuracy: 0.6875 - val_loss: 3.1368 - val_accuracy: 0.6591\n",
      "Epoch 45/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5986 - accuracy: 0.6562\n",
      "Epoch 00045: val_loss did not improve from 1.37577\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5706 - accuracy: 0.7131 - val_loss: 3.0006 - val_accuracy: 0.6705\n",
      "Epoch 46/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5304 - accuracy: 0.7188\n",
      "Epoch 00046: val_loss did not improve from 1.37577\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5745 - accuracy: 0.7074 - val_loss: 2.8638 - val_accuracy: 0.6705\n",
      "Epoch 47/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5891 - accuracy: 0.6875\n",
      "Epoch 00047: val_loss did not improve from 1.37577\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6229 - accuracy: 0.6619 - val_loss: 2.5532 - val_accuracy: 0.6818\n",
      "Epoch 48/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5365 - accuracy: 0.6875\n",
      "Epoch 00048: val_loss did not improve from 1.37577\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5892 - accuracy: 0.7045 - val_loss: 2.9187 - val_accuracy: 0.6932\n",
      "Epoch 49/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6145 - accuracy: 0.8125\n",
      "Epoch 00049: val_loss did not improve from 1.37577\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6077 - accuracy: 0.6477 - val_loss: 3.2057 - val_accuracy: 0.6591\n",
      "Epoch 50/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.8572 - accuracy: 0.6250\n",
      "Epoch 00050: val_loss did not improve from 1.37577\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6405 - accuracy: 0.6364 - val_loss: 3.3086 - val_accuracy: 0.6705\n",
      "Epoch 51/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5880 - accuracy: 0.6875\n",
      "Epoch 00051: val_loss did not improve from 1.37577\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6466 - accuracy: 0.6392 - val_loss: 3.4155 - val_accuracy: 0.6932\n",
      "Epoch 52/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4648 - accuracy: 0.7500\n",
      "Epoch 00052: val_loss did not improve from 1.37577\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.5572 - accuracy: 0.7017 - val_loss: 3.8074 - val_accuracy: 0.6591\n",
      "Epoch 53/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5836 - accuracy: 0.6875\n",
      "Epoch 00053: val_loss did not improve from 1.37577\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5725 - accuracy: 0.7074 - val_loss: 3.4022 - val_accuracy: 0.6818\n",
      "Epoch 54/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5325 - accuracy: 0.7188\n",
      "Epoch 00054: val_loss did not improve from 1.37577\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6392 - accuracy: 0.6619 - val_loss: 3.5469 - val_accuracy: 0.6591\n",
      "Epoch 55/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.8439 - accuracy: 0.5625\n",
      "Epoch 00055: val_loss did not improve from 1.37577\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6230 - accuracy: 0.6648 - val_loss: 3.6278 - val_accuracy: 0.6932\n",
      "Epoch 56/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6586 - accuracy: 0.6250\n",
      "Epoch 00056: val_loss did not improve from 1.37577\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5906 - accuracy: 0.7074 - val_loss: 3.9264 - val_accuracy: 0.6932\n",
      "Epoch 57/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5692 - accuracy: 0.7188\n",
      "Epoch 00057: val_loss did not improve from 1.37577\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.5810 - accuracy: 0.6960 - val_loss: 3.9337 - val_accuracy: 0.6818\n",
      "Epoch 58/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6796 - accuracy: 0.5938\n",
      "Epoch 00058: val_loss did not improve from 1.37577\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5943 - accuracy: 0.6790 - val_loss: 3.2134 - val_accuracy: 0.6705\n",
      "Epoch 59/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6634 - accuracy: 0.6875\n",
      "Epoch 00059: val_loss did not improve from 1.37577\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6564 - accuracy: 0.6733 - val_loss: 3.8305 - val_accuracy: 0.6932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5667 - accuracy: 0.7500\n",
      "Epoch 00060: val_loss did not improve from 1.37577\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6763 - accuracy: 0.6506 - val_loss: 2.8701 - val_accuracy: 0.6705\n",
      "Epoch 61/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6723 - accuracy: 0.5938\n",
      "Epoch 00061: val_loss did not improve from 1.37577\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6620 - accuracy: 0.6506 - val_loss: 2.1242 - val_accuracy: 0.7045\n",
      "Epoch 62/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5301 - accuracy: 0.7500\n",
      "Epoch 00062: val_loss did not improve from 1.37577\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6307 - accuracy: 0.6506 - val_loss: 2.4270 - val_accuracy: 0.6705\n",
      "Epoch 63/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6705 - accuracy: 0.6250\n",
      "Epoch 00063: val_loss did not improve from 1.37577\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6211 - accuracy: 0.6591 - val_loss: 2.6075 - val_accuracy: 0.6818\n",
      "Epoch 64/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6309 - accuracy: 0.6875\n",
      "Epoch 00064: val_loss did not improve from 1.37577\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6490 - accuracy: 0.6534 - val_loss: 2.8538 - val_accuracy: 0.6705\n",
      "Epoch 65/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6309 - accuracy: 0.6250\n",
      "Epoch 00065: val_loss did not improve from 1.37577\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5985 - accuracy: 0.6761 - val_loss: 1.9679 - val_accuracy: 0.6705\n",
      "Epoch 66/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7057 - accuracy: 0.6250\n",
      "Epoch 00066: val_loss did not improve from 1.37577\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6094 - accuracy: 0.6847 - val_loss: 2.7994 - val_accuracy: 0.6705\n",
      "Epoch 67/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5888 - accuracy: 0.6250\n",
      "Epoch 00067: val_loss improved from 1.37577 to 1.36862, saving model to save_models\\audio_classification.hdf5\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6341 - accuracy: 0.6818 - val_loss: 1.3686 - val_accuracy: 0.6818\n",
      "Epoch 68/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6220 - accuracy: 0.7812\n",
      "Epoch 00068: val_loss improved from 1.36862 to 1.34911, saving model to save_models\\audio_classification.hdf5\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6130 - accuracy: 0.6790 - val_loss: 1.3491 - val_accuracy: 0.6705\n",
      "Epoch 69/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5648 - accuracy: 0.7500\n",
      "Epoch 00069: val_loss did not improve from 1.34911\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6509 - accuracy: 0.6989 - val_loss: 2.4237 - val_accuracy: 0.6818\n",
      "Epoch 70/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4857 - accuracy: 0.7812\n",
      "Epoch 00070: val_loss improved from 1.34911 to 1.16189, saving model to save_models\\audio_classification.hdf5\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7223 - accuracy: 0.6449 - val_loss: 1.1619 - val_accuracy: 0.6591\n",
      "Epoch 71/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7647 - accuracy: 0.5312\n",
      "Epoch 00071: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6199 - accuracy: 0.6619 - val_loss: 1.6032 - val_accuracy: 0.6591\n",
      "Epoch 72/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7153 - accuracy: 0.4688\n",
      "Epoch 00072: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5988 - accuracy: 0.6932 - val_loss: 1.7593 - val_accuracy: 0.6477\n",
      "Epoch 73/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5038 - accuracy: 0.7500\n",
      "Epoch 00073: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5886 - accuracy: 0.6875 - val_loss: 1.6459 - val_accuracy: 0.6818\n",
      "Epoch 74/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6873 - accuracy: 0.5625\n",
      "Epoch 00074: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5911 - accuracy: 0.6705 - val_loss: 1.6731 - val_accuracy: 0.6932\n",
      "Epoch 75/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5003 - accuracy: 0.7500\n",
      "Epoch 00075: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5953 - accuracy: 0.6847 - val_loss: 1.7434 - val_accuracy: 0.6818\n",
      "Epoch 76/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6100 - accuracy: 0.6250\n",
      "Epoch 00076: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.5993 - accuracy: 0.6733 - val_loss: 1.9410 - val_accuracy: 0.6818\n",
      "Epoch 77/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5569 - accuracy: 0.6250\n",
      "Epoch 00077: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6133 - accuracy: 0.6477 - val_loss: 1.7485 - val_accuracy: 0.6705\n",
      "Epoch 78/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5922 - accuracy: 0.6562\n",
      "Epoch 00078: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5434 - accuracy: 0.6989 - val_loss: 1.8568 - val_accuracy: 0.6932\n",
      "Epoch 79/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6207 - accuracy: 0.6562\n",
      "Epoch 00079: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5935 - accuracy: 0.6932 - val_loss: 2.1395 - val_accuracy: 0.6591\n",
      "Epoch 80/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4204 - accuracy: 0.8125\n",
      "Epoch 00080: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5615 - accuracy: 0.6989 - val_loss: 2.1668 - val_accuracy: 0.6932\n",
      "Epoch 81/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7391 - accuracy: 0.5312\n",
      "Epoch 00081: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6001 - accuracy: 0.6761 - val_loss: 2.2751 - val_accuracy: 0.6705\n",
      "Epoch 82/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4802 - accuracy: 0.7500\n",
      "Epoch 00082: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5913 - accuracy: 0.6960 - val_loss: 1.8673 - val_accuracy: 0.6591\n",
      "Epoch 83/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5697 - accuracy: 0.7500\n",
      "Epoch 00083: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5886 - accuracy: 0.6960 - val_loss: 1.8987 - val_accuracy: 0.6932\n",
      "Epoch 84/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5544 - accuracy: 0.6875\n",
      "Epoch 00084: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6339 - accuracy: 0.6591 - val_loss: 1.9253 - val_accuracy: 0.7045\n",
      "Epoch 85/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6604 - accuracy: 0.6562\n",
      "Epoch 00085: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6309 - accuracy: 0.6477 - val_loss: 2.0432 - val_accuracy: 0.6932\n",
      "Epoch 86/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5539 - accuracy: 0.6562\n",
      "Epoch 00086: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5988 - accuracy: 0.6932 - val_loss: 2.3484 - val_accuracy: 0.6591\n",
      "Epoch 87/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5554 - accuracy: 0.7500\n",
      "Epoch 00087: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5701 - accuracy: 0.6932 - val_loss: 2.7786 - val_accuracy: 0.6705\n",
      "Epoch 88/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5115 - accuracy: 0.6875\n",
      "Epoch 00088: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5869 - accuracy: 0.6903 - val_loss: 3.0103 - val_accuracy: 0.6932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7108 - accuracy: 0.5625\n",
      "Epoch 00089: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5683 - accuracy: 0.7074 - val_loss: 2.1073 - val_accuracy: 0.6591\n",
      "Epoch 90/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5405 - accuracy: 0.7188\n",
      "Epoch 00090: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5804 - accuracy: 0.6903 - val_loss: 2.1321 - val_accuracy: 0.6705\n",
      "Epoch 91/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5142 - accuracy: 0.6875\n",
      "Epoch 00091: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5941 - accuracy: 0.6847 - val_loss: 2.6282 - val_accuracy: 0.6932\n",
      "Epoch 92/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5220 - accuracy: 0.6562\n",
      "Epoch 00092: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6183 - accuracy: 0.6676 - val_loss: 2.1258 - val_accuracy: 0.6705\n",
      "Epoch 93/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7576 - accuracy: 0.6250\n",
      "Epoch 00093: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6528 - accuracy: 0.6364 - val_loss: 1.4850 - val_accuracy: 0.6818\n",
      "Epoch 94/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5663 - accuracy: 0.6875\n",
      "Epoch 00094: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7029 - accuracy: 0.6250 - val_loss: 1.4530 - val_accuracy: 0.6932\n",
      "Epoch 95/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6329 - accuracy: 0.6250\n",
      "Epoch 00095: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6774 - accuracy: 0.6250 - val_loss: 1.4598 - val_accuracy: 0.6932\n",
      "Epoch 96/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6734 - accuracy: 0.6250\n",
      "Epoch 00096: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6653 - accuracy: 0.6335 - val_loss: 1.5774 - val_accuracy: 0.6932\n",
      "Epoch 97/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6696 - accuracy: 0.6250\n",
      "Epoch 00097: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6639 - accuracy: 0.6080 - val_loss: 1.8767 - val_accuracy: 0.6818\n",
      "Epoch 98/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7578 - accuracy: 0.5312\n",
      "Epoch 00098: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6497 - accuracy: 0.6534 - val_loss: 1.7395 - val_accuracy: 0.7045\n",
      "Epoch 99/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5662 - accuracy: 0.5938\n",
      "Epoch 00099: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6449 - accuracy: 0.6449 - val_loss: 1.7295 - val_accuracy: 0.7045\n",
      "Epoch 100/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6428 - accuracy: 0.6562\n",
      "Epoch 00100: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6111 - accuracy: 0.6619 - val_loss: 1.9779 - val_accuracy: 0.7045\n",
      "Epoch 101/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7335 - accuracy: 0.5938\n",
      "Epoch 00101: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6100 - accuracy: 0.6648 - val_loss: 2.3192 - val_accuracy: 0.6932\n",
      "Epoch 102/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.8773 - accuracy: 0.4688\n",
      "Epoch 00102: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6560 - accuracy: 0.6335 - val_loss: 2.3240 - val_accuracy: 0.7045\n",
      "Epoch 103/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6784 - accuracy: 0.6562\n",
      "Epoch 00103: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6446 - accuracy: 0.6364 - val_loss: 2.1023 - val_accuracy: 0.7045\n",
      "Epoch 104/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7296 - accuracy: 0.5000\n",
      "Epoch 00104: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6196 - accuracy: 0.6193 - val_loss: 2.0569 - val_accuracy: 0.6705\n",
      "Epoch 105/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6864 - accuracy: 0.5938\n",
      "Epoch 00105: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6818 - accuracy: 0.6108 - val_loss: 1.9803 - val_accuracy: 0.7045\n",
      "Epoch 106/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6060 - accuracy: 0.6562\n",
      "Epoch 00106: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6362 - accuracy: 0.6449 - val_loss: 2.1895 - val_accuracy: 0.7045\n",
      "Epoch 107/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6981 - accuracy: 0.6250\n",
      "Epoch 00107: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6113 - accuracy: 0.6676 - val_loss: 2.5097 - val_accuracy: 0.6932\n",
      "Epoch 108/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6813 - accuracy: 0.6250\n",
      "Epoch 00108: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6049 - accuracy: 0.6705 - val_loss: 2.6123 - val_accuracy: 0.6932\n",
      "Epoch 109/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6561 - accuracy: 0.5000\n",
      "Epoch 00109: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5310 - accuracy: 0.6903 - val_loss: 2.7889 - val_accuracy: 0.6705\n",
      "Epoch 110/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6221 - accuracy: 0.6562\n",
      "Epoch 00110: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6181 - accuracy: 0.6875 - val_loss: 2.7138 - val_accuracy: 0.6932\n",
      "Epoch 111/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5106 - accuracy: 0.7812\n",
      "Epoch 00111: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6094 - accuracy: 0.6648 - val_loss: 1.8954 - val_accuracy: 0.6932\n",
      "Epoch 112/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5806 - accuracy: 0.6875\n",
      "Epoch 00112: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5992 - accuracy: 0.6790 - val_loss: 2.1963 - val_accuracy: 0.6705\n",
      "Epoch 113/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7159 - accuracy: 0.5312\n",
      "Epoch 00113: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5953 - accuracy: 0.6619 - val_loss: 2.4448 - val_accuracy: 0.6818\n",
      "Epoch 114/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5322 - accuracy: 0.6562\n",
      "Epoch 00114: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5689 - accuracy: 0.7017 - val_loss: 2.1276 - val_accuracy: 0.6818\n",
      "Epoch 115/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4559 - accuracy: 0.8125\n",
      "Epoch 00115: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5531 - accuracy: 0.6960 - val_loss: 2.2175 - val_accuracy: 0.6591\n",
      "Epoch 116/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5472 - accuracy: 0.6562\n",
      "Epoch 00116: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5431 - accuracy: 0.6875 - val_loss: 2.4075 - val_accuracy: 0.6818\n",
      "Epoch 117/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7087 - accuracy: 0.5625\n",
      "Epoch 00117: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5922 - accuracy: 0.6790 - val_loss: 2.7367 - val_accuracy: 0.6705\n",
      "Epoch 118/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.3870 - accuracy: 0.7812\n",
      "Epoch 00118: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6109 - accuracy: 0.6790 - val_loss: 2.3901 - val_accuracy: 0.6932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 119/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5613 - accuracy: 0.5938\n",
      "Epoch 00119: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6347 - accuracy: 0.6307 - val_loss: 2.5982 - val_accuracy: 0.6932\n",
      "Epoch 120/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6226 - accuracy: 0.6250\n",
      "Epoch 00120: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6383 - accuracy: 0.6250 - val_loss: 2.7855 - val_accuracy: 0.6705\n",
      "Epoch 121/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4778 - accuracy: 0.7188\n",
      "Epoch 00121: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5821 - accuracy: 0.6307 - val_loss: 2.9307 - val_accuracy: 0.6705\n",
      "Epoch 122/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5241 - accuracy: 0.6250\n",
      "Epoch 00122: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6168 - accuracy: 0.6136 - val_loss: 2.8057 - val_accuracy: 0.6477\n",
      "Epoch 123/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4963 - accuracy: 0.6875\n",
      "Epoch 00123: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6068 - accuracy: 0.6392 - val_loss: 2.9136 - val_accuracy: 0.6818\n",
      "Epoch 124/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7437 - accuracy: 0.5938\n",
      "Epoch 00124: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6210 - accuracy: 0.6449 - val_loss: 3.0553 - val_accuracy: 0.6364\n",
      "Epoch 125/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4441 - accuracy: 0.7500\n",
      "Epoch 00125: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5995 - accuracy: 0.6562 - val_loss: 2.7392 - val_accuracy: 0.6477\n",
      "Epoch 126/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6162 - accuracy: 0.6250\n",
      "Epoch 00126: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6165 - accuracy: 0.6449 - val_loss: 2.8789 - val_accuracy: 0.6477\n",
      "Epoch 127/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6501 - accuracy: 0.6250\n",
      "Epoch 00127: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6217 - accuracy: 0.6506 - val_loss: 3.0174 - val_accuracy: 0.6250\n",
      "Epoch 128/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7162 - accuracy: 0.5312\n",
      "Epoch 00128: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5966 - accuracy: 0.6477 - val_loss: 2.8867 - val_accuracy: 0.6705\n",
      "Epoch 129/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6074 - accuracy: 0.6250\n",
      "Epoch 00129: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5996 - accuracy: 0.6676 - val_loss: 2.9495 - val_accuracy: 0.6705\n",
      "Epoch 130/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6626 - accuracy: 0.6562\n",
      "Epoch 00130: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6311 - accuracy: 0.6506 - val_loss: 2.6277 - val_accuracy: 0.6591\n",
      "Epoch 131/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6799 - accuracy: 0.6562\n",
      "Epoch 00131: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6430 - accuracy: 0.6420 - val_loss: 2.3958 - val_accuracy: 0.6591\n",
      "Epoch 132/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6659 - accuracy: 0.6250\n",
      "Epoch 00132: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6341 - accuracy: 0.6705 - val_loss: 2.3110 - val_accuracy: 0.6705\n",
      "Epoch 133/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7842 - accuracy: 0.5938\n",
      "Epoch 00133: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6429 - accuracy: 0.6818 - val_loss: 2.6018 - val_accuracy: 0.6705\n",
      "Epoch 134/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6711 - accuracy: 0.6250\n",
      "Epoch 00134: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6517 - accuracy: 0.6449 - val_loss: 2.8465 - val_accuracy: 0.6818\n",
      "Epoch 135/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5866 - accuracy: 0.6875\n",
      "Epoch 00135: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6188 - accuracy: 0.6705 - val_loss: 2.4184 - val_accuracy: 0.6591\n",
      "Epoch 136/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5310 - accuracy: 0.6562\n",
      "Epoch 00136: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6232 - accuracy: 0.6562 - val_loss: 2.6376 - val_accuracy: 0.6818\n",
      "Epoch 137/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6831 - accuracy: 0.6250\n",
      "Epoch 00137: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5902 - accuracy: 0.6790 - val_loss: 2.7809 - val_accuracy: 0.6818\n",
      "Epoch 138/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5106 - accuracy: 0.7812\n",
      "Epoch 00138: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5688 - accuracy: 0.6932 - val_loss: 3.0929 - val_accuracy: 0.6818\n",
      "Epoch 139/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4665 - accuracy: 0.8750\n",
      "Epoch 00139: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5735 - accuracy: 0.6733 - val_loss: 3.1963 - val_accuracy: 0.6818\n",
      "Epoch 140/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4765 - accuracy: 0.8125\n",
      "Epoch 00140: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5338 - accuracy: 0.7159 - val_loss: 2.7947 - val_accuracy: 0.6932\n",
      "Epoch 141/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5448 - accuracy: 0.6875\n",
      "Epoch 00141: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5686 - accuracy: 0.6960 - val_loss: 2.6734 - val_accuracy: 0.6477\n",
      "Epoch 142/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4538 - accuracy: 0.7812\n",
      "Epoch 00142: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5956 - accuracy: 0.6733 - val_loss: 2.8929 - val_accuracy: 0.6364\n",
      "Epoch 143/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5965 - accuracy: 0.6562\n",
      "Epoch 00143: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6134 - accuracy: 0.6477 - val_loss: 2.2916 - val_accuracy: 0.6591\n",
      "Epoch 144/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.3861 - accuracy: 0.8125\n",
      "Epoch 00144: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6021 - accuracy: 0.6562 - val_loss: 2.2952 - val_accuracy: 0.6591\n",
      "Epoch 145/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6797 - accuracy: 0.5625\n",
      "Epoch 00145: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5803 - accuracy: 0.6619 - val_loss: 2.6716 - val_accuracy: 0.6591\n",
      "Epoch 146/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5677 - accuracy: 0.6562\n",
      "Epoch 00146: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5876 - accuracy: 0.6648 - val_loss: 2.3470 - val_accuracy: 0.6705\n",
      "Epoch 147/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5282 - accuracy: 0.7500\n",
      "Epoch 00147: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5417 - accuracy: 0.6932 - val_loss: 2.4812 - val_accuracy: 0.6705\n",
      "Epoch 148/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7517 - accuracy: 0.6250\n",
      "Epoch 00148: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5954 - accuracy: 0.6960 - val_loss: 2.6564 - val_accuracy: 0.6818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 149/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5996 - accuracy: 0.6250\n",
      "Epoch 00149: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5762 - accuracy: 0.6847 - val_loss: 2.5733 - val_accuracy: 0.6818\n",
      "Epoch 150/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4205 - accuracy: 0.7812\n",
      "Epoch 00150: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5514 - accuracy: 0.7131 - val_loss: 2.6498 - val_accuracy: 0.6705\n",
      "Epoch 151/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.3821 - accuracy: 0.7812\n",
      "Epoch 00151: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5487 - accuracy: 0.6875 - val_loss: 2.8677 - val_accuracy: 0.6591\n",
      "Epoch 152/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5418 - accuracy: 0.7500\n",
      "Epoch 00152: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5771 - accuracy: 0.6847 - val_loss: 2.4957 - val_accuracy: 0.6818\n",
      "Epoch 153/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5683 - accuracy: 0.6562\n",
      "Epoch 00153: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5570 - accuracy: 0.6648 - val_loss: 2.5093 - val_accuracy: 0.6818\n",
      "Epoch 154/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6742 - accuracy: 0.6250\n",
      "Epoch 00154: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6050 - accuracy: 0.6562 - val_loss: 2.7919 - val_accuracy: 0.6591\n",
      "Epoch 155/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6058 - accuracy: 0.6250\n",
      "Epoch 00155: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5856 - accuracy: 0.6676 - val_loss: 3.1586 - val_accuracy: 0.7045\n",
      "Epoch 156/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6781 - accuracy: 0.5625\n",
      "Epoch 00156: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5864 - accuracy: 0.6875 - val_loss: 3.1609 - val_accuracy: 0.6818\n",
      "Epoch 157/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.2945 - accuracy: 0.9062\n",
      "Epoch 00157: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5847 - accuracy: 0.6591 - val_loss: 3.0840 - val_accuracy: 0.6818\n",
      "Epoch 158/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6851 - accuracy: 0.6250\n",
      "Epoch 00158: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5699 - accuracy: 0.6790 - val_loss: 3.1051 - val_accuracy: 0.6705\n",
      "Epoch 159/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5439 - accuracy: 0.6875\n",
      "Epoch 00159: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5528 - accuracy: 0.6818 - val_loss: 3.3776 - val_accuracy: 0.6818\n",
      "Epoch 160/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6234 - accuracy: 0.7188\n",
      "Epoch 00160: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5634 - accuracy: 0.7131 - val_loss: 3.7767 - val_accuracy: 0.6818\n",
      "Epoch 161/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5883 - accuracy: 0.7812\n",
      "Epoch 00161: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5669 - accuracy: 0.6790 - val_loss: 3.1153 - val_accuracy: 0.6477\n",
      "Epoch 162/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6532 - accuracy: 0.6562\n",
      "Epoch 00162: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6030 - accuracy: 0.6648 - val_loss: 2.7610 - val_accuracy: 0.6477\n",
      "Epoch 163/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6200 - accuracy: 0.6562\n",
      "Epoch 00163: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5710 - accuracy: 0.6676 - val_loss: 3.0189 - val_accuracy: 0.6932\n",
      "Epoch 164/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4656 - accuracy: 0.7812\n",
      "Epoch 00164: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5953 - accuracy: 0.6705 - val_loss: 2.8111 - val_accuracy: 0.6818\n",
      "Epoch 165/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5115 - accuracy: 0.7500\n",
      "Epoch 00165: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5458 - accuracy: 0.6790 - val_loss: 3.0133 - val_accuracy: 0.6705\n",
      "Epoch 166/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4608 - accuracy: 0.8125\n",
      "Epoch 00166: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5517 - accuracy: 0.6989 - val_loss: 3.3171 - val_accuracy: 0.6818\n",
      "Epoch 167/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6731 - accuracy: 0.6875\n",
      "Epoch 00167: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5678 - accuracy: 0.6960 - val_loss: 3.6358 - val_accuracy: 0.6818\n",
      "Epoch 168/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6867 - accuracy: 0.6250\n",
      "Epoch 00168: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6312 - accuracy: 0.6307 - val_loss: 3.3047 - val_accuracy: 0.6818\n",
      "Epoch 169/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4955 - accuracy: 0.7500\n",
      "Epoch 00169: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5963 - accuracy: 0.6506 - val_loss: 2.9548 - val_accuracy: 0.6705\n",
      "Epoch 170/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7135 - accuracy: 0.5938\n",
      "Epoch 00170: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5602 - accuracy: 0.7045 - val_loss: 3.3580 - val_accuracy: 0.6591\n",
      "Epoch 171/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5939 - accuracy: 0.5625\n",
      "Epoch 00171: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5889 - accuracy: 0.6591 - val_loss: 3.3004 - val_accuracy: 0.6932\n",
      "Epoch 172/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.8041 - accuracy: 0.7500\n",
      "Epoch 00172: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5643 - accuracy: 0.7074 - val_loss: 3.3801 - val_accuracy: 0.6591\n",
      "Epoch 173/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4987 - accuracy: 0.6875\n",
      "Epoch 00173: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5605 - accuracy: 0.6790 - val_loss: 2.9494 - val_accuracy: 0.6705\n",
      "Epoch 174/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5507 - accuracy: 0.6562\n",
      "Epoch 00174: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5637 - accuracy: 0.6790 - val_loss: 3.3634 - val_accuracy: 0.6705\n",
      "Epoch 175/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6934 - accuracy: 0.5312\n",
      "Epoch 00175: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5889 - accuracy: 0.6818 - val_loss: 2.9638 - val_accuracy: 0.6705\n",
      "Epoch 176/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7624 - accuracy: 0.5000\n",
      "Epoch 00176: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5868 - accuracy: 0.6591 - val_loss: 3.1701 - val_accuracy: 0.6818\n",
      "Epoch 177/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7340 - accuracy: 0.5000\n",
      "Epoch 00177: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5663 - accuracy: 0.6989 - val_loss: 3.5028 - val_accuracy: 0.6705\n",
      "Epoch 178/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5584 - accuracy: 0.7812\n",
      "Epoch 00178: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5776 - accuracy: 0.6705 - val_loss: 4.0544 - val_accuracy: 0.6818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 179/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5856 - accuracy: 0.6250\n",
      "Epoch 00179: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6154 - accuracy: 0.6818 - val_loss: 3.6723 - val_accuracy: 0.6818\n",
      "Epoch 180/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6581 - accuracy: 0.6562\n",
      "Epoch 00180: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6222 - accuracy: 0.6733 - val_loss: 3.0501 - val_accuracy: 0.7045\n",
      "Epoch 181/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6741 - accuracy: 0.6875\n",
      "Epoch 00181: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5934 - accuracy: 0.6364 - val_loss: 3.1725 - val_accuracy: 0.6705\n",
      "Epoch 182/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7292 - accuracy: 0.5312\n",
      "Epoch 00182: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5581 - accuracy: 0.7017 - val_loss: 3.2858 - val_accuracy: 0.6818\n",
      "Epoch 183/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5843 - accuracy: 0.7500\n",
      "Epoch 00183: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5586 - accuracy: 0.7045 - val_loss: 2.7196 - val_accuracy: 0.6932\n",
      "Epoch 184/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4579 - accuracy: 0.6875\n",
      "Epoch 00184: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6348 - accuracy: 0.6278 - val_loss: 2.9548 - val_accuracy: 0.6932\n",
      "Epoch 185/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4854 - accuracy: 0.7188\n",
      "Epoch 00185: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5731 - accuracy: 0.6420 - val_loss: 3.1569 - val_accuracy: 0.6591\n",
      "Epoch 186/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6749 - accuracy: 0.5625\n",
      "Epoch 00186: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5732 - accuracy: 0.7045 - val_loss: 3.0265 - val_accuracy: 0.7045\n",
      "Epoch 187/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4016 - accuracy: 0.7188\n",
      "Epoch 00187: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5326 - accuracy: 0.7074 - val_loss: 3.2499 - val_accuracy: 0.6818\n",
      "Epoch 188/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5654 - accuracy: 0.6250\n",
      "Epoch 00188: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5875 - accuracy: 0.6534 - val_loss: 3.3474 - val_accuracy: 0.6591\n",
      "Epoch 189/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.8317 - accuracy: 0.5625\n",
      "Epoch 00189: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5927 - accuracy: 0.6733 - val_loss: 3.3757 - val_accuracy: 0.6705\n",
      "Epoch 190/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6269 - accuracy: 0.8125\n",
      "Epoch 00190: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5874 - accuracy: 0.6875 - val_loss: 3.2863 - val_accuracy: 0.7045\n",
      "Epoch 191/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6182 - accuracy: 0.6250\n",
      "Epoch 00191: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5704 - accuracy: 0.6648 - val_loss: 3.1432 - val_accuracy: 0.6932\n",
      "Epoch 192/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5358 - accuracy: 0.8125\n",
      "Epoch 00192: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5665 - accuracy: 0.7102 - val_loss: 3.3340 - val_accuracy: 0.6591\n",
      "Epoch 193/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6394 - accuracy: 0.6875\n",
      "Epoch 00193: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5989 - accuracy: 0.6761 - val_loss: 3.2869 - val_accuracy: 0.6477\n",
      "Epoch 194/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5407 - accuracy: 0.7500\n",
      "Epoch 00194: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5912 - accuracy: 0.6534 - val_loss: 3.4109 - val_accuracy: 0.6477\n",
      "Epoch 195/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6949 - accuracy: 0.6875\n",
      "Epoch 00195: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5938 - accuracy: 0.7102 - val_loss: 3.3711 - val_accuracy: 0.6932\n",
      "Epoch 196/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6440 - accuracy: 0.5938\n",
      "Epoch 00196: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6070 - accuracy: 0.6562 - val_loss: 3.5154 - val_accuracy: 0.6932\n",
      "Epoch 197/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6222 - accuracy: 0.6250\n",
      "Epoch 00197: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6204 - accuracy: 0.6562 - val_loss: 3.1579 - val_accuracy: 0.7045\n",
      "Epoch 198/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5303 - accuracy: 0.7812\n",
      "Epoch 00198: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5543 - accuracy: 0.6932 - val_loss: 3.1606 - val_accuracy: 0.6477\n",
      "Epoch 199/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5200 - accuracy: 0.7500\n",
      "Epoch 00199: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5928 - accuracy: 0.6903 - val_loss: 2.9126 - val_accuracy: 0.6818\n",
      "Epoch 200/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5565 - accuracy: 0.6250\n",
      "Epoch 00200: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5743 - accuracy: 0.6648 - val_loss: 2.5549 - val_accuracy: 0.6818\n",
      "Epoch 201/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5943 - accuracy: 0.7188\n",
      "Epoch 00201: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5877 - accuracy: 0.6591 - val_loss: 2.6675 - val_accuracy: 0.6591\n",
      "Epoch 202/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6168 - accuracy: 0.6875\n",
      "Epoch 00202: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5705 - accuracy: 0.7074 - val_loss: 3.0958 - val_accuracy: 0.6818\n",
      "Epoch 203/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6062 - accuracy: 0.6875\n",
      "Epoch 00203: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5460 - accuracy: 0.6960 - val_loss: 3.4838 - val_accuracy: 0.6591\n",
      "Epoch 204/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5205 - accuracy: 0.7188\n",
      "Epoch 00204: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5616 - accuracy: 0.6705 - val_loss: 3.6192 - val_accuracy: 0.6477\n",
      "Epoch 205/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6604 - accuracy: 0.6250\n",
      "Epoch 00205: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6398 - accuracy: 0.6420 - val_loss: 1.6920 - val_accuracy: 0.6818\n",
      "Epoch 206/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7877 - accuracy: 0.5000\n",
      "Epoch 00206: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6563 - accuracy: 0.6364 - val_loss: 1.5101 - val_accuracy: 0.6818\n",
      "Epoch 207/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6831 - accuracy: 0.6250\n",
      "Epoch 00207: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6573 - accuracy: 0.6392 - val_loss: 1.8608 - val_accuracy: 0.7045\n",
      "Epoch 208/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6250 - accuracy: 0.7812\n",
      "Epoch 00208: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6335 - accuracy: 0.6591 - val_loss: 1.5942 - val_accuracy: 0.6932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 209/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7380 - accuracy: 0.5312\n",
      "Epoch 00209: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6560 - accuracy: 0.6449 - val_loss: 1.7102 - val_accuracy: 0.7045\n",
      "Epoch 210/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4741 - accuracy: 0.6875\n",
      "Epoch 00210: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6647 - accuracy: 0.6619 - val_loss: 2.6174 - val_accuracy: 0.6818\n",
      "Epoch 211/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5488 - accuracy: 0.7500\n",
      "Epoch 00211: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6272 - accuracy: 0.6591 - val_loss: 2.3007 - val_accuracy: 0.6705\n",
      "Epoch 212/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5739 - accuracy: 0.6875\n",
      "Epoch 00212: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6105 - accuracy: 0.6875 - val_loss: 2.0356 - val_accuracy: 0.6818\n",
      "Epoch 213/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5241 - accuracy: 0.7188\n",
      "Epoch 00213: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6255 - accuracy: 0.6591 - val_loss: 1.8542 - val_accuracy: 0.7045\n",
      "Epoch 214/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6757 - accuracy: 0.6250\n",
      "Epoch 00214: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6369 - accuracy: 0.6506 - val_loss: 1.4918 - val_accuracy: 0.7159\n",
      "Epoch 215/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6788 - accuracy: 0.6875\n",
      "Epoch 00215: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6643 - accuracy: 0.6818 - val_loss: 1.3934 - val_accuracy: 0.6591\n",
      "Epoch 216/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5227 - accuracy: 0.7500\n",
      "Epoch 00216: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5992 - accuracy: 0.7017 - val_loss: 1.4208 - val_accuracy: 0.6932\n",
      "Epoch 217/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5713 - accuracy: 0.7188\n",
      "Epoch 00217: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6176 - accuracy: 0.7045 - val_loss: 1.3793 - val_accuracy: 0.6591\n",
      "Epoch 218/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7504 - accuracy: 0.5938\n",
      "Epoch 00218: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6382 - accuracy: 0.6648 - val_loss: 1.5049 - val_accuracy: 0.6818\n",
      "Epoch 219/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6059 - accuracy: 0.6562\n",
      "Epoch 00219: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6100 - accuracy: 0.6761 - val_loss: 1.7016 - val_accuracy: 0.6932\n",
      "Epoch 220/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5119 - accuracy: 0.7500\n",
      "Epoch 00220: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6396 - accuracy: 0.6364 - val_loss: 1.9397 - val_accuracy: 0.6932\n",
      "Epoch 221/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6249 - accuracy: 0.6562\n",
      "Epoch 00221: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5945 - accuracy: 0.6903 - val_loss: 1.9103 - val_accuracy: 0.6477\n",
      "Epoch 222/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6577 - accuracy: 0.7188\n",
      "Epoch 00222: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5862 - accuracy: 0.7074 - val_loss: 2.2911 - val_accuracy: 0.6932\n",
      "Epoch 223/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6120 - accuracy: 0.7500\n",
      "Epoch 00223: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6043 - accuracy: 0.6932 - val_loss: 2.7508 - val_accuracy: 0.6591\n",
      "Epoch 224/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4873 - accuracy: 0.7812\n",
      "Epoch 00224: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6387 - accuracy: 0.6534 - val_loss: 2.0049 - val_accuracy: 0.6591\n",
      "Epoch 225/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7821 - accuracy: 0.5625\n",
      "Epoch 00225: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6891 - accuracy: 0.6278 - val_loss: 1.5495 - val_accuracy: 0.6591\n",
      "Epoch 226/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6919 - accuracy: 0.5625\n",
      "Epoch 00226: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6410 - accuracy: 0.6420 - val_loss: 1.4278 - val_accuracy: 0.6591\n",
      "Epoch 227/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6072 - accuracy: 0.7188\n",
      "Epoch 00227: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6824 - accuracy: 0.6335 - val_loss: 1.4486 - val_accuracy: 0.6705\n",
      "Epoch 228/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5721 - accuracy: 0.6250\n",
      "Epoch 00228: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6575 - accuracy: 0.6506 - val_loss: 1.3474 - val_accuracy: 0.6705\n",
      "Epoch 229/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5252 - accuracy: 0.8438\n",
      "Epoch 00229: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5972 - accuracy: 0.6790 - val_loss: 1.7615 - val_accuracy: 0.6591\n",
      "Epoch 230/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7860 - accuracy: 0.6250\n",
      "Epoch 00230: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6386 - accuracy: 0.6506 - val_loss: 1.9388 - val_accuracy: 0.6705\n",
      "Epoch 231/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7210 - accuracy: 0.5312\n",
      "Epoch 00231: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5975 - accuracy: 0.6875 - val_loss: 3.2855 - val_accuracy: 0.6364\n",
      "Epoch 232/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.8811 - accuracy: 0.5312\n",
      "Epoch 00232: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5991 - accuracy: 0.6847 - val_loss: 2.7413 - val_accuracy: 0.6477\n",
      "Epoch 233/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6238 - accuracy: 0.6875\n",
      "Epoch 00233: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6006 - accuracy: 0.6903 - val_loss: 2.4557 - val_accuracy: 0.6591\n",
      "Epoch 234/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6653 - accuracy: 0.7188\n",
      "Epoch 00234: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5925 - accuracy: 0.6875 - val_loss: 2.4403 - val_accuracy: 0.6818\n",
      "Epoch 235/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4837 - accuracy: 0.7500\n",
      "Epoch 00235: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5780 - accuracy: 0.6903 - val_loss: 1.9043 - val_accuracy: 0.6818\n",
      "Epoch 236/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7179 - accuracy: 0.5625\n",
      "Epoch 00236: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6136 - accuracy: 0.6875 - val_loss: 1.7506 - val_accuracy: 0.6818\n",
      "Epoch 237/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7075 - accuracy: 0.6875\n",
      "Epoch 00237: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6505 - accuracy: 0.6818 - val_loss: 2.3813 - val_accuracy: 0.6705\n",
      "Epoch 238/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4890 - accuracy: 0.7500\n",
      "Epoch 00238: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5741 - accuracy: 0.6932 - val_loss: 2.8470 - val_accuracy: 0.6818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 239/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5115 - accuracy: 0.7188\n",
      "Epoch 00239: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6052 - accuracy: 0.6903 - val_loss: 2.4396 - val_accuracy: 0.6818\n",
      "Epoch 240/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6687 - accuracy: 0.5938\n",
      "Epoch 00240: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6273 - accuracy: 0.6591 - val_loss: 2.3903 - val_accuracy: 0.6932\n",
      "Epoch 241/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5419 - accuracy: 0.8125\n",
      "Epoch 00241: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6170 - accuracy: 0.6818 - val_loss: 2.2498 - val_accuracy: 0.7045\n",
      "Epoch 242/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6428 - accuracy: 0.5938\n",
      "Epoch 00242: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6004 - accuracy: 0.6591 - val_loss: 1.9753 - val_accuracy: 0.6818\n",
      "Epoch 243/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5148 - accuracy: 0.8125\n",
      "Epoch 00243: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5717 - accuracy: 0.7074 - val_loss: 1.9985 - val_accuracy: 0.6932\n",
      "Epoch 244/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6406 - accuracy: 0.6250\n",
      "Epoch 00244: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6161 - accuracy: 0.6790 - val_loss: 2.2402 - val_accuracy: 0.6932\n",
      "Epoch 245/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4248 - accuracy: 0.8125\n",
      "Epoch 00245: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5906 - accuracy: 0.6903 - val_loss: 2.3802 - val_accuracy: 0.6818\n",
      "Epoch 246/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5807 - accuracy: 0.6562\n",
      "Epoch 00246: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6170 - accuracy: 0.6591 - val_loss: 2.6685 - val_accuracy: 0.6705\n",
      "Epoch 247/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5550 - accuracy: 0.7500\n",
      "Epoch 00247: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5762 - accuracy: 0.6790 - val_loss: 2.8223 - val_accuracy: 0.6818\n",
      "Epoch 248/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.3935 - accuracy: 0.8438\n",
      "Epoch 00248: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5616 - accuracy: 0.6790 - val_loss: 2.5398 - val_accuracy: 0.6818\n",
      "Epoch 249/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4960 - accuracy: 0.7812\n",
      "Epoch 00249: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5449 - accuracy: 0.6989 - val_loss: 2.4626 - val_accuracy: 0.6591\n",
      "Epoch 250/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4219 - accuracy: 0.7188\n",
      "Epoch 00250: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5697 - accuracy: 0.6676 - val_loss: 2.5504 - val_accuracy: 0.6932\n",
      "Epoch 251/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6558 - accuracy: 0.6875\n",
      "Epoch 00251: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5599 - accuracy: 0.7017 - val_loss: 2.5361 - val_accuracy: 0.6818\n",
      "Epoch 252/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7269 - accuracy: 0.6562\n",
      "Epoch 00252: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5547 - accuracy: 0.6903 - val_loss: 2.7812 - val_accuracy: 0.6705\n",
      "Epoch 253/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5914 - accuracy: 0.6875\n",
      "Epoch 00253: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5579 - accuracy: 0.6989 - val_loss: 3.2279 - val_accuracy: 0.6591\n",
      "Epoch 254/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5819 - accuracy: 0.6875\n",
      "Epoch 00254: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5558 - accuracy: 0.6847 - val_loss: 2.8137 - val_accuracy: 0.6591\n",
      "Epoch 255/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4944 - accuracy: 0.7188\n",
      "Epoch 00255: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5453 - accuracy: 0.7188 - val_loss: 2.2585 - val_accuracy: 0.6591\n",
      "Epoch 256/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6363 - accuracy: 0.6562\n",
      "Epoch 00256: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5933 - accuracy: 0.6648 - val_loss: 2.3124 - val_accuracy: 0.6705\n",
      "Epoch 257/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6968 - accuracy: 0.7500\n",
      "Epoch 00257: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6204 - accuracy: 0.6818 - val_loss: 2.2546 - val_accuracy: 0.6932\n",
      "Epoch 258/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6846 - accuracy: 0.5625\n",
      "Epoch 00258: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6176 - accuracy: 0.6477 - val_loss: 2.2879 - val_accuracy: 0.6705\n",
      "Epoch 259/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7037 - accuracy: 0.5625\n",
      "Epoch 00259: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6119 - accuracy: 0.6875 - val_loss: 2.0312 - val_accuracy: 0.6818\n",
      "Epoch 260/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5636 - accuracy: 0.6250\n",
      "Epoch 00260: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5878 - accuracy: 0.6875 - val_loss: 2.3541 - val_accuracy: 0.6591\n",
      "Epoch 261/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6868 - accuracy: 0.6562\n",
      "Epoch 00261: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5851 - accuracy: 0.7045 - val_loss: 2.3299 - val_accuracy: 0.6932\n",
      "Epoch 262/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5950 - accuracy: 0.6875\n",
      "Epoch 00262: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5808 - accuracy: 0.6932 - val_loss: 2.1201 - val_accuracy: 0.6932\n",
      "Epoch 263/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5352 - accuracy: 0.7812\n",
      "Epoch 00263: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5921 - accuracy: 0.7017 - val_loss: 1.8367 - val_accuracy: 0.7045\n",
      "Epoch 264/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5089 - accuracy: 0.7188\n",
      "Epoch 00264: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6746 - accuracy: 0.6790 - val_loss: 1.7289 - val_accuracy: 0.7045\n",
      "Epoch 265/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6386 - accuracy: 0.5938\n",
      "Epoch 00265: val_loss did not improve from 1.16189\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7815 - accuracy: 0.6392 - val_loss: 1.3759 - val_accuracy: 0.6705\n",
      "Epoch 266/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6225 - accuracy: 0.6250\n",
      "Epoch 00266: val_loss improved from 1.16189 to 1.12311, saving model to save_models\\audio_classification.hdf5\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.6474 - accuracy: 0.6534 - val_loss: 1.1231 - val_accuracy: 0.6932\n",
      "Epoch 267/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6505 - accuracy: 0.5312\n",
      "Epoch 00267: val_loss improved from 1.12311 to 0.98357, saving model to save_models\\audio_classification.hdf5\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6264 - accuracy: 0.6761 - val_loss: 0.9836 - val_accuracy: 0.6932\n",
      "Epoch 268/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6597 - accuracy: 0.6562\n",
      "Epoch 00268: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6521 - accuracy: 0.6477 - val_loss: 1.1121 - val_accuracy: 0.6705\n",
      "Epoch 269/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6422 - accuracy: 0.5625\n",
      "Epoch 00269: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6448 - accuracy: 0.6506 - val_loss: 1.3168 - val_accuracy: 0.6818\n",
      "Epoch 270/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6493 - accuracy: 0.6250\n",
      "Epoch 00270: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5892 - accuracy: 0.6818 - val_loss: 1.5861 - val_accuracy: 0.7159\n",
      "Epoch 271/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5662 - accuracy: 0.7188\n",
      "Epoch 00271: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5806 - accuracy: 0.6648 - val_loss: 1.8941 - val_accuracy: 0.6818\n",
      "Epoch 272/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.3914 - accuracy: 0.8125\n",
      "Epoch 00272: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6116 - accuracy: 0.6818 - val_loss: 2.0436 - val_accuracy: 0.6818\n",
      "Epoch 273/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7170 - accuracy: 0.6562\n",
      "Epoch 00273: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6375 - accuracy: 0.6875 - val_loss: 2.3282 - val_accuracy: 0.6818\n",
      "Epoch 274/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5432 - accuracy: 0.6562\n",
      "Epoch 00274: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6459 - accuracy: 0.6562 - val_loss: 2.6790 - val_accuracy: 0.6932\n",
      "Epoch 275/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6059 - accuracy: 0.5625\n",
      "Epoch 00275: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6310 - accuracy: 0.6534 - val_loss: 1.9609 - val_accuracy: 0.6818\n",
      "Epoch 276/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6848 - accuracy: 0.6562\n",
      "Epoch 00276: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6632 - accuracy: 0.6108 - val_loss: 1.8301 - val_accuracy: 0.6932\n",
      "Epoch 277/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7964 - accuracy: 0.5312\n",
      "Epoch 00277: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6614 - accuracy: 0.6420 - val_loss: 1.7706 - val_accuracy: 0.6818\n",
      "Epoch 278/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7763 - accuracy: 0.5312\n",
      "Epoch 00278: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6908 - accuracy: 0.6193 - val_loss: 1.6207 - val_accuracy: 0.6932\n",
      "Epoch 279/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6593 - accuracy: 0.5625\n",
      "Epoch 00279: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6618 - accuracy: 0.6506 - val_loss: 2.0251 - val_accuracy: 0.6818\n",
      "Epoch 280/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7322 - accuracy: 0.6562\n",
      "Epoch 00280: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6588 - accuracy: 0.6534 - val_loss: 2.6756 - val_accuracy: 0.6932\n",
      "Epoch 281/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5955 - accuracy: 0.6250\n",
      "Epoch 00281: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6678 - accuracy: 0.6165 - val_loss: 2.6124 - val_accuracy: 0.6818\n",
      "Epoch 282/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6769 - accuracy: 0.6875\n",
      "Epoch 00282: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6405 - accuracy: 0.6619 - val_loss: 2.7704 - val_accuracy: 0.6932\n",
      "Epoch 283/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5393 - accuracy: 0.7500\n",
      "Epoch 00283: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6268 - accuracy: 0.6790 - val_loss: 2.9211 - val_accuracy: 0.6932\n",
      "Epoch 284/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7176 - accuracy: 0.6250\n",
      "Epoch 00284: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6359 - accuracy: 0.6847 - val_loss: 2.5078 - val_accuracy: 0.6818\n",
      "Epoch 285/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5434 - accuracy: 0.6875\n",
      "Epoch 00285: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6139 - accuracy: 0.6506 - val_loss: 2.9697 - val_accuracy: 0.6818\n",
      "Epoch 286/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7448 - accuracy: 0.6250\n",
      "Epoch 00286: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5728 - accuracy: 0.6989 - val_loss: 2.1552 - val_accuracy: 0.6705\n",
      "Epoch 287/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5810 - accuracy: 0.7188\n",
      "Epoch 00287: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6082 - accuracy: 0.6875 - val_loss: 2.1444 - val_accuracy: 0.6818\n",
      "Epoch 288/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5568 - accuracy: 0.7188\n",
      "Epoch 00288: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.5631 - accuracy: 0.7159 - val_loss: 2.5351 - val_accuracy: 0.6932\n",
      "Epoch 289/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6185 - accuracy: 0.7188\n",
      "Epoch 00289: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6070 - accuracy: 0.6705 - val_loss: 3.0440 - val_accuracy: 0.7045\n",
      "Epoch 290/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6544 - accuracy: 0.5938\n",
      "Epoch 00290: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6016 - accuracy: 0.6847 - val_loss: 3.3124 - val_accuracy: 0.7045\n",
      "Epoch 291/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6203 - accuracy: 0.6875\n",
      "Epoch 00291: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6383 - accuracy: 0.6420 - val_loss: 3.4593 - val_accuracy: 0.6932\n",
      "Epoch 292/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5389 - accuracy: 0.6875\n",
      "Epoch 00292: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6162 - accuracy: 0.6506 - val_loss: 3.0532 - val_accuracy: 0.6932\n",
      "Epoch 293/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4799 - accuracy: 0.7188\n",
      "Epoch 00293: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5933 - accuracy: 0.7017 - val_loss: 2.9319 - val_accuracy: 0.6818\n",
      "Epoch 294/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5367 - accuracy: 0.6875\n",
      "Epoch 00294: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5417 - accuracy: 0.7045 - val_loss: 2.4948 - val_accuracy: 0.6818\n",
      "Epoch 295/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4962 - accuracy: 0.6875\n",
      "Epoch 00295: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5727 - accuracy: 0.6648 - val_loss: 2.7367 - val_accuracy: 0.6705\n",
      "Epoch 296/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5703 - accuracy: 0.6250\n",
      "Epoch 00296: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5707 - accuracy: 0.6989 - val_loss: 2.9650 - val_accuracy: 0.6818\n",
      "Epoch 297/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4909 - accuracy: 0.7188\n",
      "Epoch 00297: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5811 - accuracy: 0.7017 - val_loss: 2.7008 - val_accuracy: 0.6705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 298/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4742 - accuracy: 0.7812\n",
      "Epoch 00298: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.5718 - accuracy: 0.6591 - val_loss: 2.8652 - val_accuracy: 0.7045\n",
      "Epoch 299/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5831 - accuracy: 0.6875\n",
      "Epoch 00299: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5772 - accuracy: 0.7017 - val_loss: 3.2689 - val_accuracy: 0.7045\n",
      "Epoch 300/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6389 - accuracy: 0.6250\n",
      "Epoch 00300: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6056 - accuracy: 0.6705 - val_loss: 2.8243 - val_accuracy: 0.6932\n",
      "Epoch 301/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6358 - accuracy: 0.7188\n",
      "Epoch 00301: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5541 - accuracy: 0.6875 - val_loss: 2.7777 - val_accuracy: 0.7159\n",
      "Epoch 302/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5631 - accuracy: 0.7500\n",
      "Epoch 00302: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5659 - accuracy: 0.7159 - val_loss: 2.8725 - val_accuracy: 0.6818\n",
      "Epoch 303/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4698 - accuracy: 0.7500\n",
      "Epoch 00303: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5840 - accuracy: 0.6591 - val_loss: 2.7084 - val_accuracy: 0.6818\n",
      "Epoch 304/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5824 - accuracy: 0.7188\n",
      "Epoch 00304: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5745 - accuracy: 0.6591 - val_loss: 2.8696 - val_accuracy: 0.7045\n",
      "Epoch 305/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5902 - accuracy: 0.6562\n",
      "Epoch 00305: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5805 - accuracy: 0.6477 - val_loss: 3.1267 - val_accuracy: 0.7045\n",
      "Epoch 306/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5402 - accuracy: 0.7500\n",
      "Epoch 00306: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5951 - accuracy: 0.6676 - val_loss: 3.1514 - val_accuracy: 0.6705\n",
      "Epoch 307/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4371 - accuracy: 0.8125\n",
      "Epoch 00307: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5579 - accuracy: 0.7131 - val_loss: 3.0305 - val_accuracy: 0.6818\n",
      "Epoch 308/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5172 - accuracy: 0.6562\n",
      "Epoch 00308: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5801 - accuracy: 0.6875 - val_loss: 2.1903 - val_accuracy: 0.6705\n",
      "Epoch 309/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5800 - accuracy: 0.7500\n",
      "Epoch 00309: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5865 - accuracy: 0.7017 - val_loss: 2.3227 - val_accuracy: 0.6818\n",
      "Epoch 310/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6430 - accuracy: 0.5938\n",
      "Epoch 00310: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6558 - accuracy: 0.5824 - val_loss: 1.8094 - val_accuracy: 0.6591\n",
      "Epoch 311/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6070 - accuracy: 0.6562\n",
      "Epoch 00311: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6135 - accuracy: 0.6591 - val_loss: 1.7834 - val_accuracy: 0.6136\n",
      "Epoch 312/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4902 - accuracy: 0.6562\n",
      "Epoch 00312: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6361 - accuracy: 0.6165 - val_loss: 1.8083 - val_accuracy: 0.6591\n",
      "Epoch 313/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6146 - accuracy: 0.6562\n",
      "Epoch 00313: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6552 - accuracy: 0.6591 - val_loss: 1.5587 - val_accuracy: 0.6477\n",
      "Epoch 314/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.8380 - accuracy: 0.5312\n",
      "Epoch 00314: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.7145 - accuracy: 0.6136 - val_loss: 1.7879 - val_accuracy: 0.6250\n",
      "Epoch 315/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5970 - accuracy: 0.7188\n",
      "Epoch 00315: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6391 - accuracy: 0.6562 - val_loss: 1.4820 - val_accuracy: 0.6477\n",
      "Epoch 316/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5279 - accuracy: 0.8125\n",
      "Epoch 00316: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6541 - accuracy: 0.6676 - val_loss: 1.3372 - val_accuracy: 0.6705\n",
      "Epoch 317/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.9909 - accuracy: 0.4688\n",
      "Epoch 00317: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6766 - accuracy: 0.6335 - val_loss: 1.1743 - val_accuracy: 0.6477\n",
      "Epoch 318/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6960 - accuracy: 0.5312\n",
      "Epoch 00318: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6571 - accuracy: 0.6307 - val_loss: 1.7404 - val_accuracy: 0.7045\n",
      "Epoch 319/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4327 - accuracy: 0.7500\n",
      "Epoch 00319: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6326 - accuracy: 0.6392 - val_loss: 1.8237 - val_accuracy: 0.7045\n",
      "Epoch 320/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6017 - accuracy: 0.6250\n",
      "Epoch 00320: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6634 - accuracy: 0.6023 - val_loss: 1.3669 - val_accuracy: 0.6818\n",
      "Epoch 321/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6495 - accuracy: 0.6250\n",
      "Epoch 00321: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6666 - accuracy: 0.6335 - val_loss: 1.0273 - val_accuracy: 0.6932\n",
      "Epoch 322/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5400 - accuracy: 0.7812\n",
      "Epoch 00322: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6539 - accuracy: 0.6420 - val_loss: 1.0662 - val_accuracy: 0.6591\n",
      "Epoch 323/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5410 - accuracy: 0.7812\n",
      "Epoch 00323: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6364 - accuracy: 0.6591 - val_loss: 1.1863 - val_accuracy: 0.6591\n",
      "Epoch 324/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5047 - accuracy: 0.7500\n",
      "Epoch 00324: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6717 - accuracy: 0.6193 - val_loss: 1.2401 - val_accuracy: 0.6818\n",
      "Epoch 325/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6518 - accuracy: 0.7500\n",
      "Epoch 00325: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6832 - accuracy: 0.6108 - val_loss: 1.1741 - val_accuracy: 0.6591\n",
      "Epoch 326/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6439 - accuracy: 0.5938\n",
      "Epoch 00326: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6500 - accuracy: 0.6080 - val_loss: 1.1785 - val_accuracy: 0.6818\n",
      "Epoch 327/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4942 - accuracy: 0.8438\n",
      "Epoch 00327: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6647 - accuracy: 0.6562 - val_loss: 1.3386 - val_accuracy: 0.6932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 328/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5814 - accuracy: 0.5625\n",
      "Epoch 00328: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6705 - accuracy: 0.5966 - val_loss: 1.6218 - val_accuracy: 0.6705\n",
      "Epoch 329/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6668 - accuracy: 0.6250\n",
      "Epoch 00329: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6462 - accuracy: 0.6222 - val_loss: 1.9496 - val_accuracy: 0.6818\n",
      "Epoch 330/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5590 - accuracy: 0.7188\n",
      "Epoch 00330: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6703 - accuracy: 0.6165 - val_loss: 2.2142 - val_accuracy: 0.6477\n",
      "Epoch 331/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7863 - accuracy: 0.5625\n",
      "Epoch 00331: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6958 - accuracy: 0.6364 - val_loss: 1.8868 - val_accuracy: 0.6705\n",
      "Epoch 332/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6361 - accuracy: 0.5625\n",
      "Epoch 00332: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6466 - accuracy: 0.6648 - val_loss: 1.8563 - val_accuracy: 0.6591\n",
      "Epoch 333/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5343 - accuracy: 0.6875\n",
      "Epoch 00333: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6442 - accuracy: 0.6562 - val_loss: 2.1317 - val_accuracy: 0.6591\n",
      "Epoch 334/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4980 - accuracy: 0.7500\n",
      "Epoch 00334: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6452 - accuracy: 0.6705 - val_loss: 2.0754 - val_accuracy: 0.6818\n",
      "Epoch 335/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.3866 - accuracy: 0.7188\n",
      "Epoch 00335: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6019 - accuracy: 0.6705 - val_loss: 1.4768 - val_accuracy: 0.6932\n",
      "Epoch 336/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5650 - accuracy: 0.7188\n",
      "Epoch 00336: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6395 - accuracy: 0.6364 - val_loss: 1.3070 - val_accuracy: 0.6818\n",
      "Epoch 337/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6365 - accuracy: 0.5938\n",
      "Epoch 00337: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6285 - accuracy: 0.6335 - val_loss: 1.5621 - val_accuracy: 0.6705\n",
      "Epoch 338/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6919 - accuracy: 0.5625\n",
      "Epoch 00338: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6671 - accuracy: 0.6222 - val_loss: 1.8872 - val_accuracy: 0.6591\n",
      "Epoch 339/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5363 - accuracy: 0.7500\n",
      "Epoch 00339: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6159 - accuracy: 0.6818 - val_loss: 1.6457 - val_accuracy: 0.6818\n",
      "Epoch 340/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7266 - accuracy: 0.6250\n",
      "Epoch 00340: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6198 - accuracy: 0.6449 - val_loss: 1.8189 - val_accuracy: 0.6818\n",
      "Epoch 341/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5298 - accuracy: 0.7188\n",
      "Epoch 00341: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6610 - accuracy: 0.6250 - val_loss: 1.8372 - val_accuracy: 0.7159\n",
      "Epoch 342/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6856 - accuracy: 0.5938\n",
      "Epoch 00342: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6570 - accuracy: 0.6392 - val_loss: 1.5236 - val_accuracy: 0.7159\n",
      "Epoch 343/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5654 - accuracy: 0.6875\n",
      "Epoch 00343: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6540 - accuracy: 0.6307 - val_loss: 1.4003 - val_accuracy: 0.7159\n",
      "Epoch 344/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7511 - accuracy: 0.5312\n",
      "Epoch 00344: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6424 - accuracy: 0.6335 - val_loss: 1.3041 - val_accuracy: 0.7159\n",
      "Epoch 345/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7625 - accuracy: 0.6875\n",
      "Epoch 00345: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6669 - accuracy: 0.6477 - val_loss: 1.7097 - val_accuracy: 0.6932\n",
      "Epoch 346/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7337 - accuracy: 0.6250\n",
      "Epoch 00346: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6257 - accuracy: 0.6534 - val_loss: 2.2656 - val_accuracy: 0.6705\n",
      "Epoch 347/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6506 - accuracy: 0.6562\n",
      "Epoch 00347: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6704 - accuracy: 0.6506 - val_loss: 2.2194 - val_accuracy: 0.6932\n",
      "Epoch 348/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6061 - accuracy: 0.6250\n",
      "Epoch 00348: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6273 - accuracy: 0.6818 - val_loss: 2.5410 - val_accuracy: 0.6477\n",
      "Epoch 349/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6453 - accuracy: 0.6875\n",
      "Epoch 00349: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6092 - accuracy: 0.6818 - val_loss: 2.3843 - val_accuracy: 0.6705\n",
      "Epoch 350/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7280 - accuracy: 0.5625\n",
      "Epoch 00350: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6051 - accuracy: 0.6676 - val_loss: 2.4429 - val_accuracy: 0.6818\n",
      "Epoch 351/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4641 - accuracy: 0.7500\n",
      "Epoch 00351: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6032 - accuracy: 0.6705 - val_loss: 3.0539 - val_accuracy: 0.6705\n",
      "Epoch 352/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7083 - accuracy: 0.5312\n",
      "Epoch 00352: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6213 - accuracy: 0.6676 - val_loss: 2.8098 - val_accuracy: 0.6818\n",
      "Epoch 353/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4698 - accuracy: 0.7500\n",
      "Epoch 00353: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5918 - accuracy: 0.6648 - val_loss: 2.6062 - val_accuracy: 0.6591\n",
      "Epoch 354/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4296 - accuracy: 0.8125\n",
      "Epoch 00354: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6421 - accuracy: 0.6676 - val_loss: 2.1534 - val_accuracy: 0.6932\n",
      "Epoch 355/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7064 - accuracy: 0.6562\n",
      "Epoch 00355: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6413 - accuracy: 0.6562 - val_loss: 2.3193 - val_accuracy: 0.6591\n",
      "Epoch 356/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.8396 - accuracy: 0.4688\n",
      "Epoch 00356: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6415 - accuracy: 0.6506 - val_loss: 2.5883 - val_accuracy: 0.6818\n",
      "Epoch 357/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6423 - accuracy: 0.6250\n",
      "Epoch 00357: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6435 - accuracy: 0.6392 - val_loss: 2.0160 - val_accuracy: 0.6705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 358/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5711 - accuracy: 0.7500\n",
      "Epoch 00358: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6170 - accuracy: 0.6420 - val_loss: 2.0232 - val_accuracy: 0.6591\n",
      "Epoch 359/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5995 - accuracy: 0.6562\n",
      "Epoch 00359: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6491 - accuracy: 0.6335 - val_loss: 2.0405 - val_accuracy: 0.6705\n",
      "Epoch 360/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5866 - accuracy: 0.8125\n",
      "Epoch 00360: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6359 - accuracy: 0.6648 - val_loss: 2.2994 - val_accuracy: 0.6591\n",
      "Epoch 361/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6282 - accuracy: 0.6875\n",
      "Epoch 00361: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6321 - accuracy: 0.6562 - val_loss: 2.2332 - val_accuracy: 0.6818\n",
      "Epoch 362/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5337 - accuracy: 0.8125\n",
      "Epoch 00362: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6234 - accuracy: 0.6761 - val_loss: 2.1912 - val_accuracy: 0.6932\n",
      "Epoch 363/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7223 - accuracy: 0.6562\n",
      "Epoch 00363: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6230 - accuracy: 0.6932 - val_loss: 2.4926 - val_accuracy: 0.6591\n",
      "Epoch 364/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5158 - accuracy: 0.7812\n",
      "Epoch 00364: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6227 - accuracy: 0.6733 - val_loss: 2.3416 - val_accuracy: 0.6591\n",
      "Epoch 365/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.8039 - accuracy: 0.5000\n",
      "Epoch 00365: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6394 - accuracy: 0.6420 - val_loss: 2.4311 - val_accuracy: 0.6705\n",
      "Epoch 366/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.9220 - accuracy: 0.4688\n",
      "Epoch 00366: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6688 - accuracy: 0.6307 - val_loss: 2.5591 - val_accuracy: 0.6591\n",
      "Epoch 367/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7565 - accuracy: 0.5312\n",
      "Epoch 00367: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6226 - accuracy: 0.6790 - val_loss: 2.4638 - val_accuracy: 0.6591\n",
      "Epoch 368/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6122 - accuracy: 0.5938\n",
      "Epoch 00368: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6435 - accuracy: 0.6420 - val_loss: 2.2341 - val_accuracy: 0.6364\n",
      "Epoch 369/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6070 - accuracy: 0.6562\n",
      "Epoch 00369: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6338 - accuracy: 0.6506 - val_loss: 2.2406 - val_accuracy: 0.6705\n",
      "Epoch 370/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7698 - accuracy: 0.5000\n",
      "Epoch 00370: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6656 - accuracy: 0.6307 - val_loss: 2.3315 - val_accuracy: 0.6705\n",
      "Epoch 371/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5672 - accuracy: 0.7812\n",
      "Epoch 00371: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6469 - accuracy: 0.6506 - val_loss: 2.5923 - val_accuracy: 0.6591\n",
      "Epoch 372/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6643 - accuracy: 0.5625\n",
      "Epoch 00372: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6571 - accuracy: 0.6420 - val_loss: 2.7259 - val_accuracy: 0.6705\n",
      "Epoch 373/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6194 - accuracy: 0.6562\n",
      "Epoch 00373: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6380 - accuracy: 0.6733 - val_loss: 2.8783 - val_accuracy: 0.6591\n",
      "Epoch 374/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5346 - accuracy: 0.8438\n",
      "Epoch 00374: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5992 - accuracy: 0.6761 - val_loss: 3.2189 - val_accuracy: 0.6477\n",
      "Epoch 375/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5196 - accuracy: 0.6875\n",
      "Epoch 00375: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6007 - accuracy: 0.6847 - val_loss: 2.3255 - val_accuracy: 0.6818\n",
      "Epoch 376/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6199 - accuracy: 0.5625\n",
      "Epoch 00376: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6327 - accuracy: 0.6733 - val_loss: 2.4488 - val_accuracy: 0.6932\n",
      "Epoch 377/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5456 - accuracy: 0.7500\n",
      "Epoch 00377: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6186 - accuracy: 0.6733 - val_loss: 2.8059 - val_accuracy: 0.6477\n",
      "Epoch 378/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6100 - accuracy: 0.7188\n",
      "Epoch 00378: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6108 - accuracy: 0.6875 - val_loss: 3.0665 - val_accuracy: 0.6591\n",
      "Epoch 379/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7470 - accuracy: 0.5938\n",
      "Epoch 00379: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6305 - accuracy: 0.6648 - val_loss: 3.5540 - val_accuracy: 0.6477\n",
      "Epoch 380/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7109 - accuracy: 0.6562\n",
      "Epoch 00380: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6102 - accuracy: 0.6790 - val_loss: 2.6829 - val_accuracy: 0.6591\n",
      "Epoch 381/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6729 - accuracy: 0.5938\n",
      "Epoch 00381: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6517 - accuracy: 0.6591 - val_loss: 3.0096 - val_accuracy: 0.6705\n",
      "Epoch 382/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5703 - accuracy: 0.6250\n",
      "Epoch 00382: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6481 - accuracy: 0.6591 - val_loss: 2.4938 - val_accuracy: 0.6932\n",
      "Epoch 383/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6541 - accuracy: 0.7188\n",
      "Epoch 00383: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6608 - accuracy: 0.6392 - val_loss: 1.7421 - val_accuracy: 0.7045\n",
      "Epoch 384/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7455 - accuracy: 0.6875\n",
      "Epoch 00384: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6803 - accuracy: 0.6477 - val_loss: 1.5248 - val_accuracy: 0.6591\n",
      "Epoch 385/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4230 - accuracy: 0.7812\n",
      "Epoch 00385: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7055 - accuracy: 0.6051 - val_loss: 1.6800 - val_accuracy: 0.6932\n",
      "Epoch 386/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5417 - accuracy: 0.6562\n",
      "Epoch 00386: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6623 - accuracy: 0.6222 - val_loss: 1.8773 - val_accuracy: 0.6705\n",
      "Epoch 387/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7370 - accuracy: 0.6875\n",
      "Epoch 00387: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6761 - accuracy: 0.6136 - val_loss: 2.0923 - val_accuracy: 0.6591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 388/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5174 - accuracy: 0.7812\n",
      "Epoch 00388: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6443 - accuracy: 0.6420 - val_loss: 2.5632 - val_accuracy: 0.6477\n",
      "Epoch 389/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5933 - accuracy: 0.6562\n",
      "Epoch 00389: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6113 - accuracy: 0.6562 - val_loss: 2.5623 - val_accuracy: 0.6705\n",
      "Epoch 390/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6991 - accuracy: 0.6562\n",
      "Epoch 00390: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6282 - accuracy: 0.6420 - val_loss: 2.3690 - val_accuracy: 0.6705\n",
      "Epoch 391/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6867 - accuracy: 0.6875\n",
      "Epoch 00391: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6356 - accuracy: 0.6278 - val_loss: 2.4544 - val_accuracy: 0.6477\n",
      "Epoch 392/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7043 - accuracy: 0.7188\n",
      "Epoch 00392: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6522 - accuracy: 0.6477 - val_loss: 2.7669 - val_accuracy: 0.6477\n",
      "Epoch 393/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5129 - accuracy: 0.7812\n",
      "Epoch 00393: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6371 - accuracy: 0.6534 - val_loss: 2.1337 - val_accuracy: 0.6705\n",
      "Epoch 394/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6234 - accuracy: 0.5312\n",
      "Epoch 00394: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6349 - accuracy: 0.6449 - val_loss: 2.0313 - val_accuracy: 0.6591\n",
      "Epoch 395/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7667 - accuracy: 0.5312\n",
      "Epoch 00395: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6499 - accuracy: 0.6619 - val_loss: 2.5877 - val_accuracy: 0.6818\n",
      "Epoch 396/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6311 - accuracy: 0.6875\n",
      "Epoch 00396: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6649 - accuracy: 0.6562 - val_loss: 2.9227 - val_accuracy: 0.7273\n",
      "Epoch 397/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6080 - accuracy: 0.6562\n",
      "Epoch 00397: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6198 - accuracy: 0.6619 - val_loss: 2.0398 - val_accuracy: 0.6705\n",
      "Epoch 398/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7646 - accuracy: 0.5000\n",
      "Epoch 00398: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6865 - accuracy: 0.6108 - val_loss: 1.9548 - val_accuracy: 0.6818\n",
      "Epoch 399/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7594 - accuracy: 0.6250\n",
      "Epoch 00399: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6296 - accuracy: 0.6392 - val_loss: 2.2837 - val_accuracy: 0.6818\n",
      "Epoch 400/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7673 - accuracy: 0.6875\n",
      "Epoch 00400: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6523 - accuracy: 0.6477 - val_loss: 1.6436 - val_accuracy: 0.6705\n",
      "Epoch 401/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4727 - accuracy: 0.7812\n",
      "Epoch 00401: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6508 - accuracy: 0.6222 - val_loss: 1.8049 - val_accuracy: 0.6818\n",
      "Epoch 402/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6636 - accuracy: 0.5938\n",
      "Epoch 00402: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6671 - accuracy: 0.6477 - val_loss: 2.2272 - val_accuracy: 0.6591\n",
      "Epoch 403/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6764 - accuracy: 0.5938\n",
      "Epoch 00403: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6360 - accuracy: 0.6648 - val_loss: 1.5921 - val_accuracy: 0.6591\n",
      "Epoch 404/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7969 - accuracy: 0.5938\n",
      "Epoch 00404: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6543 - accuracy: 0.6392 - val_loss: 2.0949 - val_accuracy: 0.6477\n",
      "Epoch 405/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5755 - accuracy: 0.7188\n",
      "Epoch 00405: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6339 - accuracy: 0.6591 - val_loss: 1.8140 - val_accuracy: 0.6818\n",
      "Epoch 406/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7093 - accuracy: 0.5938\n",
      "Epoch 00406: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6324 - accuracy: 0.6591 - val_loss: 1.7736 - val_accuracy: 0.6818\n",
      "Epoch 407/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5517 - accuracy: 0.7188\n",
      "Epoch 00407: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6082 - accuracy: 0.6875 - val_loss: 2.0786 - val_accuracy: 0.6818\n",
      "Epoch 408/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6867 - accuracy: 0.6250\n",
      "Epoch 00408: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6160 - accuracy: 0.6648 - val_loss: 2.8432 - val_accuracy: 0.6932\n",
      "Epoch 409/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7734 - accuracy: 0.6250\n",
      "Epoch 00409: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6378 - accuracy: 0.6562 - val_loss: 3.2770 - val_accuracy: 0.6705\n",
      "Epoch 410/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5715 - accuracy: 0.7500\n",
      "Epoch 00410: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6174 - accuracy: 0.6676 - val_loss: 3.0804 - val_accuracy: 0.6705\n",
      "Epoch 411/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6460 - accuracy: 0.6875\n",
      "Epoch 00411: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5912 - accuracy: 0.6875 - val_loss: 3.3722 - val_accuracy: 0.6705\n",
      "Epoch 412/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6689 - accuracy: 0.5625\n",
      "Epoch 00412: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6391 - accuracy: 0.6591 - val_loss: 3.5675 - val_accuracy: 0.6591\n",
      "Epoch 413/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6205 - accuracy: 0.7188\n",
      "Epoch 00413: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6341 - accuracy: 0.6676 - val_loss: 3.3286 - val_accuracy: 0.6591\n",
      "Epoch 414/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6945 - accuracy: 0.5000\n",
      "Epoch 00414: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6440 - accuracy: 0.6420 - val_loss: 3.0565 - val_accuracy: 0.6818\n",
      "Epoch 415/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5243 - accuracy: 0.7500\n",
      "Epoch 00415: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6173 - accuracy: 0.6648 - val_loss: 2.9663 - val_accuracy: 0.6705\n",
      "Epoch 416/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6312 - accuracy: 0.6562\n",
      "Epoch 00416: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6196 - accuracy: 0.6506 - val_loss: 3.3866 - val_accuracy: 0.6818\n",
      "Epoch 417/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6510 - accuracy: 0.7500\n",
      "Epoch 00417: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6077 - accuracy: 0.6733 - val_loss: 3.4733 - val_accuracy: 0.6818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 418/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5235 - accuracy: 0.7188\n",
      "Epoch 00418: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6276 - accuracy: 0.6420 - val_loss: 3.1285 - val_accuracy: 0.6818\n",
      "Epoch 419/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6947 - accuracy: 0.6562\n",
      "Epoch 00419: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6476 - accuracy: 0.6335 - val_loss: 2.9610 - val_accuracy: 0.6818\n",
      "Epoch 420/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5965 - accuracy: 0.6875\n",
      "Epoch 00420: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6236 - accuracy: 0.6506 - val_loss: 2.8149 - val_accuracy: 0.6932\n",
      "Epoch 421/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6349 - accuracy: 0.6562\n",
      "Epoch 00421: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5792 - accuracy: 0.6847 - val_loss: 2.7733 - val_accuracy: 0.6932\n",
      "Epoch 422/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5907 - accuracy: 0.7188\n",
      "Epoch 00422: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6428 - accuracy: 0.6534 - val_loss: 3.1041 - val_accuracy: 0.6705\n",
      "Epoch 423/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7252 - accuracy: 0.5625\n",
      "Epoch 00423: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6467 - accuracy: 0.6534 - val_loss: 3.0367 - val_accuracy: 0.6705\n",
      "Epoch 424/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6789 - accuracy: 0.5938\n",
      "Epoch 00424: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6173 - accuracy: 0.6818 - val_loss: 3.1783 - val_accuracy: 0.7045\n",
      "Epoch 425/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6089 - accuracy: 0.6562\n",
      "Epoch 00425: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5995 - accuracy: 0.6761 - val_loss: 2.7066 - val_accuracy: 0.6818\n",
      "Epoch 426/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5478 - accuracy: 0.7812\n",
      "Epoch 00426: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5898 - accuracy: 0.7045 - val_loss: 2.8007 - val_accuracy: 0.6818\n",
      "Epoch 427/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6159 - accuracy: 0.5938\n",
      "Epoch 00427: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5920 - accuracy: 0.6847 - val_loss: 3.1032 - val_accuracy: 0.6818\n",
      "Epoch 428/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5114 - accuracy: 0.6875\n",
      "Epoch 00428: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6241 - accuracy: 0.6506 - val_loss: 3.5837 - val_accuracy: 0.7045\n",
      "Epoch 429/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7057 - accuracy: 0.7188\n",
      "Epoch 00429: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5971 - accuracy: 0.6875 - val_loss: 3.5592 - val_accuracy: 0.6818\n",
      "Epoch 430/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6999 - accuracy: 0.5938\n",
      "Epoch 00430: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6759 - accuracy: 0.6676 - val_loss: 3.1915 - val_accuracy: 0.6932\n",
      "Epoch 431/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6612 - accuracy: 0.5938\n",
      "Epoch 00431: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6082 - accuracy: 0.6847 - val_loss: 2.6946 - val_accuracy: 0.6705\n",
      "Epoch 432/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6909 - accuracy: 0.6250\n",
      "Epoch 00432: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6201 - accuracy: 0.6506 - val_loss: 2.0952 - val_accuracy: 0.6705\n",
      "Epoch 433/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5700 - accuracy: 0.7500\n",
      "Epoch 00433: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6329 - accuracy: 0.6733 - val_loss: 1.9918 - val_accuracy: 0.6932\n",
      "Epoch 434/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6315 - accuracy: 0.6562\n",
      "Epoch 00434: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6600 - accuracy: 0.6449 - val_loss: 1.4563 - val_accuracy: 0.6818\n",
      "Epoch 435/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5096 - accuracy: 0.7500\n",
      "Epoch 00435: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6133 - accuracy: 0.6648 - val_loss: 1.6502 - val_accuracy: 0.6705\n",
      "Epoch 436/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7765 - accuracy: 0.5938\n",
      "Epoch 00436: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6384 - accuracy: 0.6477 - val_loss: 1.6969 - val_accuracy: 0.6932\n",
      "Epoch 437/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6404 - accuracy: 0.7188\n",
      "Epoch 00437: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6877 - accuracy: 0.6108 - val_loss: 1.4429 - val_accuracy: 0.6818\n",
      "Epoch 438/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7253 - accuracy: 0.5312\n",
      "Epoch 00438: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6470 - accuracy: 0.6392 - val_loss: 1.5908 - val_accuracy: 0.6818\n",
      "Epoch 439/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6521 - accuracy: 0.6562\n",
      "Epoch 00439: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6339 - accuracy: 0.6392 - val_loss: 1.6040 - val_accuracy: 0.6932\n",
      "Epoch 440/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6421 - accuracy: 0.7500\n",
      "Epoch 00440: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6578 - accuracy: 0.6392 - val_loss: 1.7623 - val_accuracy: 0.6705\n",
      "Epoch 441/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7233 - accuracy: 0.5938\n",
      "Epoch 00441: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6510 - accuracy: 0.6449 - val_loss: 2.1622 - val_accuracy: 0.6477\n",
      "Epoch 442/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6243 - accuracy: 0.5938\n",
      "Epoch 00442: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6853 - accuracy: 0.6250 - val_loss: 1.5695 - val_accuracy: 0.6818\n",
      "Epoch 443/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7141 - accuracy: 0.5625\n",
      "Epoch 00443: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6694 - accuracy: 0.6278 - val_loss: 1.5027 - val_accuracy: 0.7045\n",
      "Epoch 444/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7002 - accuracy: 0.6875\n",
      "Epoch 00444: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6329 - accuracy: 0.6591 - val_loss: 1.9743 - val_accuracy: 0.6932\n",
      "Epoch 445/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7528 - accuracy: 0.6562\n",
      "Epoch 00445: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.6612 - accuracy: 0.6648 - val_loss: 2.1262 - val_accuracy: 0.6932\n",
      "Epoch 446/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.8043 - accuracy: 0.5938\n",
      "Epoch 00446: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6779 - accuracy: 0.6136 - val_loss: 1.7706 - val_accuracy: 0.7045\n",
      "Epoch 447/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5794 - accuracy: 0.6562\n",
      "Epoch 00447: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6150 - accuracy: 0.6619 - val_loss: 1.9808 - val_accuracy: 0.6932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 448/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5386 - accuracy: 0.7812\n",
      "Epoch 00448: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6376 - accuracy: 0.6477 - val_loss: 2.0537 - val_accuracy: 0.6591\n",
      "Epoch 449/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.8148 - accuracy: 0.6875\n",
      "Epoch 00449: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6094 - accuracy: 0.6960 - val_loss: 2.1312 - val_accuracy: 0.7159\n",
      "Epoch 450/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6646 - accuracy: 0.6250\n",
      "Epoch 00450: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6544 - accuracy: 0.6562 - val_loss: 2.3226 - val_accuracy: 0.6932\n",
      "Epoch 451/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6831 - accuracy: 0.5938\n",
      "Epoch 00451: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6420 - accuracy: 0.6506 - val_loss: 2.5342 - val_accuracy: 0.6932\n",
      "Epoch 452/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5047 - accuracy: 0.7188\n",
      "Epoch 00452: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6606 - accuracy: 0.6534 - val_loss: 2.6560 - val_accuracy: 0.6818\n",
      "Epoch 453/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5842 - accuracy: 0.6250\n",
      "Epoch 00453: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5973 - accuracy: 0.6790 - val_loss: 2.7676 - val_accuracy: 0.6818\n",
      "Epoch 454/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5769 - accuracy: 0.7188\n",
      "Epoch 00454: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6412 - accuracy: 0.6449 - val_loss: 2.8466 - val_accuracy: 0.6705\n",
      "Epoch 455/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6026 - accuracy: 0.5938\n",
      "Epoch 00455: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6302 - accuracy: 0.6335 - val_loss: 2.8961 - val_accuracy: 0.6705\n",
      "Epoch 456/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7337 - accuracy: 0.5938\n",
      "Epoch 00456: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6211 - accuracy: 0.6818 - val_loss: 3.1287 - val_accuracy: 0.6705\n",
      "Epoch 457/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6856 - accuracy: 0.6250\n",
      "Epoch 00457: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6690 - accuracy: 0.6136 - val_loss: 2.8011 - val_accuracy: 0.6818\n",
      "Epoch 458/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6346 - accuracy: 0.6250\n",
      "Epoch 00458: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6675 - accuracy: 0.6506 - val_loss: 2.7000 - val_accuracy: 0.6705\n",
      "Epoch 459/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6522 - accuracy: 0.7188\n",
      "Epoch 00459: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6679 - accuracy: 0.6506 - val_loss: 2.6553 - val_accuracy: 0.6705\n",
      "Epoch 460/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4798 - accuracy: 0.8125\n",
      "Epoch 00460: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6768 - accuracy: 0.6278 - val_loss: 2.5343 - val_accuracy: 0.6932\n",
      "Epoch 461/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6191 - accuracy: 0.7500\n",
      "Epoch 00461: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6511 - accuracy: 0.6733 - val_loss: 2.3080 - val_accuracy: 0.6705\n",
      "Epoch 462/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7500 - accuracy: 0.5625\n",
      "Epoch 00462: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6590 - accuracy: 0.6619 - val_loss: 2.8835 - val_accuracy: 0.6932\n",
      "Epoch 463/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6844 - accuracy: 0.6875\n",
      "Epoch 00463: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6689 - accuracy: 0.6676 - val_loss: 1.8665 - val_accuracy: 0.6705\n",
      "Epoch 464/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6961 - accuracy: 0.5625\n",
      "Epoch 00464: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6828 - accuracy: 0.6477 - val_loss: 1.8741 - val_accuracy: 0.6591\n",
      "Epoch 465/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6881 - accuracy: 0.6562\n",
      "Epoch 00465: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6910 - accuracy: 0.6477 - val_loss: 1.8963 - val_accuracy: 0.6591\n",
      "Epoch 466/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7252 - accuracy: 0.7188\n",
      "Epoch 00466: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6964 - accuracy: 0.6420 - val_loss: 1.3170 - val_accuracy: 0.6591\n",
      "Epoch 467/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7115 - accuracy: 0.5938\n",
      "Epoch 00467: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7370 - accuracy: 0.6080 - val_loss: 1.4936 - val_accuracy: 0.6591\n",
      "Epoch 468/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6918 - accuracy: 0.6875\n",
      "Epoch 00468: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7075 - accuracy: 0.6193 - val_loss: 1.6956 - val_accuracy: 0.6705\n",
      "Epoch 469/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.8749 - accuracy: 0.5000\n",
      "Epoch 00469: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7100 - accuracy: 0.6250 - val_loss: 1.8403 - val_accuracy: 0.6705\n",
      "Epoch 470/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7801 - accuracy: 0.5000\n",
      "Epoch 00470: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7001 - accuracy: 0.6364 - val_loss: 1.5849 - val_accuracy: 0.6705\n",
      "Epoch 471/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6584 - accuracy: 0.7188\n",
      "Epoch 00471: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7236 - accuracy: 0.6278 - val_loss: 1.7880 - val_accuracy: 0.6818\n",
      "Epoch 472/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6784 - accuracy: 0.6562\n",
      "Epoch 00472: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6658 - accuracy: 0.6562 - val_loss: 2.0418 - val_accuracy: 0.6818\n",
      "Epoch 473/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5156 - accuracy: 0.7812\n",
      "Epoch 00473: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6143 - accuracy: 0.6705 - val_loss: 2.2635 - val_accuracy: 0.6932\n",
      "Epoch 474/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5994 - accuracy: 0.6250\n",
      "Epoch 00474: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6362 - accuracy: 0.6477 - val_loss: 2.6757 - val_accuracy: 0.6818\n",
      "Epoch 475/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6260 - accuracy: 0.7500\n",
      "Epoch 00475: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6616 - accuracy: 0.6506 - val_loss: 2.7505 - val_accuracy: 0.6818\n",
      "Epoch 476/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6354 - accuracy: 0.6875\n",
      "Epoch 00476: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6442 - accuracy: 0.6449 - val_loss: 2.6760 - val_accuracy: 0.6818\n",
      "Epoch 477/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5504 - accuracy: 0.7812\n",
      "Epoch 00477: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6489 - accuracy: 0.6534 - val_loss: 2.5267 - val_accuracy: 0.6932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 478/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6019 - accuracy: 0.6875\n",
      "Epoch 00478: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.6289 - accuracy: 0.6761 - val_loss: 3.8313 - val_accuracy: 0.6818\n",
      "Epoch 479/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.3445 - accuracy: 0.7812\n",
      "Epoch 00479: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5892 - accuracy: 0.6676 - val_loss: 1.9230 - val_accuracy: 0.7159\n",
      "Epoch 480/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7454 - accuracy: 0.4375\n",
      "Epoch 00480: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6408 - accuracy: 0.6165 - val_loss: 2.0322 - val_accuracy: 0.6591\n",
      "Epoch 481/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5875 - accuracy: 0.6875\n",
      "Epoch 00481: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6626 - accuracy: 0.5966 - val_loss: 1.9926 - val_accuracy: 0.6818\n",
      "Epoch 482/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5434 - accuracy: 0.7812\n",
      "Epoch 00482: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6575 - accuracy: 0.6222 - val_loss: 1.7996 - val_accuracy: 0.7159\n",
      "Epoch 483/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7015 - accuracy: 0.5625\n",
      "Epoch 00483: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6318 - accuracy: 0.6506 - val_loss: 2.3296 - val_accuracy: 0.7273\n",
      "Epoch 484/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6087 - accuracy: 0.6562\n",
      "Epoch 00484: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6713 - accuracy: 0.6136 - val_loss: 1.7339 - val_accuracy: 0.7159\n",
      "Epoch 485/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5537 - accuracy: 0.6875\n",
      "Epoch 00485: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6812 - accuracy: 0.6335 - val_loss: 1.7479 - val_accuracy: 0.7159\n",
      "Epoch 486/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7186 - accuracy: 0.5938\n",
      "Epoch 00486: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6753 - accuracy: 0.6193 - val_loss: 1.7182 - val_accuracy: 0.7159\n",
      "Epoch 487/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.8054 - accuracy: 0.5312\n",
      "Epoch 00487: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6661 - accuracy: 0.6364 - val_loss: 1.6799 - val_accuracy: 0.6932\n",
      "Epoch 488/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7284 - accuracy: 0.6562\n",
      "Epoch 00488: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6559 - accuracy: 0.6477 - val_loss: 1.9917 - val_accuracy: 0.6818\n",
      "Epoch 489/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7421 - accuracy: 0.6250\n",
      "Epoch 00489: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6780 - accuracy: 0.6420 - val_loss: 2.1527 - val_accuracy: 0.6818\n",
      "Epoch 490/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6743 - accuracy: 0.6250\n",
      "Epoch 00490: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6559 - accuracy: 0.6023 - val_loss: 2.8333 - val_accuracy: 0.7045\n",
      "Epoch 491/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6300 - accuracy: 0.6250\n",
      "Epoch 00491: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6489 - accuracy: 0.6477 - val_loss: 2.6805 - val_accuracy: 0.6932\n",
      "Epoch 492/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7235 - accuracy: 0.6562\n",
      "Epoch 00492: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6346 - accuracy: 0.6562 - val_loss: 2.1809 - val_accuracy: 0.6818\n",
      "Epoch 493/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6796 - accuracy: 0.5000\n",
      "Epoch 00493: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6422 - accuracy: 0.6420 - val_loss: 1.9257 - val_accuracy: 0.7045\n",
      "Epoch 494/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7580 - accuracy: 0.5625\n",
      "Epoch 00494: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6642 - accuracy: 0.6193 - val_loss: 2.1154 - val_accuracy: 0.6932\n",
      "Epoch 495/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5981 - accuracy: 0.5938\n",
      "Epoch 00495: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6762 - accuracy: 0.6364 - val_loss: 2.8208 - val_accuracy: 0.6591\n",
      "Epoch 496/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6441 - accuracy: 0.6875\n",
      "Epoch 00496: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6176 - accuracy: 0.6676 - val_loss: 2.9216 - val_accuracy: 0.7045\n",
      "Epoch 497/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6447 - accuracy: 0.5938\n",
      "Epoch 00497: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5945 - accuracy: 0.6705 - val_loss: 2.8512 - val_accuracy: 0.6818\n",
      "Epoch 498/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6922 - accuracy: 0.5625\n",
      "Epoch 00498: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6126 - accuracy: 0.6676 - val_loss: 2.7761 - val_accuracy: 0.6932\n",
      "Epoch 499/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5737 - accuracy: 0.6250\n",
      "Epoch 00499: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6069 - accuracy: 0.6562 - val_loss: 2.6897 - val_accuracy: 0.7045\n",
      "Epoch 500/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6505 - accuracy: 0.7188\n",
      "Epoch 00500: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6248 - accuracy: 0.6648 - val_loss: 3.1122 - val_accuracy: 0.6818\n",
      "Epoch 501/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5006 - accuracy: 0.6250\n",
      "Epoch 00501: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6244 - accuracy: 0.6818 - val_loss: 2.3195 - val_accuracy: 0.7045\n",
      "Epoch 502/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.8980 - accuracy: 0.5625\n",
      "Epoch 00502: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7605 - accuracy: 0.6307 - val_loss: 2.3173 - val_accuracy: 0.7159\n",
      "Epoch 503/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7175 - accuracy: 0.5312\n",
      "Epoch 00503: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6448 - accuracy: 0.6449 - val_loss: 1.0665 - val_accuracy: 0.7045\n",
      "Epoch 504/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4916 - accuracy: 0.6250\n",
      "Epoch 00504: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6751 - accuracy: 0.6080 - val_loss: 1.4413 - val_accuracy: 0.6932\n",
      "Epoch 505/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6142 - accuracy: 0.7500\n",
      "Epoch 00505: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6650 - accuracy: 0.6222 - val_loss: 1.5727 - val_accuracy: 0.6818\n",
      "Epoch 506/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7238 - accuracy: 0.5625\n",
      "Epoch 00506: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7134 - accuracy: 0.5795 - val_loss: 1.9686 - val_accuracy: 0.6932\n",
      "Epoch 507/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7790 - accuracy: 0.5938\n",
      "Epoch 00507: val_loss did not improve from 0.98357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 4ms/step - loss: 0.7402 - accuracy: 0.5824 - val_loss: 1.5943 - val_accuracy: 0.6705\n",
      "Epoch 508/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6155 - accuracy: 0.6562\n",
      "Epoch 00508: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7070 - accuracy: 0.5852 - val_loss: 1.3275 - val_accuracy: 0.6591\n",
      "Epoch 509/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.9015 - accuracy: 0.5625\n",
      "Epoch 00509: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7263 - accuracy: 0.5938 - val_loss: 1.5291 - val_accuracy: 0.6705\n",
      "Epoch 510/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7625 - accuracy: 0.5625\n",
      "Epoch 00510: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7192 - accuracy: 0.5852 - val_loss: 1.9729 - val_accuracy: 0.6818\n",
      "Epoch 511/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6382 - accuracy: 0.6875\n",
      "Epoch 00511: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7065 - accuracy: 0.6307 - val_loss: 1.7840 - val_accuracy: 0.6705\n",
      "Epoch 512/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7300 - accuracy: 0.6875\n",
      "Epoch 00512: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6892 - accuracy: 0.6222 - val_loss: 1.8133 - val_accuracy: 0.6705\n",
      "Epoch 513/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7593 - accuracy: 0.7500\n",
      "Epoch 00513: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6888 - accuracy: 0.6449 - val_loss: 1.5110 - val_accuracy: 0.6705\n",
      "Epoch 514/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5975 - accuracy: 0.5312\n",
      "Epoch 00514: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7043 - accuracy: 0.6051 - val_loss: 1.5414 - val_accuracy: 0.6932\n",
      "Epoch 515/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7603 - accuracy: 0.6250\n",
      "Epoch 00515: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7370 - accuracy: 0.6193 - val_loss: 1.4416 - val_accuracy: 0.6818\n",
      "Epoch 516/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7452 - accuracy: 0.5938\n",
      "Epoch 00516: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6966 - accuracy: 0.6534 - val_loss: 1.9630 - val_accuracy: 0.6818\n",
      "Epoch 517/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7110 - accuracy: 0.5938\n",
      "Epoch 00517: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7160 - accuracy: 0.6335 - val_loss: 1.3747 - val_accuracy: 0.7045\n",
      "Epoch 518/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.8561 - accuracy: 0.6562\n",
      "Epoch 00518: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7213 - accuracy: 0.6648 - val_loss: 1.6815 - val_accuracy: 0.7045\n",
      "Epoch 519/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6508 - accuracy: 0.7188\n",
      "Epoch 00519: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.7002 - accuracy: 0.6676 - val_loss: 2.0009 - val_accuracy: 0.6818\n",
      "Epoch 520/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7935 - accuracy: 0.4688\n",
      "Epoch 00520: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7212 - accuracy: 0.6136 - val_loss: 1.8306 - val_accuracy: 0.6932\n",
      "Epoch 521/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5658 - accuracy: 0.5625\n",
      "Epoch 00521: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7006 - accuracy: 0.6250 - val_loss: 1.8551 - val_accuracy: 0.7045\n",
      "Epoch 522/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6501 - accuracy: 0.7188\n",
      "Epoch 00522: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7024 - accuracy: 0.6506 - val_loss: 2.0178 - val_accuracy: 0.6818\n",
      "Epoch 523/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6087 - accuracy: 0.6250\n",
      "Epoch 00523: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6982 - accuracy: 0.6477 - val_loss: 2.0394 - val_accuracy: 0.6932\n",
      "Epoch 524/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6894 - accuracy: 0.6875\n",
      "Epoch 00524: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6773 - accuracy: 0.6420 - val_loss: 2.1126 - val_accuracy: 0.7045\n",
      "Epoch 525/1000\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.7296 - accuracy: 0.6313\n",
      "Epoch 00525: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.7193 - accuracy: 0.6392 - val_loss: 2.1770 - val_accuracy: 0.7045\n",
      "Epoch 526/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6349 - accuracy: 0.5312\n",
      "Epoch 00526: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7265 - accuracy: 0.6193 - val_loss: 2.4144 - val_accuracy: 0.6932\n",
      "Epoch 527/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6133 - accuracy: 0.6875\n",
      "Epoch 00527: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7061 - accuracy: 0.6335 - val_loss: 2.6500 - val_accuracy: 0.6818\n",
      "Epoch 528/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7745 - accuracy: 0.5625\n",
      "Epoch 00528: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6924 - accuracy: 0.6307 - val_loss: 2.5689 - val_accuracy: 0.6932\n",
      "Epoch 529/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.8030 - accuracy: 0.6562\n",
      "Epoch 00529: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7146 - accuracy: 0.6364 - val_loss: 2.6973 - val_accuracy: 0.6932\n",
      "Epoch 530/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7226 - accuracy: 0.5625\n",
      "Epoch 00530: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.7129 - accuracy: 0.6136 - val_loss: 2.6719 - val_accuracy: 0.6932\n",
      "Epoch 531/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7475 - accuracy: 0.6875\n",
      "Epoch 00531: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6850 - accuracy: 0.6449 - val_loss: 2.7611 - val_accuracy: 0.6705\n",
      "Epoch 532/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7208 - accuracy: 0.5938\n",
      "Epoch 00532: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6799 - accuracy: 0.6449 - val_loss: 2.4152 - val_accuracy: 0.6818\n",
      "Epoch 533/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7752 - accuracy: 0.5312\n",
      "Epoch 00533: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6372 - accuracy: 0.6648 - val_loss: 2.4834 - val_accuracy: 0.6591\n",
      "Epoch 534/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.8371 - accuracy: 0.4688\n",
      "Epoch 00534: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7009 - accuracy: 0.6250 - val_loss: 2.5157 - val_accuracy: 0.6818\n",
      "Epoch 535/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6471 - accuracy: 0.5938\n",
      "Epoch 00535: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6901 - accuracy: 0.6307 - val_loss: 2.5854 - val_accuracy: 0.6705\n",
      "Epoch 536/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6313 - accuracy: 0.6875\n",
      "Epoch 00536: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6641 - accuracy: 0.6619 - val_loss: 2.4935 - val_accuracy: 0.6818\n",
      "Epoch 537/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7305 - accuracy: 0.6250\n",
      "Epoch 00537: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6661 - accuracy: 0.6477 - val_loss: 2.4888 - val_accuracy: 0.6591\n",
      "Epoch 538/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6672 - accuracy: 0.6562\n",
      "Epoch 00538: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6958 - accuracy: 0.6136 - val_loss: 2.3064 - val_accuracy: 0.6705\n",
      "Epoch 539/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5257 - accuracy: 0.6875\n",
      "Epoch 00539: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6712 - accuracy: 0.6449 - val_loss: 2.1776 - val_accuracy: 0.6705\n",
      "Epoch 540/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6163 - accuracy: 0.6562\n",
      "Epoch 00540: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6971 - accuracy: 0.6477 - val_loss: 1.8653 - val_accuracy: 0.6705\n",
      "Epoch 541/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6604 - accuracy: 0.6875\n",
      "Epoch 00541: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.7390 - accuracy: 0.6108 - val_loss: 1.4489 - val_accuracy: 0.6818\n",
      "Epoch 542/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6833 - accuracy: 0.6250\n",
      "Epoch 00542: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6998 - accuracy: 0.6193 - val_loss: 1.3199 - val_accuracy: 0.6818\n",
      "Epoch 543/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6966 - accuracy: 0.6875\n",
      "Epoch 00543: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.7462 - accuracy: 0.6648 - val_loss: 1.2020 - val_accuracy: 0.6818\n",
      "Epoch 544/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7883 - accuracy: 0.5312\n",
      "Epoch 00544: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7151 - accuracy: 0.6307 - val_loss: 1.4813 - val_accuracy: 0.6705\n",
      "Epoch 545/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7360 - accuracy: 0.5625\n",
      "Epoch 00545: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6990 - accuracy: 0.6307 - val_loss: 1.2776 - val_accuracy: 0.6932\n",
      "Epoch 546/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6351 - accuracy: 0.6875\n",
      "Epoch 00546: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6697 - accuracy: 0.6619 - val_loss: 1.3463 - val_accuracy: 0.6818\n",
      "Epoch 547/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.8033 - accuracy: 0.5625\n",
      "Epoch 00547: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7202 - accuracy: 0.6250 - val_loss: 1.5119 - val_accuracy: 0.6818\n",
      "Epoch 548/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6989 - accuracy: 0.5938\n",
      "Epoch 00548: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6886 - accuracy: 0.6278 - val_loss: 1.6301 - val_accuracy: 0.6818\n",
      "Epoch 549/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5964 - accuracy: 0.7188\n",
      "Epoch 00549: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6983 - accuracy: 0.6420 - val_loss: 1.4219 - val_accuracy: 0.6932\n",
      "Epoch 550/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6149 - accuracy: 0.7812\n",
      "Epoch 00550: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6834 - accuracy: 0.6648 - val_loss: 1.4485 - val_accuracy: 0.6591\n",
      "Epoch 551/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7203 - accuracy: 0.6562\n",
      "Epoch 00551: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7432 - accuracy: 0.5966 - val_loss: 1.7631 - val_accuracy: 0.6136\n",
      "Epoch 552/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7193 - accuracy: 0.5938\n",
      "Epoch 00552: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7679 - accuracy: 0.6051 - val_loss: 1.4270 - val_accuracy: 0.6477\n",
      "Epoch 553/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7338 - accuracy: 0.5312\n",
      "Epoch 00553: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7416 - accuracy: 0.6165 - val_loss: 1.3604 - val_accuracy: 0.6932\n",
      "Epoch 554/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7768 - accuracy: 0.6250\n",
      "Epoch 00554: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7500 - accuracy: 0.6222 - val_loss: 1.4072 - val_accuracy: 0.6818\n",
      "Epoch 555/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6127 - accuracy: 0.7188\n",
      "Epoch 00555: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7145 - accuracy: 0.6307 - val_loss: 1.5458 - val_accuracy: 0.6705\n",
      "Epoch 556/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6959 - accuracy: 0.6562\n",
      "Epoch 00556: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6911 - accuracy: 0.6449 - val_loss: 1.6016 - val_accuracy: 0.6932\n",
      "Epoch 557/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7060 - accuracy: 0.6250\n",
      "Epoch 00557: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7228 - accuracy: 0.6222 - val_loss: 1.4632 - val_accuracy: 0.6932\n",
      "Epoch 558/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.8034 - accuracy: 0.5625\n",
      "Epoch 00558: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7381 - accuracy: 0.6222 - val_loss: 1.5556 - val_accuracy: 0.6818\n",
      "Epoch 559/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.8021 - accuracy: 0.5000\n",
      "Epoch 00559: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.7344 - accuracy: 0.6250 - val_loss: 1.6617 - val_accuracy: 0.6818\n",
      "Epoch 560/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6926 - accuracy: 0.6250\n",
      "Epoch 00560: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.7574 - accuracy: 0.6136 - val_loss: 1.4517 - val_accuracy: 0.6818\n",
      "Epoch 561/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.8773 - accuracy: 0.5000\n",
      "Epoch 00561: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.8017 - accuracy: 0.5653 - val_loss: 1.4367 - val_accuracy: 0.6364\n",
      "Epoch 562/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7712 - accuracy: 0.4375\n",
      "Epoch 00562: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.8098 - accuracy: 0.5341 - val_loss: 1.4440 - val_accuracy: 0.5909\n",
      "Epoch 563/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.8739 - accuracy: 0.5000\n",
      "Epoch 00563: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.8023 - accuracy: 0.5653 - val_loss: 1.3597 - val_accuracy: 0.6591\n",
      "Epoch 564/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7889 - accuracy: 0.5938\n",
      "Epoch 00564: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.7746 - accuracy: 0.5795 - val_loss: 1.4252 - val_accuracy: 0.6591\n",
      "Epoch 565/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.8211 - accuracy: 0.4375\n",
      "Epoch 00565: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.7404 - accuracy: 0.5909 - val_loss: 1.5897 - val_accuracy: 0.6705\n",
      "Epoch 566/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7623 - accuracy: 0.5625\n",
      "Epoch 00566: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.7686 - accuracy: 0.6222 - val_loss: 1.5408 - val_accuracy: 0.6705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 567/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6178 - accuracy: 0.6875\n",
      "Epoch 00567: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.7365 - accuracy: 0.6193 - val_loss: 1.3680 - val_accuracy: 0.6818\n",
      "Epoch 568/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6279 - accuracy: 0.6250\n",
      "Epoch 00568: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.7332 - accuracy: 0.6449 - val_loss: 1.4719 - val_accuracy: 0.6705\n",
      "Epoch 569/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7323 - accuracy: 0.5938\n",
      "Epoch 00569: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6978 - accuracy: 0.6420 - val_loss: 1.5704 - val_accuracy: 0.6818\n",
      "Epoch 570/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5263 - accuracy: 0.7500\n",
      "Epoch 00570: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.7135 - accuracy: 0.6193 - val_loss: 1.7128 - val_accuracy: 0.6932\n",
      "Epoch 571/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7751 - accuracy: 0.5625\n",
      "Epoch 00571: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.7085 - accuracy: 0.6335 - val_loss: 2.3209 - val_accuracy: 0.6818\n",
      "Epoch 572/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5005 - accuracy: 0.7188\n",
      "Epoch 00572: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.7086 - accuracy: 0.6165 - val_loss: 1.5073 - val_accuracy: 0.6818\n",
      "Epoch 573/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.8300 - accuracy: 0.5000\n",
      "Epoch 00573: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.7077 - accuracy: 0.6307 - val_loss: 1.5099 - val_accuracy: 0.6705\n",
      "Epoch 574/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6765 - accuracy: 0.5000\n",
      "Epoch 00574: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.7023 - accuracy: 0.6278 - val_loss: 1.2048 - val_accuracy: 0.6818\n",
      "Epoch 575/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.8378 - accuracy: 0.5938\n",
      "Epoch 00575: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.7200 - accuracy: 0.6165 - val_loss: 1.4025 - val_accuracy: 0.6705\n",
      "Epoch 576/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5583 - accuracy: 0.7500\n",
      "Epoch 00576: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6725 - accuracy: 0.6193 - val_loss: 1.6904 - val_accuracy: 0.6932\n",
      "Epoch 577/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6677 - accuracy: 0.5938\n",
      "Epoch 00577: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6591 - accuracy: 0.6392 - val_loss: 1.6342 - val_accuracy: 0.7045\n",
      "Epoch 578/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6434 - accuracy: 0.6562\n",
      "Epoch 00578: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6812 - accuracy: 0.6364 - val_loss: 1.3238 - val_accuracy: 0.6932\n",
      "Epoch 579/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.8431 - accuracy: 0.5312\n",
      "Epoch 00579: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7166 - accuracy: 0.6222 - val_loss: 1.3900 - val_accuracy: 0.6705\n",
      "Epoch 580/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7965 - accuracy: 0.5000\n",
      "Epoch 00580: val_loss did not improve from 0.98357\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.7497 - accuracy: 0.6023 - val_loss: 1.0928 - val_accuracy: 0.6705\n",
      "Epoch 581/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.8360 - accuracy: 0.5312\n",
      "Epoch 00581: val_loss improved from 0.98357 to 0.84669, saving model to save_models\\audio_classification.hdf5\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7100 - accuracy: 0.6193 - val_loss: 0.8467 - val_accuracy: 0.6705\n",
      "Epoch 582/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7076 - accuracy: 0.6875\n",
      "Epoch 00582: val_loss did not improve from 0.84669\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.7562 - accuracy: 0.6193 - val_loss: 0.9331 - val_accuracy: 0.6591\n",
      "Epoch 583/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6830 - accuracy: 0.7188\n",
      "Epoch 00583: val_loss did not improve from 0.84669\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.7324 - accuracy: 0.6449 - val_loss: 0.9021 - val_accuracy: 0.6136\n",
      "Epoch 584/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7222 - accuracy: 0.5625\n",
      "Epoch 00584: val_loss improved from 0.84669 to 0.76896, saving model to save_models\\audio_classification.hdf5\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.8177 - accuracy: 0.5682 - val_loss: 0.7690 - val_accuracy: 0.6250\n",
      "Epoch 585/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7794 - accuracy: 0.6250\n",
      "Epoch 00585: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.7236 - accuracy: 0.6080 - val_loss: 0.8629 - val_accuracy: 0.6705\n",
      "Epoch 586/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6519 - accuracy: 0.6875\n",
      "Epoch 00586: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6796 - accuracy: 0.6676 - val_loss: 1.0121 - val_accuracy: 0.6932\n",
      "Epoch 587/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7365 - accuracy: 0.5938\n",
      "Epoch 00587: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6757 - accuracy: 0.6562 - val_loss: 1.8484 - val_accuracy: 0.6705\n",
      "Epoch 588/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5870 - accuracy: 0.7188\n",
      "Epoch 00588: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.7470 - accuracy: 0.6165 - val_loss: 0.9866 - val_accuracy: 0.6818\n",
      "Epoch 589/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.8547 - accuracy: 0.6562\n",
      "Epoch 00589: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7446 - accuracy: 0.6278 - val_loss: 1.0143 - val_accuracy: 0.6818\n",
      "Epoch 590/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6100 - accuracy: 0.7188\n",
      "Epoch 00590: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7059 - accuracy: 0.6420 - val_loss: 1.5690 - val_accuracy: 0.6818\n",
      "Epoch 591/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5578 - accuracy: 0.6875\n",
      "Epoch 00591: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7112 - accuracy: 0.6392 - val_loss: 0.9420 - val_accuracy: 0.6818\n",
      "Epoch 592/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6762 - accuracy: 0.6562\n",
      "Epoch 00592: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.7367 - accuracy: 0.6307 - val_loss: 1.0445 - val_accuracy: 0.6705\n",
      "Epoch 593/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5230 - accuracy: 0.7188\n",
      "Epoch 00593: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6877 - accuracy: 0.6449 - val_loss: 1.1147 - val_accuracy: 0.6705\n",
      "Epoch 594/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5880 - accuracy: 0.6562\n",
      "Epoch 00594: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6557 - accuracy: 0.6619 - val_loss: 1.3984 - val_accuracy: 0.6705\n",
      "Epoch 595/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7796 - accuracy: 0.5938\n",
      "Epoch 00595: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7388 - accuracy: 0.5994 - val_loss: 1.2717 - val_accuracy: 0.6705\n",
      "Epoch 596/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5888 - accuracy: 0.6875\n",
      "Epoch 00596: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7107 - accuracy: 0.6080 - val_loss: 1.3015 - val_accuracy: 0.6818\n",
      "Epoch 597/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6610 - accuracy: 0.5938\n",
      "Epoch 00597: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6725 - accuracy: 0.6477 - val_loss: 1.4801 - val_accuracy: 0.6932\n",
      "Epoch 598/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6874 - accuracy: 0.6250\n",
      "Epoch 00598: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6697 - accuracy: 0.6449 - val_loss: 1.4173 - val_accuracy: 0.6818\n",
      "Epoch 599/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6922 - accuracy: 0.7812\n",
      "Epoch 00599: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6644 - accuracy: 0.6364 - val_loss: 1.4707 - val_accuracy: 0.6932\n",
      "Epoch 600/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5724 - accuracy: 0.7812\n",
      "Epoch 00600: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6663 - accuracy: 0.6364 - val_loss: 1.4806 - val_accuracy: 0.6932\n",
      "Epoch 601/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6599 - accuracy: 0.6875\n",
      "Epoch 00601: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6927 - accuracy: 0.6335 - val_loss: 1.5738 - val_accuracy: 0.6705\n",
      "Epoch 602/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4916 - accuracy: 0.8125\n",
      "Epoch 00602: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7082 - accuracy: 0.6165 - val_loss: 1.7969 - val_accuracy: 0.6932\n",
      "Epoch 603/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7073 - accuracy: 0.6875\n",
      "Epoch 00603: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6731 - accuracy: 0.6250 - val_loss: 2.3007 - val_accuracy: 0.6818\n",
      "Epoch 604/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6522 - accuracy: 0.6250\n",
      "Epoch 00604: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.7269 - accuracy: 0.6364 - val_loss: 1.2573 - val_accuracy: 0.6705\n",
      "Epoch 605/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6776 - accuracy: 0.6562\n",
      "Epoch 00605: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7381 - accuracy: 0.6250 - val_loss: 1.2930 - val_accuracy: 0.6932\n",
      "Epoch 606/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7023 - accuracy: 0.6875\n",
      "Epoch 00606: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6807 - accuracy: 0.6506 - val_loss: 1.1057 - val_accuracy: 0.6932\n",
      "Epoch 607/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5499 - accuracy: 0.7500\n",
      "Epoch 00607: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.7432 - accuracy: 0.6222 - val_loss: 1.3323 - val_accuracy: 0.6932\n",
      "Epoch 608/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6981 - accuracy: 0.6250\n",
      "Epoch 00608: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6830 - accuracy: 0.6392 - val_loss: 1.6481 - val_accuracy: 0.7045\n",
      "Epoch 609/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6208 - accuracy: 0.6562\n",
      "Epoch 00609: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7065 - accuracy: 0.6250 - val_loss: 1.9071 - val_accuracy: 0.6932\n",
      "Epoch 610/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7128 - accuracy: 0.6250\n",
      "Epoch 00610: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6964 - accuracy: 0.6193 - val_loss: 1.8637 - val_accuracy: 0.6818\n",
      "Epoch 611/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7631 - accuracy: 0.5625\n",
      "Epoch 00611: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6577 - accuracy: 0.6278 - val_loss: 1.5711 - val_accuracy: 0.6932\n",
      "Epoch 612/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7742 - accuracy: 0.5312\n",
      "Epoch 00612: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6731 - accuracy: 0.6392 - val_loss: 1.8804 - val_accuracy: 0.6591\n",
      "Epoch 613/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5752 - accuracy: 0.6250\n",
      "Epoch 00613: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.7263 - accuracy: 0.5909 - val_loss: 1.7462 - val_accuracy: 0.6364\n",
      "Epoch 614/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6638 - accuracy: 0.6250\n",
      "Epoch 00614: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6944 - accuracy: 0.6051 - val_loss: 1.4856 - val_accuracy: 0.6932\n",
      "Epoch 615/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6100 - accuracy: 0.6562\n",
      "Epoch 00615: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6805 - accuracy: 0.6420 - val_loss: 1.5118 - val_accuracy: 0.6932\n",
      "Epoch 616/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5510 - accuracy: 0.6875\n",
      "Epoch 00616: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7158 - accuracy: 0.6222 - val_loss: 1.6176 - val_accuracy: 0.6932\n",
      "Epoch 617/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5408 - accuracy: 0.7188\n",
      "Epoch 00617: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6612 - accuracy: 0.6222 - val_loss: 1.6768 - val_accuracy: 0.6932\n",
      "Epoch 618/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6480 - accuracy: 0.6250\n",
      "Epoch 00618: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6985 - accuracy: 0.6165 - val_loss: 1.7980 - val_accuracy: 0.6705\n",
      "Epoch 619/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.8419 - accuracy: 0.5000\n",
      "Epoch 00619: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6947 - accuracy: 0.6193 - val_loss: 1.7479 - val_accuracy: 0.6818\n",
      "Epoch 620/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7273 - accuracy: 0.7188\n",
      "Epoch 00620: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6924 - accuracy: 0.6307 - val_loss: 1.7215 - val_accuracy: 0.6932\n",
      "Epoch 621/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.8110 - accuracy: 0.5625\n",
      "Epoch 00621: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7091 - accuracy: 0.6278 - val_loss: 1.8161 - val_accuracy: 0.6932\n",
      "Epoch 622/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6072 - accuracy: 0.7188\n",
      "Epoch 00622: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6650 - accuracy: 0.6364 - val_loss: 2.0916 - val_accuracy: 0.7045\n",
      "Epoch 623/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6784 - accuracy: 0.5312\n",
      "Epoch 00623: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6774 - accuracy: 0.6278 - val_loss: 2.1044 - val_accuracy: 0.6818\n",
      "Epoch 624/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6029 - accuracy: 0.7188\n",
      "Epoch 00624: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6639 - accuracy: 0.6676 - val_loss: 2.0308 - val_accuracy: 0.6932\n",
      "Epoch 625/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7666 - accuracy: 0.5312\n",
      "Epoch 00625: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6875 - accuracy: 0.6364 - val_loss: 2.3060 - val_accuracy: 0.6932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 626/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5405 - accuracy: 0.7188\n",
      "Epoch 00626: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6478 - accuracy: 0.6562 - val_loss: 2.1910 - val_accuracy: 0.6818\n",
      "Epoch 627/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5854 - accuracy: 0.6875\n",
      "Epoch 00627: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.7038 - accuracy: 0.5966 - val_loss: 2.1397 - val_accuracy: 0.6932\n",
      "Epoch 628/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6409 - accuracy: 0.6250\n",
      "Epoch 00628: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6861 - accuracy: 0.6364 - val_loss: 2.1386 - val_accuracy: 0.6932\n",
      "Epoch 629/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7632 - accuracy: 0.6250\n",
      "Epoch 00629: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6976 - accuracy: 0.5824 - val_loss: 2.2931 - val_accuracy: 0.6136\n",
      "Epoch 630/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6916 - accuracy: 0.6250\n",
      "Epoch 00630: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.7140 - accuracy: 0.5994 - val_loss: 2.4581 - val_accuracy: 0.6023\n",
      "Epoch 631/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5253 - accuracy: 0.7188\n",
      "Epoch 00631: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6711 - accuracy: 0.5795 - val_loss: 2.5256 - val_accuracy: 0.6136\n",
      "Epoch 632/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7241 - accuracy: 0.5625\n",
      "Epoch 00632: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6911 - accuracy: 0.5852 - val_loss: 1.4466 - val_accuracy: 0.6023\n",
      "Epoch 633/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6625 - accuracy: 0.5938\n",
      "Epoch 00633: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.7265 - accuracy: 0.5824 - val_loss: 1.3506 - val_accuracy: 0.6136\n",
      "Epoch 634/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.8611 - accuracy: 0.5000\n",
      "Epoch 00634: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.7365 - accuracy: 0.5881 - val_loss: 1.6014 - val_accuracy: 0.6250\n",
      "Epoch 635/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.8160 - accuracy: 0.4688\n",
      "Epoch 00635: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.7338 - accuracy: 0.5966 - val_loss: 1.8633 - val_accuracy: 0.6250\n",
      "Epoch 636/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6548 - accuracy: 0.6250\n",
      "Epoch 00636: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7049 - accuracy: 0.5824 - val_loss: 1.7926 - val_accuracy: 0.6250\n",
      "Epoch 637/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7991 - accuracy: 0.5312\n",
      "Epoch 00637: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.7166 - accuracy: 0.5966 - val_loss: 1.7400 - val_accuracy: 0.6136\n",
      "Epoch 638/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7089 - accuracy: 0.5312\n",
      "Epoch 00638: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.7231 - accuracy: 0.5852 - val_loss: 2.6669 - val_accuracy: 0.6136\n",
      "Epoch 639/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6743 - accuracy: 0.6875\n",
      "Epoch 00639: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.7032 - accuracy: 0.6136 - val_loss: 1.7019 - val_accuracy: 0.6250\n",
      "Epoch 640/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7609 - accuracy: 0.4688\n",
      "Epoch 00640: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.7219 - accuracy: 0.5568 - val_loss: 1.6367 - val_accuracy: 0.6250\n",
      "Epoch 641/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7759 - accuracy: 0.5625\n",
      "Epoch 00641: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7189 - accuracy: 0.5824 - val_loss: 1.7954 - val_accuracy: 0.6136\n",
      "Epoch 642/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7824 - accuracy: 0.5625\n",
      "Epoch 00642: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7451 - accuracy: 0.5682 - val_loss: 1.9036 - val_accuracy: 0.6136\n",
      "Epoch 643/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7281 - accuracy: 0.5625\n",
      "Epoch 00643: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.7281 - accuracy: 0.5653 - val_loss: 2.2835 - val_accuracy: 0.6364\n",
      "Epoch 644/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5362 - accuracy: 0.6562\n",
      "Epoch 00644: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7011 - accuracy: 0.6165 - val_loss: 2.0227 - val_accuracy: 0.6591\n",
      "Epoch 645/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7489 - accuracy: 0.6875\n",
      "Epoch 00645: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.7000 - accuracy: 0.6278 - val_loss: 2.0084 - val_accuracy: 0.6591\n",
      "Epoch 646/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6313 - accuracy: 0.6562\n",
      "Epoch 00646: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6912 - accuracy: 0.6278 - val_loss: 2.1919 - val_accuracy: 0.6591\n",
      "Epoch 647/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6100 - accuracy: 0.7500\n",
      "Epoch 00647: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7071 - accuracy: 0.6193 - val_loss: 2.3348 - val_accuracy: 0.6591\n",
      "Epoch 648/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.8614 - accuracy: 0.4688\n",
      "Epoch 00648: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6998 - accuracy: 0.6193 - val_loss: 2.3653 - val_accuracy: 0.6364\n",
      "Epoch 649/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6411 - accuracy: 0.6875\n",
      "Epoch 00649: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6871 - accuracy: 0.6278 - val_loss: 1.9111 - val_accuracy: 0.7045\n",
      "Epoch 650/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5300 - accuracy: 0.6875\n",
      "Epoch 00650: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6775 - accuracy: 0.6222 - val_loss: 1.8474 - val_accuracy: 0.6818\n",
      "Epoch 651/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6297 - accuracy: 0.6562\n",
      "Epoch 00651: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6738 - accuracy: 0.6165 - val_loss: 2.0281 - val_accuracy: 0.6932\n",
      "Epoch 652/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7987 - accuracy: 0.6250\n",
      "Epoch 00652: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6485 - accuracy: 0.6420 - val_loss: 2.2263 - val_accuracy: 0.6818\n",
      "Epoch 653/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5967 - accuracy: 0.6562\n",
      "Epoch 00653: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6359 - accuracy: 0.6534 - val_loss: 2.4276 - val_accuracy: 0.6818\n",
      "Epoch 654/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7688 - accuracy: 0.6562\n",
      "Epoch 00654: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.7113 - accuracy: 0.5966 - val_loss: 2.0355 - val_accuracy: 0.7045\n",
      "Epoch 655/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6785 - accuracy: 0.6250\n",
      "Epoch 00655: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6899 - accuracy: 0.5909 - val_loss: 1.8804 - val_accuracy: 0.7159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 656/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7737 - accuracy: 0.6562\n",
      "Epoch 00656: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6694 - accuracy: 0.6477 - val_loss: 1.0964 - val_accuracy: 0.6818\n",
      "Epoch 657/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6328 - accuracy: 0.6250\n",
      "Epoch 00657: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.7064 - accuracy: 0.5966 - val_loss: 1.0956 - val_accuracy: 0.6932\n",
      "Epoch 658/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6412 - accuracy: 0.6562\n",
      "Epoch 00658: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.7740 - accuracy: 0.5710 - val_loss: 0.9902 - val_accuracy: 0.6591\n",
      "Epoch 659/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7969 - accuracy: 0.5938\n",
      "Epoch 00659: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7635 - accuracy: 0.5909 - val_loss: 1.3035 - val_accuracy: 0.6364\n",
      "Epoch 660/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6614 - accuracy: 0.5312\n",
      "Epoch 00660: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.7451 - accuracy: 0.5994 - val_loss: 1.4504 - val_accuracy: 0.6364\n",
      "Epoch 661/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7338 - accuracy: 0.5312\n",
      "Epoch 00661: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.7742 - accuracy: 0.5852 - val_loss: 1.2513 - val_accuracy: 0.6818\n",
      "Epoch 662/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6715 - accuracy: 0.6562\n",
      "Epoch 00662: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.7200 - accuracy: 0.6136 - val_loss: 1.7705 - val_accuracy: 0.6932\n",
      "Epoch 663/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6893 - accuracy: 0.7188\n",
      "Epoch 00663: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7428 - accuracy: 0.6193 - val_loss: 1.0086 - val_accuracy: 0.6932\n",
      "Epoch 664/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7752 - accuracy: 0.6250\n",
      "Epoch 00664: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7313 - accuracy: 0.6108 - val_loss: 0.9822 - val_accuracy: 0.6932\n",
      "Epoch 665/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6155 - accuracy: 0.7500\n",
      "Epoch 00665: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.7308 - accuracy: 0.6278 - val_loss: 1.0868 - val_accuracy: 0.6932\n",
      "Epoch 666/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7239 - accuracy: 0.6875\n",
      "Epoch 00666: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7092 - accuracy: 0.6420 - val_loss: 1.2607 - val_accuracy: 0.6932\n",
      "Epoch 667/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5439 - accuracy: 0.6562\n",
      "Epoch 00667: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6685 - accuracy: 0.6591 - val_loss: 1.6044 - val_accuracy: 0.6932\n",
      "Epoch 668/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.8576 - accuracy: 0.4688\n",
      "Epoch 00668: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6549 - accuracy: 0.6364 - val_loss: 1.8111 - val_accuracy: 0.6932\n",
      "Epoch 669/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5652 - accuracy: 0.7500\n",
      "Epoch 00669: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6779 - accuracy: 0.6449 - val_loss: 2.2241 - val_accuracy: 0.6932\n",
      "Epoch 670/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6107 - accuracy: 0.6875\n",
      "Epoch 00670: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6165 - accuracy: 0.6477 - val_loss: 1.7090 - val_accuracy: 0.6818\n",
      "Epoch 671/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6024 - accuracy: 0.7812\n",
      "Epoch 00671: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6531 - accuracy: 0.6534 - val_loss: 1.8995 - val_accuracy: 0.6818\n",
      "Epoch 672/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6824 - accuracy: 0.6562\n",
      "Epoch 00672: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6860 - accuracy: 0.6364 - val_loss: 2.1336 - val_accuracy: 0.6932\n",
      "Epoch 673/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5941 - accuracy: 0.6562\n",
      "Epoch 00673: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6486 - accuracy: 0.6250 - val_loss: 2.4014 - val_accuracy: 0.6818\n",
      "Epoch 674/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4577 - accuracy: 0.8125\n",
      "Epoch 00674: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6574 - accuracy: 0.6222 - val_loss: 2.5324 - val_accuracy: 0.6705\n",
      "Epoch 675/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5285 - accuracy: 0.5625\n",
      "Epoch 00675: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6478 - accuracy: 0.6080 - val_loss: 2.5956 - val_accuracy: 0.6932\n",
      "Epoch 676/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5321 - accuracy: 0.6562\n",
      "Epoch 00676: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6620 - accuracy: 0.6420 - val_loss: 2.3874 - val_accuracy: 0.6932\n",
      "Epoch 677/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6604 - accuracy: 0.6875\n",
      "Epoch 00677: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6300 - accuracy: 0.6449 - val_loss: 2.4907 - val_accuracy: 0.6932\n",
      "Epoch 678/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4861 - accuracy: 0.7500\n",
      "Epoch 00678: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6427 - accuracy: 0.6278 - val_loss: 2.8905 - val_accuracy: 0.7045\n",
      "Epoch 679/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7438 - accuracy: 0.5000\n",
      "Epoch 00679: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6620 - accuracy: 0.6193 - val_loss: 2.4306 - val_accuracy: 0.7045\n",
      "Epoch 680/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5497 - accuracy: 0.7188\n",
      "Epoch 00680: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6612 - accuracy: 0.6080 - val_loss: 2.5887 - val_accuracy: 0.7045\n",
      "Epoch 681/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6796 - accuracy: 0.6250\n",
      "Epoch 00681: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6311 - accuracy: 0.6392 - val_loss: 2.7746 - val_accuracy: 0.7045\n",
      "Epoch 682/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.8019 - accuracy: 0.4688\n",
      "Epoch 00682: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6223 - accuracy: 0.6449 - val_loss: 2.9089 - val_accuracy: 0.7045\n",
      "Epoch 683/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5660 - accuracy: 0.7188\n",
      "Epoch 00683: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.5883 - accuracy: 0.6534 - val_loss: 3.0626 - val_accuracy: 0.7045\n",
      "Epoch 684/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5729 - accuracy: 0.6875\n",
      "Epoch 00684: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6032 - accuracy: 0.6335 - val_loss: 2.7660 - val_accuracy: 0.6932\n",
      "Epoch 685/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7709 - accuracy: 0.5000\n",
      "Epoch 00685: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6117 - accuracy: 0.6562 - val_loss: 2.8096 - val_accuracy: 0.7045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 686/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7204 - accuracy: 0.6562\n",
      "Epoch 00686: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6556 - accuracy: 0.5909 - val_loss: 2.8560 - val_accuracy: 0.6818\n",
      "Epoch 687/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4962 - accuracy: 0.8125\n",
      "Epoch 00687: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6419 - accuracy: 0.6222 - val_loss: 2.9527 - val_accuracy: 0.6818\n",
      "Epoch 688/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5079 - accuracy: 0.5938\n",
      "Epoch 00688: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6464 - accuracy: 0.5994 - val_loss: 2.9463 - val_accuracy: 0.6932\n",
      "Epoch 689/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6519 - accuracy: 0.6875\n",
      "Epoch 00689: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6354 - accuracy: 0.6562 - val_loss: 2.6755 - val_accuracy: 0.6932\n",
      "Epoch 690/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7380 - accuracy: 0.6562\n",
      "Epoch 00690: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6091 - accuracy: 0.6562 - val_loss: 2.5625 - val_accuracy: 0.6932\n",
      "Epoch 691/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7418 - accuracy: 0.6250\n",
      "Epoch 00691: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6539 - accuracy: 0.6307 - val_loss: 3.5451 - val_accuracy: 0.6818\n",
      "Epoch 692/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7044 - accuracy: 0.5938\n",
      "Epoch 00692: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6384 - accuracy: 0.6477 - val_loss: 2.4315 - val_accuracy: 0.6932\n",
      "Epoch 693/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5104 - accuracy: 0.7500\n",
      "Epoch 00693: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6469 - accuracy: 0.6392 - val_loss: 2.3436 - val_accuracy: 0.6932\n",
      "Epoch 694/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4706 - accuracy: 0.7812\n",
      "Epoch 00694: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6532 - accuracy: 0.6080 - val_loss: 2.3450 - val_accuracy: 0.6477\n",
      "Epoch 695/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7960 - accuracy: 0.5938\n",
      "Epoch 00695: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6522 - accuracy: 0.6420 - val_loss: 2.8440 - val_accuracy: 0.6818\n",
      "Epoch 696/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7423 - accuracy: 0.5625\n",
      "Epoch 00696: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6731 - accuracy: 0.6108 - val_loss: 2.4224 - val_accuracy: 0.6818\n",
      "Epoch 697/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5414 - accuracy: 0.7188\n",
      "Epoch 00697: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6400 - accuracy: 0.6420 - val_loss: 2.4746 - val_accuracy: 0.6932\n",
      "Epoch 698/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6887 - accuracy: 0.5625\n",
      "Epoch 00698: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6257 - accuracy: 0.6534 - val_loss: 2.5730 - val_accuracy: 0.6818\n",
      "Epoch 699/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6430 - accuracy: 0.8125\n",
      "Epoch 00699: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6371 - accuracy: 0.6591 - val_loss: 2.7006 - val_accuracy: 0.6932\n",
      "Epoch 700/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6714 - accuracy: 0.5625\n",
      "Epoch 00700: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6350 - accuracy: 0.6591 - val_loss: 2.8010 - val_accuracy: 0.6932\n",
      "Epoch 701/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6522 - accuracy: 0.6875\n",
      "Epoch 00701: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6362 - accuracy: 0.6847 - val_loss: 2.7975 - val_accuracy: 0.6818\n",
      "Epoch 702/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7082 - accuracy: 0.5000\n",
      "Epoch 00702: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6459 - accuracy: 0.6534 - val_loss: 2.8882 - val_accuracy: 0.6932\n",
      "Epoch 703/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5013 - accuracy: 0.7188\n",
      "Epoch 00703: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6349 - accuracy: 0.6392 - val_loss: 3.3133 - val_accuracy: 0.6705\n",
      "Epoch 704/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4859 - accuracy: 0.7500\n",
      "Epoch 00704: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6329 - accuracy: 0.6307 - val_loss: 3.1120 - val_accuracy: 0.7045\n",
      "Epoch 705/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5878 - accuracy: 0.5938\n",
      "Epoch 00705: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6412 - accuracy: 0.6591 - val_loss: 2.5715 - val_accuracy: 0.7045\n",
      "Epoch 706/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6192 - accuracy: 0.5938\n",
      "Epoch 00706: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6493 - accuracy: 0.6392 - val_loss: 2.5899 - val_accuracy: 0.7045\n",
      "Epoch 707/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5024 - accuracy: 0.7500\n",
      "Epoch 00707: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6404 - accuracy: 0.6364 - val_loss: 2.3087 - val_accuracy: 0.6932\n",
      "Epoch 708/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6664 - accuracy: 0.6562\n",
      "Epoch 00708: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6502 - accuracy: 0.6165 - val_loss: 2.2035 - val_accuracy: 0.6818\n",
      "Epoch 709/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.8912 - accuracy: 0.4688\n",
      "Epoch 00709: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6766 - accuracy: 0.6335 - val_loss: 2.2867 - val_accuracy: 0.7045\n",
      "Epoch 710/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4185 - accuracy: 0.7188\n",
      "Epoch 00710: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6608 - accuracy: 0.6335 - val_loss: 1.9976 - val_accuracy: 0.6932\n",
      "Epoch 711/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5334 - accuracy: 0.5938\n",
      "Epoch 00711: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6427 - accuracy: 0.6619 - val_loss: 2.0317 - val_accuracy: 0.6705\n",
      "Epoch 712/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7927 - accuracy: 0.4688\n",
      "Epoch 00712: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7190 - accuracy: 0.5909 - val_loss: 1.4825 - val_accuracy: 0.6591\n",
      "Epoch 713/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7437 - accuracy: 0.5938\n",
      "Epoch 00713: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7572 - accuracy: 0.5597 - val_loss: 1.5654 - val_accuracy: 0.6591\n",
      "Epoch 714/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5460 - accuracy: 0.7500\n",
      "Epoch 00714: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.7152 - accuracy: 0.5938 - val_loss: 1.7955 - val_accuracy: 0.6591\n",
      "Epoch 715/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6712 - accuracy: 0.6562\n",
      "Epoch 00715: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6968 - accuracy: 0.5625 - val_loss: 1.7605 - val_accuracy: 0.6250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 716/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.8044 - accuracy: 0.3438\n",
      "Epoch 00716: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.7576 - accuracy: 0.5341 - val_loss: 2.1050 - val_accuracy: 0.6364\n",
      "Epoch 717/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6653 - accuracy: 0.5938\n",
      "Epoch 00717: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6956 - accuracy: 0.6193 - val_loss: 2.1933 - val_accuracy: 0.6591\n",
      "Epoch 718/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6830 - accuracy: 0.6250\n",
      "Epoch 00718: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6789 - accuracy: 0.6307 - val_loss: 2.1383 - val_accuracy: 0.6477\n",
      "Epoch 719/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7351 - accuracy: 0.5000\n",
      "Epoch 00719: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.7187 - accuracy: 0.5653 - val_loss: 1.9226 - val_accuracy: 0.6818\n",
      "Epoch 720/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.8759 - accuracy: 0.8125\n",
      "Epoch 00720: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.7421 - accuracy: 0.5994 - val_loss: 1.6241 - val_accuracy: 0.6591\n",
      "Epoch 721/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7438 - accuracy: 0.5625\n",
      "Epoch 00721: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.7196 - accuracy: 0.5795 - val_loss: 1.7400 - val_accuracy: 0.6591\n",
      "Epoch 722/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6754 - accuracy: 0.5312\n",
      "Epoch 00722: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6881 - accuracy: 0.6080 - val_loss: 1.9702 - val_accuracy: 0.6477\n",
      "Epoch 723/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7320 - accuracy: 0.5000\n",
      "Epoch 00723: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6691 - accuracy: 0.6193 - val_loss: 1.8588 - val_accuracy: 0.6477\n",
      "Epoch 724/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7616 - accuracy: 0.5312\n",
      "Epoch 00724: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6644 - accuracy: 0.6307 - val_loss: 2.0033 - val_accuracy: 0.6591\n",
      "Epoch 725/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6294 - accuracy: 0.5938\n",
      "Epoch 00725: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.5824 - val_loss: 2.0905 - val_accuracy: 0.6705\n",
      "Epoch 726/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5957 - accuracy: 0.6562\n",
      "Epoch 00726: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.7276 - accuracy: 0.5739 - val_loss: 2.0403 - val_accuracy: 0.6591\n",
      "Epoch 727/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.8941 - accuracy: 0.5312\n",
      "Epoch 00727: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.7344 - accuracy: 0.5597 - val_loss: 2.3195 - val_accuracy: 0.6705\n",
      "Epoch 728/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7644 - accuracy: 0.4688\n",
      "Epoch 00728: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6886 - accuracy: 0.5966 - val_loss: 2.1422 - val_accuracy: 0.6705\n",
      "Epoch 729/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7459 - accuracy: 0.6250\n",
      "Epoch 00729: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6814 - accuracy: 0.6392 - val_loss: 2.1348 - val_accuracy: 0.6818\n",
      "Epoch 730/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6518 - accuracy: 0.5625\n",
      "Epoch 00730: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6719 - accuracy: 0.6562 - val_loss: 1.8954 - val_accuracy: 0.6591\n",
      "Epoch 731/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6757 - accuracy: 0.6875\n",
      "Epoch 00731: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6915 - accuracy: 0.6136 - val_loss: 1.9396 - val_accuracy: 0.6818\n",
      "Epoch 732/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.8412 - accuracy: 0.5000\n",
      "Epoch 00732: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6970 - accuracy: 0.6250 - val_loss: 2.0579 - val_accuracy: 0.6705\n",
      "Epoch 733/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7463 - accuracy: 0.5000\n",
      "Epoch 00733: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.7217 - accuracy: 0.6136 - val_loss: 2.0622 - val_accuracy: 0.6818\n",
      "Epoch 734/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6548 - accuracy: 0.6875\n",
      "Epoch 00734: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6945 - accuracy: 0.6080 - val_loss: 2.0716 - val_accuracy: 0.6705\n",
      "Epoch 735/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7364 - accuracy: 0.5938\n",
      "Epoch 00735: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6642 - accuracy: 0.6335 - val_loss: 3.0189 - val_accuracy: 0.6705\n",
      "Epoch 736/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7633 - accuracy: 0.6562\n",
      "Epoch 00736: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6452 - accuracy: 0.6506 - val_loss: 1.9937 - val_accuracy: 0.6818\n",
      "Epoch 737/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7153 - accuracy: 0.7500\n",
      "Epoch 00737: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6866 - accuracy: 0.6307 - val_loss: 1.9548 - val_accuracy: 0.6932\n",
      "Epoch 738/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.9282 - accuracy: 0.5000\n",
      "Epoch 00738: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6963 - accuracy: 0.6222 - val_loss: 2.0332 - val_accuracy: 0.6705\n",
      "Epoch 739/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6365 - accuracy: 0.6875\n",
      "Epoch 00739: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6924 - accuracy: 0.6250 - val_loss: 2.0768 - val_accuracy: 0.6705\n",
      "Epoch 740/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6368 - accuracy: 0.7500\n",
      "Epoch 00740: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6592 - accuracy: 0.6534 - val_loss: 2.1243 - val_accuracy: 0.6818\n",
      "Epoch 741/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7969 - accuracy: 0.6250\n",
      "Epoch 00741: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7160 - accuracy: 0.6051 - val_loss: 2.1849 - val_accuracy: 0.6818\n",
      "Epoch 742/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.8494 - accuracy: 0.5000\n",
      "Epoch 00742: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7124 - accuracy: 0.6108 - val_loss: 2.2314 - val_accuracy: 0.6591\n",
      "Epoch 743/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.8682 - accuracy: 0.3750\n",
      "Epoch 00743: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6826 - accuracy: 0.6335 - val_loss: 2.2508 - val_accuracy: 0.6932\n",
      "Epoch 744/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6780 - accuracy: 0.6562\n",
      "Epoch 00744: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6571 - accuracy: 0.6477 - val_loss: 2.4573 - val_accuracy: 0.6818\n",
      "Epoch 745/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6904 - accuracy: 0.6875\n",
      "Epoch 00745: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6448 - accuracy: 0.6477 - val_loss: 2.6548 - val_accuracy: 0.6932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 746/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7607 - accuracy: 0.6250\n",
      "Epoch 00746: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6996 - accuracy: 0.6136 - val_loss: 2.6099 - val_accuracy: 0.6818\n",
      "Epoch 747/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7390 - accuracy: 0.5312\n",
      "Epoch 00747: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7190 - accuracy: 0.5767 - val_loss: 2.1921 - val_accuracy: 0.6705\n",
      "Epoch 748/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4891 - accuracy: 0.6875\n",
      "Epoch 00748: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6477 - accuracy: 0.6278 - val_loss: 2.1369 - val_accuracy: 0.6932\n",
      "Epoch 749/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6700 - accuracy: 0.6250\n",
      "Epoch 00749: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6681 - accuracy: 0.6222 - val_loss: 2.8696 - val_accuracy: 0.6818\n",
      "Epoch 750/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4953 - accuracy: 0.7500\n",
      "Epoch 00750: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6313 - accuracy: 0.6534 - val_loss: 3.7728 - val_accuracy: 0.6705\n",
      "Epoch 751/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6401 - accuracy: 0.6875\n",
      "Epoch 00751: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6392 - accuracy: 0.6364 - val_loss: 2.6664 - val_accuracy: 0.6818\n",
      "Epoch 752/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6819 - accuracy: 0.6250\n",
      "Epoch 00752: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6248 - accuracy: 0.6307 - val_loss: 2.5581 - val_accuracy: 0.6818\n",
      "Epoch 753/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6617 - accuracy: 0.5312\n",
      "Epoch 00753: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6271 - accuracy: 0.5994 - val_loss: 2.5897 - val_accuracy: 0.6932\n",
      "Epoch 754/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7351 - accuracy: 0.5938\n",
      "Epoch 00754: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6721 - accuracy: 0.6250 - val_loss: 2.7651 - val_accuracy: 0.6705\n",
      "Epoch 755/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7503 - accuracy: 0.5938\n",
      "Epoch 00755: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6505 - accuracy: 0.6392 - val_loss: 2.7201 - val_accuracy: 0.6705\n",
      "Epoch 756/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6404 - accuracy: 0.5312\n",
      "Epoch 00756: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6506 - accuracy: 0.6420 - val_loss: 2.7733 - val_accuracy: 0.6932\n",
      "Epoch 757/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4280 - accuracy: 0.7188\n",
      "Epoch 00757: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6591 - accuracy: 0.6222 - val_loss: 2.8421 - val_accuracy: 0.6818\n",
      "Epoch 758/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6031 - accuracy: 0.6250\n",
      "Epoch 00758: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6184 - accuracy: 0.6562 - val_loss: 2.8723 - val_accuracy: 0.6932\n",
      "Epoch 759/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5027 - accuracy: 0.6875\n",
      "Epoch 00759: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6438 - accuracy: 0.6591 - val_loss: 2.9349 - val_accuracy: 0.6932\n",
      "Epoch 760/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6408 - accuracy: 0.6250\n",
      "Epoch 00760: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6132 - accuracy: 0.6648 - val_loss: 2.6971 - val_accuracy: 0.6818\n",
      "Epoch 761/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6802 - accuracy: 0.5938\n",
      "Epoch 00761: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6588 - accuracy: 0.5966 - val_loss: 2.5618 - val_accuracy: 0.6932\n",
      "Epoch 762/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7586 - accuracy: 0.5625\n",
      "Epoch 00762: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6098 - accuracy: 0.6591 - val_loss: 2.8503 - val_accuracy: 0.6932\n",
      "Epoch 763/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.8035 - accuracy: 0.6250\n",
      "Epoch 00763: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6573 - accuracy: 0.6080 - val_loss: 2.9863 - val_accuracy: 0.6932\n",
      "Epoch 764/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6826 - accuracy: 0.5938\n",
      "Epoch 00764: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6774 - accuracy: 0.6023 - val_loss: 3.0005 - val_accuracy: 0.6932\n",
      "Epoch 765/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6927 - accuracy: 0.6875\n",
      "Epoch 00765: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6638 - accuracy: 0.6250 - val_loss: 3.1333 - val_accuracy: 0.6932\n",
      "Epoch 766/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4811 - accuracy: 0.6875\n",
      "Epoch 00766: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6662 - accuracy: 0.6108 - val_loss: 3.2051 - val_accuracy: 0.6932\n",
      "Epoch 767/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6840 - accuracy: 0.5938\n",
      "Epoch 00767: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6450 - accuracy: 0.6392 - val_loss: 3.1951 - val_accuracy: 0.6477\n",
      "Epoch 768/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.8179 - accuracy: 0.5312\n",
      "Epoch 00768: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6964 - accuracy: 0.6051 - val_loss: 3.2553 - val_accuracy: 0.6364\n",
      "Epoch 769/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.9295 - accuracy: 0.4375\n",
      "Epoch 00769: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.7168 - accuracy: 0.5966 - val_loss: 3.3240 - val_accuracy: 0.6705\n",
      "Epoch 770/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.8600 - accuracy: 0.5000\n",
      "Epoch 00770: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6845 - accuracy: 0.6023 - val_loss: 3.3189 - val_accuracy: 0.6364\n",
      "Epoch 771/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6264 - accuracy: 0.5938\n",
      "Epoch 00771: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6568 - accuracy: 0.6222 - val_loss: 3.3658 - val_accuracy: 0.6477\n",
      "Epoch 772/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6085 - accuracy: 0.5938\n",
      "Epoch 00772: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6493 - accuracy: 0.6449 - val_loss: 3.4698 - val_accuracy: 0.6705\n",
      "Epoch 773/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7513 - accuracy: 0.5625\n",
      "Epoch 00773: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6363 - accuracy: 0.6477 - val_loss: 3.9919 - val_accuracy: 0.6591\n",
      "Epoch 774/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6108 - accuracy: 0.7500\n",
      "Epoch 00774: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6634 - accuracy: 0.6477 - val_loss: 3.9144 - val_accuracy: 0.6705\n",
      "Epoch 775/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5509 - accuracy: 0.5938\n",
      "Epoch 00775: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6617 - accuracy: 0.6392 - val_loss: 3.5335 - val_accuracy: 0.6705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 776/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6553 - accuracy: 0.5625\n",
      "Epoch 00776: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6445 - accuracy: 0.6392 - val_loss: 3.5404 - val_accuracy: 0.6591\n",
      "Epoch 777/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6116 - accuracy: 0.6562\n",
      "Epoch 00777: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6662 - accuracy: 0.6392 - val_loss: 3.7017 - val_accuracy: 0.6477\n",
      "Epoch 778/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7453 - accuracy: 0.5625\n",
      "Epoch 00778: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6830 - accuracy: 0.6136 - val_loss: 3.9397 - val_accuracy: 0.6477\n",
      "Epoch 779/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.8048 - accuracy: 0.5312\n",
      "Epoch 00779: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6361 - accuracy: 0.6449 - val_loss: 3.3297 - val_accuracy: 0.6705\n",
      "Epoch 780/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6003 - accuracy: 0.7188\n",
      "Epoch 00780: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6808 - accuracy: 0.5938 - val_loss: 3.2456 - val_accuracy: 0.6364\n",
      "Epoch 781/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5345 - accuracy: 0.6562\n",
      "Epoch 00781: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6536 - accuracy: 0.6136 - val_loss: 3.1987 - val_accuracy: 0.6591\n",
      "Epoch 782/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6297 - accuracy: 0.6562\n",
      "Epoch 00782: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6969 - accuracy: 0.6080 - val_loss: 3.3150 - val_accuracy: 0.6477\n",
      "Epoch 783/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6172 - accuracy: 0.5312\n",
      "Epoch 00783: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6569 - accuracy: 0.6278 - val_loss: 3.1843 - val_accuracy: 0.6364\n",
      "Epoch 784/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6644 - accuracy: 0.5938\n",
      "Epoch 00784: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6628 - accuracy: 0.6307 - val_loss: 3.2090 - val_accuracy: 0.6477\n",
      "Epoch 785/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6488 - accuracy: 0.6562\n",
      "Epoch 00785: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6738 - accuracy: 0.6193 - val_loss: 3.2875 - val_accuracy: 0.6136\n",
      "Epoch 786/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6176 - accuracy: 0.6562\n",
      "Epoch 00786: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6820 - accuracy: 0.6222 - val_loss: 3.4116 - val_accuracy: 0.6364\n",
      "Epoch 787/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6288 - accuracy: 0.5938\n",
      "Epoch 00787: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6684 - accuracy: 0.6278 - val_loss: 3.9924 - val_accuracy: 0.6591\n",
      "Epoch 788/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5493 - accuracy: 0.7812\n",
      "Epoch 00788: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6291 - accuracy: 0.6449 - val_loss: 3.9946 - val_accuracy: 0.6591\n",
      "Epoch 789/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6002 - accuracy: 0.6250\n",
      "Epoch 00789: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6388 - accuracy: 0.6676 - val_loss: 2.7637 - val_accuracy: 0.6932\n",
      "Epoch 790/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5845 - accuracy: 0.7500\n",
      "Epoch 00790: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6370 - accuracy: 0.6506 - val_loss: 2.6514 - val_accuracy: 0.6477\n",
      "Epoch 791/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6042 - accuracy: 0.7812\n",
      "Epoch 00791: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6761 - accuracy: 0.6562 - val_loss: 3.5757 - val_accuracy: 0.6136\n",
      "Epoch 792/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.8326 - accuracy: 0.4375\n",
      "Epoch 00792: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.7848 - accuracy: 0.6222 - val_loss: 2.3606 - val_accuracy: 0.6705\n",
      "Epoch 793/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7649 - accuracy: 0.5938\n",
      "Epoch 00793: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6813 - accuracy: 0.6335 - val_loss: 1.9755 - val_accuracy: 0.7045\n",
      "Epoch 794/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6245 - accuracy: 0.5938\n",
      "Epoch 00794: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6826 - accuracy: 0.5824 - val_loss: 1.9647 - val_accuracy: 0.7159\n",
      "Epoch 795/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5754 - accuracy: 0.8438\n",
      "Epoch 00795: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6695 - accuracy: 0.6165 - val_loss: 1.9359 - val_accuracy: 0.7159\n",
      "Epoch 796/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6415 - accuracy: 0.5625\n",
      "Epoch 00796: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6621 - accuracy: 0.6023 - val_loss: 2.5902 - val_accuracy: 0.6591\n",
      "Epoch 797/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6259 - accuracy: 0.6250\n",
      "Epoch 00797: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6765 - accuracy: 0.6562 - val_loss: 2.2627 - val_accuracy: 0.6932\n",
      "Epoch 798/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7836 - accuracy: 0.5000\n",
      "Epoch 00798: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.5983 - accuracy: 0.6790 - val_loss: 2.5140 - val_accuracy: 0.7045\n",
      "Epoch 799/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5746 - accuracy: 0.6875\n",
      "Epoch 00799: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6158 - accuracy: 0.6591 - val_loss: 2.1844 - val_accuracy: 0.7159\n",
      "Epoch 800/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4990 - accuracy: 0.7812\n",
      "Epoch 00800: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.5736 - accuracy: 0.6932 - val_loss: 1.9672 - val_accuracy: 0.7159\n",
      "Epoch 801/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6531 - accuracy: 0.6250\n",
      "Epoch 00801: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6089 - accuracy: 0.6733 - val_loss: 2.0160 - val_accuracy: 0.7045\n",
      "Epoch 802/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5561 - accuracy: 0.7188\n",
      "Epoch 00802: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6568 - accuracy: 0.6278 - val_loss: 2.2581 - val_accuracy: 0.7159\n",
      "Epoch 803/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6966 - accuracy: 0.6250\n",
      "Epoch 00803: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6299 - accuracy: 0.6307 - val_loss: 2.5112 - val_accuracy: 0.7045\n",
      "Epoch 804/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6072 - accuracy: 0.6562\n",
      "Epoch 00804: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6259 - accuracy: 0.6364 - val_loss: 2.7191 - val_accuracy: 0.7045\n",
      "Epoch 805/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7233 - accuracy: 0.6250\n",
      "Epoch 00805: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6792 - accuracy: 0.6392 - val_loss: 2.4308 - val_accuracy: 0.7045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 806/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5529 - accuracy: 0.6250\n",
      "Epoch 00806: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6523 - accuracy: 0.6108 - val_loss: 2.7248 - val_accuracy: 0.6932\n",
      "Epoch 807/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5433 - accuracy: 0.6562\n",
      "Epoch 00807: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6677 - accuracy: 0.6080 - val_loss: 2.8016 - val_accuracy: 0.6818\n",
      "Epoch 808/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6295 - accuracy: 0.6250\n",
      "Epoch 00808: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6429 - accuracy: 0.6477 - val_loss: 2.3486 - val_accuracy: 0.7045\n",
      "Epoch 809/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6601 - accuracy: 0.5312\n",
      "Epoch 00809: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6327 - accuracy: 0.6506 - val_loss: 2.3486 - val_accuracy: 0.6591\n",
      "Epoch 810/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6070 - accuracy: 0.6250\n",
      "Epoch 00810: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6515 - accuracy: 0.6165 - val_loss: 2.8982 - val_accuracy: 0.6818\n",
      "Epoch 811/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.8210 - accuracy: 0.5000\n",
      "Epoch 00811: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6722 - accuracy: 0.6222 - val_loss: 3.0064 - val_accuracy: 0.6818\n",
      "Epoch 812/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7372 - accuracy: 0.6562\n",
      "Epoch 00812: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6476 - accuracy: 0.6420 - val_loss: 2.5544 - val_accuracy: 0.7045\n",
      "Epoch 813/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5644 - accuracy: 0.7188\n",
      "Epoch 00813: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.6278 - accuracy: 0.6562 - val_loss: 2.4264 - val_accuracy: 0.7045\n",
      "Epoch 814/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5849 - accuracy: 0.6562\n",
      "Epoch 00814: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6566 - accuracy: 0.6591 - val_loss: 2.2507 - val_accuracy: 0.7045\n",
      "Epoch 815/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6883 - accuracy: 0.6562\n",
      "Epoch 00815: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6301 - accuracy: 0.6818 - val_loss: 2.0811 - val_accuracy: 0.6705\n",
      "Epoch 816/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5919 - accuracy: 0.7812\n",
      "Epoch 00816: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6435 - accuracy: 0.6165 - val_loss: 2.1013 - val_accuracy: 0.6705\n",
      "Epoch 817/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6276 - accuracy: 0.7188\n",
      "Epoch 00817: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6461 - accuracy: 0.6307 - val_loss: 2.2399 - val_accuracy: 0.6364\n",
      "Epoch 818/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5844 - accuracy: 0.6250\n",
      "Epoch 00818: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6529 - accuracy: 0.6307 - val_loss: 2.4101 - val_accuracy: 0.7045\n",
      "Epoch 819/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6051 - accuracy: 0.5938\n",
      "Epoch 00819: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6619 - accuracy: 0.6250 - val_loss: 1.8875 - val_accuracy: 0.7045\n",
      "Epoch 820/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5442 - accuracy: 0.6875\n",
      "Epoch 00820: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6555 - accuracy: 0.6364 - val_loss: 1.9725 - val_accuracy: 0.6818\n",
      "Epoch 821/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5524 - accuracy: 0.6562\n",
      "Epoch 00821: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6701 - accuracy: 0.6278 - val_loss: 1.7504 - val_accuracy: 0.6818\n",
      "Epoch 822/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5711 - accuracy: 0.7812\n",
      "Epoch 00822: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6348 - accuracy: 0.6534 - val_loss: 2.2321 - val_accuracy: 0.6705\n",
      "Epoch 823/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7862 - accuracy: 0.5000\n",
      "Epoch 00823: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6424 - accuracy: 0.6477 - val_loss: 2.1243 - val_accuracy: 0.7045\n",
      "Epoch 824/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5726 - accuracy: 0.6875\n",
      "Epoch 00824: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6533 - accuracy: 0.6307 - val_loss: 1.4393 - val_accuracy: 0.6932\n",
      "Epoch 825/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6071 - accuracy: 0.5938\n",
      "Epoch 00825: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6775 - accuracy: 0.6051 - val_loss: 1.9673 - val_accuracy: 0.7045\n",
      "Epoch 826/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6258 - accuracy: 0.7500\n",
      "Epoch 00826: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6824 - accuracy: 0.6307 - val_loss: 1.2142 - val_accuracy: 0.6250\n",
      "Epoch 827/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6265 - accuracy: 0.8125\n",
      "Epoch 00827: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6781 - accuracy: 0.6222 - val_loss: 1.3140 - val_accuracy: 0.6591\n",
      "Epoch 828/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7469 - accuracy: 0.5312\n",
      "Epoch 00828: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6894 - accuracy: 0.6108 - val_loss: 1.4047 - val_accuracy: 0.6932\n",
      "Epoch 829/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6972 - accuracy: 0.5312\n",
      "Epoch 00829: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6587 - accuracy: 0.6420 - val_loss: 1.6339 - val_accuracy: 0.6705\n",
      "Epoch 830/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6455 - accuracy: 0.5000\n",
      "Epoch 00830: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6666 - accuracy: 0.5852 - val_loss: 2.5034 - val_accuracy: 0.6818\n",
      "Epoch 831/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6264 - accuracy: 0.7812\n",
      "Epoch 00831: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6482 - accuracy: 0.6733 - val_loss: 0.9989 - val_accuracy: 0.6932\n",
      "Epoch 832/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6794 - accuracy: 0.5625\n",
      "Epoch 00832: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6599 - accuracy: 0.6307 - val_loss: 1.0749 - val_accuracy: 0.6932\n",
      "Epoch 833/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.8056 - accuracy: 0.5312\n",
      "Epoch 00833: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6670 - accuracy: 0.6080 - val_loss: 1.4773 - val_accuracy: 0.6818\n",
      "Epoch 834/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7077 - accuracy: 0.5625\n",
      "Epoch 00834: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6800 - accuracy: 0.6165 - val_loss: 1.4196 - val_accuracy: 0.6591\n",
      "Epoch 835/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7526 - accuracy: 0.6250\n",
      "Epoch 00835: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6691 - accuracy: 0.6392 - val_loss: 1.4794 - val_accuracy: 0.6364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 836/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5165 - accuracy: 0.7812\n",
      "Epoch 00836: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6580 - accuracy: 0.6335 - val_loss: 1.6934 - val_accuracy: 0.6932\n",
      "Epoch 837/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6739 - accuracy: 0.4688\n",
      "Epoch 00837: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6678 - accuracy: 0.5881 - val_loss: 1.9920 - val_accuracy: 0.6932\n",
      "Epoch 838/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7182 - accuracy: 0.6562\n",
      "Epoch 00838: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6380 - accuracy: 0.6364 - val_loss: 2.2089 - val_accuracy: 0.6932\n",
      "Epoch 839/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7179 - accuracy: 0.6875\n",
      "Epoch 00839: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7020 - accuracy: 0.6364 - val_loss: 2.1279 - val_accuracy: 0.6932\n",
      "Epoch 840/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5546 - accuracy: 0.6875\n",
      "Epoch 00840: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6609 - accuracy: 0.6506 - val_loss: 2.1157 - val_accuracy: 0.6932\n",
      "Epoch 841/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7879 - accuracy: 0.5625\n",
      "Epoch 00841: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6796 - accuracy: 0.6392 - val_loss: 1.6217 - val_accuracy: 0.6932\n",
      "Epoch 842/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6997 - accuracy: 0.5312\n",
      "Epoch 00842: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7055 - accuracy: 0.6165 - val_loss: 1.4626 - val_accuracy: 0.7045\n",
      "Epoch 843/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.9370 - accuracy: 0.5000\n",
      "Epoch 00843: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6778 - accuracy: 0.6278 - val_loss: 1.7242 - val_accuracy: 0.6932\n",
      "Epoch 844/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5730 - accuracy: 0.7500\n",
      "Epoch 00844: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6635 - accuracy: 0.6705 - val_loss: 1.7040 - val_accuracy: 0.6932\n",
      "Epoch 845/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7187 - accuracy: 0.5938\n",
      "Epoch 00845: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6408 - accuracy: 0.6534 - val_loss: 1.5505 - val_accuracy: 0.6932\n",
      "Epoch 846/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5718 - accuracy: 0.6875\n",
      "Epoch 00846: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6658 - accuracy: 0.6364 - val_loss: 1.3060 - val_accuracy: 0.6818\n",
      "Epoch 847/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.8867 - accuracy: 0.6250\n",
      "Epoch 00847: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7683 - accuracy: 0.5710 - val_loss: 1.2448 - val_accuracy: 0.6705\n",
      "Epoch 848/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6547 - accuracy: 0.5938\n",
      "Epoch 00848: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7098 - accuracy: 0.5966 - val_loss: 1.3702 - val_accuracy: 0.6705\n",
      "Epoch 849/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.8206 - accuracy: 0.4062\n",
      "Epoch 00849: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.7059 - accuracy: 0.6080 - val_loss: 1.4036 - val_accuracy: 0.6705\n",
      "Epoch 850/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7006 - accuracy: 0.6250\n",
      "Epoch 00850: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6926 - accuracy: 0.5938 - val_loss: 1.7199 - val_accuracy: 0.6705\n",
      "Epoch 851/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.8749 - accuracy: 0.5000\n",
      "Epoch 00851: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6640 - accuracy: 0.6222 - val_loss: 2.2669 - val_accuracy: 0.6932\n",
      "Epoch 852/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5869 - accuracy: 0.6562\n",
      "Epoch 00852: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6758 - accuracy: 0.6250 - val_loss: 2.6183 - val_accuracy: 0.7045\n",
      "Epoch 853/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6551 - accuracy: 0.6875\n",
      "Epoch 00853: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6221 - accuracy: 0.6534 - val_loss: 2.3279 - val_accuracy: 0.7159\n",
      "Epoch 854/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7452 - accuracy: 0.6250\n",
      "Epoch 00854: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6307 - accuracy: 0.6506 - val_loss: 2.1106 - val_accuracy: 0.7159\n",
      "Epoch 855/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6786 - accuracy: 0.6562\n",
      "Epoch 00855: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6184 - accuracy: 0.6449 - val_loss: 1.7263 - val_accuracy: 0.7273\n",
      "Epoch 856/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6876 - accuracy: 0.6250\n",
      "Epoch 00856: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6840 - accuracy: 0.6080 - val_loss: 1.5355 - val_accuracy: 0.6932\n",
      "Epoch 857/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5862 - accuracy: 0.6875\n",
      "Epoch 00857: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6924 - accuracy: 0.6392 - val_loss: 1.7090 - val_accuracy: 0.7045\n",
      "Epoch 858/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6535 - accuracy: 0.6875\n",
      "Epoch 00858: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6764 - accuracy: 0.6335 - val_loss: 1.6575 - val_accuracy: 0.6818\n",
      "Epoch 859/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5065 - accuracy: 0.6250\n",
      "Epoch 00859: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6404 - accuracy: 0.6648 - val_loss: 1.8687 - val_accuracy: 0.6705\n",
      "Epoch 860/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7962 - accuracy: 0.5625\n",
      "Epoch 00860: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6534 - accuracy: 0.6278 - val_loss: 2.1797 - val_accuracy: 0.6818\n",
      "Epoch 861/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7685 - accuracy: 0.5625\n",
      "Epoch 00861: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5938 - val_loss: 1.4854 - val_accuracy: 0.7045\n",
      "Epoch 862/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6090 - accuracy: 0.6562\n",
      "Epoch 00862: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6411 - accuracy: 0.6477 - val_loss: 1.2462 - val_accuracy: 0.6818\n",
      "Epoch 863/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7568 - accuracy: 0.5000\n",
      "Epoch 00863: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6851 - accuracy: 0.6193 - val_loss: 1.3334 - val_accuracy: 0.7045\n",
      "Epoch 864/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6586 - accuracy: 0.6562\n",
      "Epoch 00864: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6845 - accuracy: 0.6250 - val_loss: 1.6233 - val_accuracy: 0.7045\n",
      "Epoch 865/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7459 - accuracy: 0.6250\n",
      "Epoch 00865: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.7194 - accuracy: 0.6392 - val_loss: 1.6591 - val_accuracy: 0.7045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 866/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7110 - accuracy: 0.5625\n",
      "Epoch 00866: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6429 - accuracy: 0.6420 - val_loss: 1.9811 - val_accuracy: 0.6591\n",
      "Epoch 867/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5547 - accuracy: 0.6875\n",
      "Epoch 00867: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6930 - accuracy: 0.6165 - val_loss: 1.2010 - val_accuracy: 0.6818\n",
      "Epoch 868/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7777 - accuracy: 0.5625\n",
      "Epoch 00868: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6901 - accuracy: 0.6136 - val_loss: 1.2553 - val_accuracy: 0.6818\n",
      "Epoch 869/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6684 - accuracy: 0.5312\n",
      "Epoch 00869: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6429 - accuracy: 0.6534 - val_loss: 1.4647 - val_accuracy: 0.7273\n",
      "Epoch 870/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7006 - accuracy: 0.5625\n",
      "Epoch 00870: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6385 - accuracy: 0.6051 - val_loss: 2.2613 - val_accuracy: 0.7159\n",
      "Epoch 871/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5563 - accuracy: 0.7188\n",
      "Epoch 00871: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6793 - accuracy: 0.6278 - val_loss: 2.0263 - val_accuracy: 0.7045\n",
      "Epoch 872/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6090 - accuracy: 0.7188\n",
      "Epoch 00872: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6560 - accuracy: 0.6392 - val_loss: 1.7875 - val_accuracy: 0.7159\n",
      "Epoch 873/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5680 - accuracy: 0.6875\n",
      "Epoch 00873: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6356 - accuracy: 0.6676 - val_loss: 1.8159 - val_accuracy: 0.7159\n",
      "Epoch 874/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7995 - accuracy: 0.5625\n",
      "Epoch 00874: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6647 - accuracy: 0.6619 - val_loss: 1.6114 - val_accuracy: 0.7159\n",
      "Epoch 875/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6080 - accuracy: 0.6875\n",
      "Epoch 00875: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.7041 - accuracy: 0.6222 - val_loss: 1.4207 - val_accuracy: 0.7159\n",
      "Epoch 876/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7784 - accuracy: 0.5938\n",
      "Epoch 00876: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6579 - accuracy: 0.6591 - val_loss: 1.5704 - val_accuracy: 0.7159\n",
      "Epoch 877/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6661 - accuracy: 0.5625\n",
      "Epoch 00877: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6545 - accuracy: 0.6250 - val_loss: 2.3124 - val_accuracy: 0.7045\n",
      "Epoch 878/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5372 - accuracy: 0.6562\n",
      "Epoch 00878: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6468 - accuracy: 0.6307 - val_loss: 2.5297 - val_accuracy: 0.7045\n",
      "Epoch 879/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4901 - accuracy: 0.7812\n",
      "Epoch 00879: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6325 - accuracy: 0.6364 - val_loss: 2.2454 - val_accuracy: 0.7159\n",
      "Epoch 880/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5552 - accuracy: 0.7500\n",
      "Epoch 00880: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6307 - accuracy: 0.6392 - val_loss: 1.9124 - val_accuracy: 0.6364\n",
      "Epoch 881/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6067 - accuracy: 0.6562\n",
      "Epoch 00881: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6714 - accuracy: 0.5966 - val_loss: 2.3418 - val_accuracy: 0.6136\n",
      "Epoch 882/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6204 - accuracy: 0.6562\n",
      "Epoch 00882: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6729 - accuracy: 0.6278 - val_loss: 1.7168 - val_accuracy: 0.6818\n",
      "Epoch 883/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6614 - accuracy: 0.5938\n",
      "Epoch 00883: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6709 - accuracy: 0.6136 - val_loss: 2.1856 - val_accuracy: 0.6932\n",
      "Epoch 884/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5386 - accuracy: 0.7812\n",
      "Epoch 00884: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6965 - accuracy: 0.6136 - val_loss: 2.3405 - val_accuracy: 0.6932\n",
      "Epoch 885/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6860 - accuracy: 0.6562\n",
      "Epoch 00885: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6758 - accuracy: 0.6420 - val_loss: 2.9993 - val_accuracy: 0.6818\n",
      "Epoch 886/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7102 - accuracy: 0.6250\n",
      "Epoch 00886: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6658 - accuracy: 0.6506 - val_loss: 2.7274 - val_accuracy: 0.6705\n",
      "Epoch 887/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6210 - accuracy: 0.6875\n",
      "Epoch 00887: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6500 - accuracy: 0.6705 - val_loss: 3.2099 - val_accuracy: 0.6818\n",
      "Epoch 888/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5871 - accuracy: 0.6875\n",
      "Epoch 00888: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6979 - accuracy: 0.6477 - val_loss: 3.0757 - val_accuracy: 0.6932\n",
      "Epoch 889/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7831 - accuracy: 0.6250\n",
      "Epoch 00889: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.7038 - accuracy: 0.6250 - val_loss: 2.7776 - val_accuracy: 0.6932\n",
      "Epoch 890/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7336 - accuracy: 0.5000\n",
      "Epoch 00890: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6618 - accuracy: 0.6705 - val_loss: 2.8825 - val_accuracy: 0.6818\n",
      "Epoch 891/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6402 - accuracy: 0.6250\n",
      "Epoch 00891: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6508 - accuracy: 0.6705 - val_loss: 3.5494 - val_accuracy: 0.6932\n",
      "Epoch 892/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6526 - accuracy: 0.6250\n",
      "Epoch 00892: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6368 - accuracy: 0.6790 - val_loss: 3.1959 - val_accuracy: 0.6818\n",
      "Epoch 893/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6812 - accuracy: 0.7188\n",
      "Epoch 00893: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6385 - accuracy: 0.6733 - val_loss: 2.7276 - val_accuracy: 0.6932\n",
      "Epoch 894/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6119 - accuracy: 0.6562\n",
      "Epoch 00894: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6490 - accuracy: 0.6562 - val_loss: 2.5313 - val_accuracy: 0.6818\n",
      "Epoch 895/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7695 - accuracy: 0.5000\n",
      "Epoch 00895: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6565 - accuracy: 0.6619 - val_loss: 2.6808 - val_accuracy: 0.6932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 896/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7001 - accuracy: 0.6250\n",
      "Epoch 00896: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6677 - accuracy: 0.6477 - val_loss: 3.7591 - val_accuracy: 0.6932\n",
      "Epoch 897/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6737 - accuracy: 0.6875\n",
      "Epoch 00897: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6497 - accuracy: 0.6676 - val_loss: 3.6043 - val_accuracy: 0.7045\n",
      "Epoch 898/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6718 - accuracy: 0.6250\n",
      "Epoch 00898: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6766 - accuracy: 0.6335 - val_loss: 3.4044 - val_accuracy: 0.6932\n",
      "Epoch 899/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7704 - accuracy: 0.5312\n",
      "Epoch 00899: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6520 - accuracy: 0.6591 - val_loss: 3.4518 - val_accuracy: 0.6932\n",
      "Epoch 900/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7152 - accuracy: 0.6250\n",
      "Epoch 00900: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6313 - accuracy: 0.6733 - val_loss: 3.3681 - val_accuracy: 0.6932\n",
      "Epoch 901/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7868 - accuracy: 0.4688\n",
      "Epoch 00901: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6200 - accuracy: 0.6705 - val_loss: 3.5614 - val_accuracy: 0.7045\n",
      "Epoch 902/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7316 - accuracy: 0.6562\n",
      "Epoch 00902: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6350 - accuracy: 0.6818 - val_loss: 4.1371 - val_accuracy: 0.7159\n",
      "Epoch 903/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6632 - accuracy: 0.6875\n",
      "Epoch 00903: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.5754 - accuracy: 0.6932 - val_loss: 2.5917 - val_accuracy: 0.7159\n",
      "Epoch 904/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7947 - accuracy: 0.5312\n",
      "Epoch 00904: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6370 - accuracy: 0.6449 - val_loss: 2.5988 - val_accuracy: 0.7159\n",
      "Epoch 905/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6458 - accuracy: 0.7188\n",
      "Epoch 00905: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6235 - accuracy: 0.6875 - val_loss: 2.9834 - val_accuracy: 0.7159\n",
      "Epoch 906/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5553 - accuracy: 0.7188\n",
      "Epoch 00906: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6019 - accuracy: 0.6960 - val_loss: 2.7166 - val_accuracy: 0.6818\n",
      "Epoch 907/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5162 - accuracy: 0.7500\n",
      "Epoch 00907: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6084 - accuracy: 0.6875 - val_loss: 2.4945 - val_accuracy: 0.6818\n",
      "Epoch 908/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5909 - accuracy: 0.6875\n",
      "Epoch 00908: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6141 - accuracy: 0.6761 - val_loss: 2.4014 - val_accuracy: 0.7159\n",
      "Epoch 909/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4549 - accuracy: 0.7812\n",
      "Epoch 00909: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6325 - accuracy: 0.6705 - val_loss: 2.5876 - val_accuracy: 0.7045\n",
      "Epoch 910/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.3780 - accuracy: 0.7812\n",
      "Epoch 00910: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.5688 - accuracy: 0.7017 - val_loss: 2.7944 - val_accuracy: 0.7159\n",
      "Epoch 911/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7267 - accuracy: 0.6250\n",
      "Epoch 00911: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6118 - accuracy: 0.6875 - val_loss: 2.8695 - val_accuracy: 0.7045\n",
      "Epoch 912/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6116 - accuracy: 0.7188\n",
      "Epoch 00912: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6077 - accuracy: 0.6534 - val_loss: 2.7103 - val_accuracy: 0.6818\n",
      "Epoch 913/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6463 - accuracy: 0.6250\n",
      "Epoch 00913: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6399 - accuracy: 0.6392 - val_loss: 2.9070 - val_accuracy: 0.7273\n",
      "Epoch 914/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6362 - accuracy: 0.6250\n",
      "Epoch 00914: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6208 - accuracy: 0.6619 - val_loss: 2.9026 - val_accuracy: 0.7273\n",
      "Epoch 915/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6017 - accuracy: 0.7188\n",
      "Epoch 00915: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6207 - accuracy: 0.6733 - val_loss: 2.9663 - val_accuracy: 0.7273\n",
      "Epoch 916/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5424 - accuracy: 0.7188\n",
      "Epoch 00916: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6036 - accuracy: 0.6847 - val_loss: 3.3262 - val_accuracy: 0.7159\n",
      "Epoch 917/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6339 - accuracy: 0.7500\n",
      "Epoch 00917: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6185 - accuracy: 0.6761 - val_loss: 3.2373 - val_accuracy: 0.7273\n",
      "Epoch 918/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6972 - accuracy: 0.6562\n",
      "Epoch 00918: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6033 - accuracy: 0.6676 - val_loss: 3.6491 - val_accuracy: 0.7045\n",
      "Epoch 919/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7575 - accuracy: 0.5938\n",
      "Epoch 00919: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6007 - accuracy: 0.6903 - val_loss: 3.0301 - val_accuracy: 0.7045\n",
      "Epoch 920/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6651 - accuracy: 0.6562\n",
      "Epoch 00920: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5909 - accuracy: 0.6932 - val_loss: 3.0295 - val_accuracy: 0.7159\n",
      "Epoch 921/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7562 - accuracy: 0.5312\n",
      "Epoch 00921: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6535 - accuracy: 0.6534 - val_loss: 2.6873 - val_accuracy: 0.7159\n",
      "Epoch 922/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5567 - accuracy: 0.7188\n",
      "Epoch 00922: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6040 - accuracy: 0.6562 - val_loss: 2.5152 - val_accuracy: 0.7159\n",
      "Epoch 923/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7357 - accuracy: 0.5000\n",
      "Epoch 00923: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6394 - accuracy: 0.6477 - val_loss: 2.5801 - val_accuracy: 0.6705\n",
      "Epoch 924/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7696 - accuracy: 0.5312\n",
      "Epoch 00924: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6549 - accuracy: 0.6392 - val_loss: 2.8535 - val_accuracy: 0.6705\n",
      "Epoch 925/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5506 - accuracy: 0.7188\n",
      "Epoch 00925: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6898 - accuracy: 0.6449 - val_loss: 3.6089 - val_accuracy: 0.6591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 926/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5906 - accuracy: 0.6875\n",
      "Epoch 00926: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6909 - accuracy: 0.6335 - val_loss: 2.3810 - val_accuracy: 0.6705\n",
      "Epoch 927/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6258 - accuracy: 0.5938\n",
      "Epoch 00927: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6633 - accuracy: 0.6449 - val_loss: 2.4651 - val_accuracy: 0.6705\n",
      "Epoch 928/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6733 - accuracy: 0.5625\n",
      "Epoch 00928: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6319 - accuracy: 0.6705 - val_loss: 2.5880 - val_accuracy: 0.6705\n",
      "Epoch 929/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6342 - accuracy: 0.6250\n",
      "Epoch 00929: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6500 - accuracy: 0.6449 - val_loss: 2.7582 - val_accuracy: 0.6705\n",
      "Epoch 930/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6414 - accuracy: 0.6250\n",
      "Epoch 00930: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6515 - accuracy: 0.6420 - val_loss: 3.2866 - val_accuracy: 0.6932\n",
      "Epoch 931/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5055 - accuracy: 0.7812\n",
      "Epoch 00931: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6488 - accuracy: 0.6420 - val_loss: 3.1649 - val_accuracy: 0.6932\n",
      "Epoch 932/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5612 - accuracy: 0.7188\n",
      "Epoch 00932: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6236 - accuracy: 0.6761 - val_loss: 3.0642 - val_accuracy: 0.7045\n",
      "Epoch 933/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6118 - accuracy: 0.6562\n",
      "Epoch 00933: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6458 - accuracy: 0.6477 - val_loss: 3.0213 - val_accuracy: 0.7045\n",
      "Epoch 934/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6920 - accuracy: 0.5938\n",
      "Epoch 00934: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.5724 - accuracy: 0.6960 - val_loss: 3.0493 - val_accuracy: 0.7273\n",
      "Epoch 935/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6059 - accuracy: 0.5938\n",
      "Epoch 00935: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6143 - accuracy: 0.6818 - val_loss: 3.1756 - val_accuracy: 0.7273\n",
      "Epoch 936/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4880 - accuracy: 0.7188\n",
      "Epoch 00936: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6022 - accuracy: 0.6903 - val_loss: 3.3030 - val_accuracy: 0.7273\n",
      "Epoch 937/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7145 - accuracy: 0.6250\n",
      "Epoch 00937: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.5864 - accuracy: 0.6790 - val_loss: 3.2217 - val_accuracy: 0.7273\n",
      "Epoch 938/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7977 - accuracy: 0.5000\n",
      "Epoch 00938: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6173 - accuracy: 0.6619 - val_loss: 3.7284 - val_accuracy: 0.7159\n",
      "Epoch 939/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5448 - accuracy: 0.7500\n",
      "Epoch 00939: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6243 - accuracy: 0.6648 - val_loss: 3.0239 - val_accuracy: 0.7273\n",
      "Epoch 940/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5113 - accuracy: 0.7188\n",
      "Epoch 00940: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.5975 - accuracy: 0.6875 - val_loss: 2.7646 - val_accuracy: 0.7159\n",
      "Epoch 941/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6844 - accuracy: 0.6875\n",
      "Epoch 00941: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6255 - accuracy: 0.6648 - val_loss: 2.7637 - val_accuracy: 0.7159\n",
      "Epoch 942/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6538 - accuracy: 0.5938\n",
      "Epoch 00942: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6105 - accuracy: 0.6562 - val_loss: 3.0325 - val_accuracy: 0.7159\n",
      "Epoch 943/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5469 - accuracy: 0.7500\n",
      "Epoch 00943: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6238 - accuracy: 0.6761 - val_loss: 2.1321 - val_accuracy: 0.7045\n",
      "Epoch 944/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5837 - accuracy: 0.6250\n",
      "Epoch 00944: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6545 - accuracy: 0.6392 - val_loss: 1.9766 - val_accuracy: 0.6932\n",
      "Epoch 945/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.8157 - accuracy: 0.4688\n",
      "Epoch 00945: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6333 - accuracy: 0.6420 - val_loss: 1.7380 - val_accuracy: 0.6932\n",
      "Epoch 946/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7990 - accuracy: 0.4375\n",
      "Epoch 00946: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6859 - accuracy: 0.6278 - val_loss: 1.4161 - val_accuracy: 0.6932\n",
      "Epoch 947/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5550 - accuracy: 0.7188\n",
      "Epoch 00947: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6825 - accuracy: 0.6477 - val_loss: 1.5442 - val_accuracy: 0.6932\n",
      "Epoch 948/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7561 - accuracy: 0.6250\n",
      "Epoch 00948: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6905 - accuracy: 0.6278 - val_loss: 1.4471 - val_accuracy: 0.7045\n",
      "Epoch 949/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7616 - accuracy: 0.5000\n",
      "Epoch 00949: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6847 - accuracy: 0.6051 - val_loss: 1.6875 - val_accuracy: 0.6818\n",
      "Epoch 950/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6395 - accuracy: 0.5625\n",
      "Epoch 00950: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6911 - accuracy: 0.6250 - val_loss: 1.6432 - val_accuracy: 0.6932\n",
      "Epoch 951/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.8251 - accuracy: 0.5000\n",
      "Epoch 00951: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6576 - accuracy: 0.6420 - val_loss: 1.8077 - val_accuracy: 0.7045\n",
      "Epoch 952/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6108 - accuracy: 0.6562\n",
      "Epoch 00952: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6346 - accuracy: 0.6506 - val_loss: 2.3480 - val_accuracy: 0.7159\n",
      "Epoch 953/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6410 - accuracy: 0.7812\n",
      "Epoch 00953: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6336 - accuracy: 0.6960 - val_loss: 2.3878 - val_accuracy: 0.7045\n",
      "Epoch 954/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7049 - accuracy: 0.5625\n",
      "Epoch 00954: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6379 - accuracy: 0.6506 - val_loss: 2.9942 - val_accuracy: 0.6477\n",
      "Epoch 955/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6844 - accuracy: 0.6875\n",
      "Epoch 00955: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6473 - accuracy: 0.6705 - val_loss: 2.8980 - val_accuracy: 0.6591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 956/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4829 - accuracy: 0.8750\n",
      "Epoch 00956: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6440 - accuracy: 0.6562 - val_loss: 2.8927 - val_accuracy: 0.6932\n",
      "Epoch 957/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5167 - accuracy: 0.6562\n",
      "Epoch 00957: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6640 - accuracy: 0.6364 - val_loss: 2.0153 - val_accuracy: 0.7159\n",
      "Epoch 958/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6298 - accuracy: 0.6562\n",
      "Epoch 00958: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6435 - accuracy: 0.6420 - val_loss: 2.0209 - val_accuracy: 0.7159\n",
      "Epoch 959/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6528 - accuracy: 0.5938\n",
      "Epoch 00959: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.5921 - accuracy: 0.6705 - val_loss: 2.4552 - val_accuracy: 0.7045\n",
      "Epoch 960/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6910 - accuracy: 0.6875\n",
      "Epoch 00960: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6572 - accuracy: 0.6506 - val_loss: 2.4150 - val_accuracy: 0.6818\n",
      "Epoch 961/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6654 - accuracy: 0.6875\n",
      "Epoch 00961: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6025 - accuracy: 0.6932 - val_loss: 2.3209 - val_accuracy: 0.7159\n",
      "Epoch 962/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6815 - accuracy: 0.6250\n",
      "Epoch 00962: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6188 - accuracy: 0.6506 - val_loss: 2.4385 - val_accuracy: 0.7159\n",
      "Epoch 963/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6793 - accuracy: 0.5312\n",
      "Epoch 00963: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6272 - accuracy: 0.6591 - val_loss: 2.5378 - val_accuracy: 0.7045\n",
      "Epoch 964/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5488 - accuracy: 0.7812\n",
      "Epoch 00964: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6022 - accuracy: 0.6989 - val_loss: 2.5271 - val_accuracy: 0.7045\n",
      "Epoch 965/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6235 - accuracy: 0.6875\n",
      "Epoch 00965: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6141 - accuracy: 0.6733 - val_loss: 2.5719 - val_accuracy: 0.7159\n",
      "Epoch 966/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6761 - accuracy: 0.6875\n",
      "Epoch 00966: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6497 - accuracy: 0.6619 - val_loss: 2.6241 - val_accuracy: 0.6932\n",
      "Epoch 967/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6082 - accuracy: 0.6562\n",
      "Epoch 00967: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6159 - accuracy: 0.6733 - val_loss: 2.6748 - val_accuracy: 0.6932\n",
      "Epoch 968/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6832 - accuracy: 0.6250\n",
      "Epoch 00968: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6161 - accuracy: 0.6562 - val_loss: 2.7050 - val_accuracy: 0.7045\n",
      "Epoch 969/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7144 - accuracy: 0.6250\n",
      "Epoch 00969: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6529 - accuracy: 0.6392 - val_loss: 2.7166 - val_accuracy: 0.7045\n",
      "Epoch 970/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6269 - accuracy: 0.7500\n",
      "Epoch 00970: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5982 - accuracy: 0.6989 - val_loss: 2.7868 - val_accuracy: 0.6932\n",
      "Epoch 971/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6941 - accuracy: 0.7188\n",
      "Epoch 00971: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6069 - accuracy: 0.6733 - val_loss: 2.6938 - val_accuracy: 0.6932\n",
      "Epoch 972/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6771 - accuracy: 0.6250\n",
      "Epoch 00972: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6430 - accuracy: 0.6364 - val_loss: 2.7322 - val_accuracy: 0.6932\n",
      "Epoch 973/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6520 - accuracy: 0.6875\n",
      "Epoch 00973: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5927 - accuracy: 0.6875 - val_loss: 2.8739 - val_accuracy: 0.7273\n",
      "Epoch 974/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4284 - accuracy: 0.8125\n",
      "Epoch 00974: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6355 - accuracy: 0.6477 - val_loss: 2.7707 - val_accuracy: 0.7045\n",
      "Epoch 975/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5377 - accuracy: 0.6250\n",
      "Epoch 00975: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6097 - accuracy: 0.6761 - val_loss: 2.5726 - val_accuracy: 0.6932\n",
      "Epoch 976/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5561 - accuracy: 0.6562\n",
      "Epoch 00976: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6641 - accuracy: 0.6449 - val_loss: 2.6512 - val_accuracy: 0.7159\n",
      "Epoch 977/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5700 - accuracy: 0.6562\n",
      "Epoch 00977: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.5925 - accuracy: 0.6960 - val_loss: 2.9482 - val_accuracy: 0.6932\n",
      "Epoch 978/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6679 - accuracy: 0.6250\n",
      "Epoch 00978: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6256 - accuracy: 0.6562 - val_loss: 3.2315 - val_accuracy: 0.6818\n",
      "Epoch 979/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7868 - accuracy: 0.5625\n",
      "Epoch 00979: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6183 - accuracy: 0.6818 - val_loss: 3.3912 - val_accuracy: 0.6818\n",
      "Epoch 980/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5891 - accuracy: 0.6562\n",
      "Epoch 00980: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6188 - accuracy: 0.6847 - val_loss: 3.5767 - val_accuracy: 0.6477\n",
      "Epoch 981/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.8160 - accuracy: 0.5000\n",
      "Epoch 00981: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6793 - accuracy: 0.6477 - val_loss: 3.4398 - val_accuracy: 0.6705\n",
      "Epoch 982/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7847 - accuracy: 0.5312\n",
      "Epoch 00982: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6174 - accuracy: 0.6761 - val_loss: 3.3951 - val_accuracy: 0.6591\n",
      "Epoch 983/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6927 - accuracy: 0.5625\n",
      "Epoch 00983: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6181 - accuracy: 0.6648 - val_loss: 3.4119 - val_accuracy: 0.6591\n",
      "Epoch 984/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6725 - accuracy: 0.6562\n",
      "Epoch 00984: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6055 - accuracy: 0.6733 - val_loss: 3.5318 - val_accuracy: 0.6818\n",
      "Epoch 985/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7045 - accuracy: 0.6250\n",
      "Epoch 00985: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6202 - accuracy: 0.6648 - val_loss: 3.7171 - val_accuracy: 0.6932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 986/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.3648 - accuracy: 0.8750\n",
      "Epoch 00986: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6094 - accuracy: 0.6761 - val_loss: 3.8441 - val_accuracy: 0.6932\n",
      "Epoch 987/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4768 - accuracy: 0.7500\n",
      "Epoch 00987: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6123 - accuracy: 0.6875 - val_loss: 4.2894 - val_accuracy: 0.6932\n",
      "Epoch 988/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6120 - accuracy: 0.7188\n",
      "Epoch 00988: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5816 - accuracy: 0.6960 - val_loss: 4.3202 - val_accuracy: 0.6932\n",
      "Epoch 989/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7034 - accuracy: 0.5312\n",
      "Epoch 00989: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5936 - accuracy: 0.6903 - val_loss: 4.5630 - val_accuracy: 0.7045\n",
      "Epoch 990/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4396 - accuracy: 0.7812\n",
      "Epoch 00990: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5878 - accuracy: 0.7017 - val_loss: 4.7322 - val_accuracy: 0.7045\n",
      "Epoch 991/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5959 - accuracy: 0.7188\n",
      "Epoch 00991: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5944 - accuracy: 0.6847 - val_loss: 4.5910 - val_accuracy: 0.6932\n",
      "Epoch 992/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5562 - accuracy: 0.7500\n",
      "Epoch 00992: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6005 - accuracy: 0.6818 - val_loss: 4.1854 - val_accuracy: 0.6932\n",
      "Epoch 993/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5662 - accuracy: 0.7812\n",
      "Epoch 00993: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6007 - accuracy: 0.6847 - val_loss: 4.3643 - val_accuracy: 0.6932\n",
      "Epoch 994/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4746 - accuracy: 0.7188\n",
      "Epoch 00994: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5736 - accuracy: 0.6932 - val_loss: 4.5580 - val_accuracy: 0.6932\n",
      "Epoch 995/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6643 - accuracy: 0.5938\n",
      "Epoch 00995: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5747 - accuracy: 0.7045 - val_loss: 4.6658 - val_accuracy: 0.6932\n",
      "Epoch 996/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6187 - accuracy: 0.6562\n",
      "Epoch 00996: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6129 - accuracy: 0.6761 - val_loss: 5.5457 - val_accuracy: 0.6932\n",
      "Epoch 997/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5754 - accuracy: 0.7188\n",
      "Epoch 00997: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6479 - accuracy: 0.6676 - val_loss: 3.7026 - val_accuracy: 0.6932\n",
      "Epoch 998/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5976 - accuracy: 0.6250\n",
      "Epoch 00998: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5995 - accuracy: 0.6676 - val_loss: 2.9977 - val_accuracy: 0.6932\n",
      "Epoch 999/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5309 - accuracy: 0.6875\n",
      "Epoch 00999: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6286 - accuracy: 0.6420 - val_loss: 2.8307 - val_accuracy: 0.6932\n",
      "Epoch 1000/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6247 - accuracy: 0.6250\n",
      "Epoch 01000: val_loss did not improve from 0.76896\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6335 - accuracy: 0.6648 - val_loss: 3.1123 - val_accuracy: 0.7045\n",
      "Training completed in time:  0:00:32.933442\n"
     ]
    }
   ],
   "source": [
    "## Training the model\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from datetime import datetime\n",
    "\n",
    "num_epochs= 1000\n",
    "num_batch_size=32\n",
    "\n",
    "checkpointer=ModelCheckpoint(filepath='save_models/audio_classification.hdf5',verbose=1,save_best_only=True)\n",
    "start=datetime.now()\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(X_test,y_test), callbacks= [checkpointer])\n",
    "\n",
    "\n",
    "duration = datetime.now()- start\n",
    "print(\"Training completed in time: \", duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7045454382896423\n"
     ]
    }
   ],
   "source": [
    "test_accurancy=model.evaluate(X_test,y_test,verbose=0)\n",
    "print(test_accurancy[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.1051413e+02  1.3952419e+02 -5.8901123e+01 -8.8972130e+00\n",
      "  2.5681480e+01 -2.7331490e+01 -1.1936617e+01  7.5096984e+00\n",
      " -1.2321795e+01  3.9308410e+00  1.3287276e+01 -5.7260122e+00\n",
      "  1.4570723e+00  1.5687888e+00 -1.5360000e+01 -2.3001361e+00\n",
      "  5.6203284e+00 -1.0245839e+01 -3.4561040e+00  1.0385894e+01\n",
      "  4.7979097e+00  2.4438143e+00  1.4755886e+00 -4.7268567e+00\n",
      " -2.9156420e+00 -1.5844594e+00 -2.6022174e+00  3.3887012e+00\n",
      "  3.1356783e+00 -1.7447152e+00  2.4727128e+00  2.3718290e+00\n",
      " -4.2563739e+00 -2.6077342e+00  1.8934812e-01 -7.4680054e-01\n",
      "  1.0415385e+00  7.6720171e-02 -1.9701738e+00  1.7957147e+00]\n",
      "[[-5.1051413e+02  1.3952419e+02 -5.8901123e+01 -8.8972130e+00\n",
      "   2.5681480e+01 -2.7331490e+01 -1.1936617e+01  7.5096984e+00\n",
      "  -1.2321795e+01  3.9308410e+00  1.3287276e+01 -5.7260122e+00\n",
      "   1.4570723e+00  1.5687888e+00 -1.5360000e+01 -2.3001361e+00\n",
      "   5.6203284e+00 -1.0245839e+01 -3.4561040e+00  1.0385894e+01\n",
      "   4.7979097e+00  2.4438143e+00  1.4755886e+00 -4.7268567e+00\n",
      "  -2.9156420e+00 -1.5844594e+00 -2.6022174e+00  3.3887012e+00\n",
      "   3.1356783e+00 -1.7447152e+00  2.4727128e+00  2.3718290e+00\n",
      "  -4.2563739e+00 -2.6077342e+00  1.8934812e-01 -7.4680054e-01\n",
      "   1.0415385e+00  7.6720171e-02 -1.9701738e+00  1.7957147e+00]]\n",
      "(1, 40)\n",
      "[2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['waiting_for_food'], dtype='<U16')"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Test it out\n",
    "filename=\"ML project\\F_BAC01_MC_MN_SIM01_103.wav\"\n",
    "audio,sample_rate=librosa.load(filename,res_type='kaiser_fast')\n",
    "mfccs_features = librosa.feature.mfcc(y=audio,sr=sample_rate,n_mfcc=40)\n",
    "mfccs_scaled_features=np.mean(mfccs_features.T,axis=0)\n",
    "\n",
    "print(mfccs_scaled_features)\n",
    "mfccs_scaled_features=mfccs_scaled_features.reshape(1,-1)\n",
    "print(mfccs_scaled_features)\n",
    "print(mfccs_scaled_features.shape)\n",
    "predicted_label=model.predict_classes(mfccs_scaled_features)\n",
    "print(predicted_label)\n",
    "prediction_class=labelencoder.inverse_transform(predicted_label)\n",
    "prediction_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
